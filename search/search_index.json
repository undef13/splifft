{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SpliFFT","text":"<p>Lightweight utilities for music source separation and transcription.</p> <p>This library is a ground-up rewrite of the zfturbo's MSST repo, with a strong focus on robustness, simplicity and extensibility. We keep third-party dependencies to an absolute minimum to ease installation.</p> <p>\u26a0\ufe0f This is pre-alpha software, expect significant breaking changes before v0.1.</p>"},{"location":"#supported-models","title":"Supported Models","text":"<ul> <li>BS-Roformer (including unwa's HyperACE v1 and v2, Large Inst modifications)</li> <li>Mel-Roformer</li> <li>MDX23C TFC-TDF v3</li> <li>beat this! for beat tracking without DBN postprocessing</li> <li>PESTO for monophonic pitch estimation</li> <li>basic pitch for polyphonic pitch estimation (only frame-level onset, multipitch and posteriorgrams, no MIDI postprocessing)</li> </ul> <p>Our default registry supports 110+ community-trained separation models.</p>"},{"location":"#installation-usage","title":"Installation &amp; Usage","text":"<ul> <li>I just want to run it \u00bb</li> <li>I want to add it as a library to my Python project \u00bb</li> <li>I want to contribute \u00bb</li> </ul> <p>More information about models and config can be found on the documentation.</p>"},{"location":"#cli","title":"CLI","text":"<p>There are three steps. You do not need to have Python installed.</p> <ol> <li> <p>Install <code>uv</code> if you haven't already. It is an awesome Python package and library manager with pip compatibility.</p> <pre><code># Linux / MacOS\nwget -qO- https://astral.sh/uv/install.sh | sh\n# Windows\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre> </li> <li> <p>Open a new terminal and install the latest stable PyPI release as a tool. It will install the Python interpreter, all necessary packages and add the <code>splifft</code> executable to your <code>PATH</code>:</p> <pre><code>uv tool install \"splifft[config,inference,cli,web]\"\n</code></pre> <p> Explanation of feature flags <p>The core is kept as minimal as possible. Pick which ones you need:</p> <ul> <li>The <code>config</code> extra is used to parse the model configuration from JSON and discover the registry's default cache dir.</li> <li>The <code>inference</code> extra is used to decode audio formats.</li> <li>The <code>cli</code> extra provides you with the <code>splifft</code> command line tool</li> <li>The <code>web</code> extra is used to download models. </li> </ul> <p> I want the latest bleeding-edge version <p>This directly pulls from the <code>main</code> branch, which may be unstable:</p> <pre><code>uv tool install \"git+https://github.com/undef13/splifft.git[config,inference,cli,web]\"\n</code></pre> <li> <p>We recommend using our built-in registry to manage model config and weights:</p> <pre><code># list all available models, including those not yet available locally\nsplifft ls -a\n\n# download model files and config to your user cache directory\n# ~/.cache/splifft on linux\nsplifft pull bs_roformer-fruit-sw\n\n# view information about the configuration\n# modify the configuration, such as batch size according to your hardware\nsplifft info bs_roformer-fruit-sw\n\n# run inference\nsplifft run data/audio/input/3BFTio5296w.flac --model bs_roformer-fruit-sw\n</code></pre> <p>Alternatively, for custom models, you can manage files manually. Go into a new directory and place the model checkpoint and configuration inside it. Assuming your current directory has this structure (doesn't have to be exactly this):</p> <p> Minimal reproduction: with example audio from YouTube <pre><code>uv tool install yt-dlp\nyt-dlp -f bestaudio -o data/audio/input/3BFTio5296w.flac 3BFTio5296w\nwget -P data/models/ https://huggingface.co/undef13/splifft/resolve/main/roformer-fp16.pt?download=true\nwget -P data/config/ https://raw.githubusercontent.com/undef13/splifft/refs/heads/main/data/config/bs_roformer.json\n</code></pre> <pre><code>.\n\u2514\u2500\u2500 data\n    \u251c\u2500\u2500 audio\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 input\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 3BFTio5296w.flac\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 output\n    \u251c\u2500\u2500 config\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 bs_roformer.json\n    \u2514\u2500\u2500 models\n        \u2514\u2500\u2500 roformer-fp16.pt\n</code></pre> <p>Run:</p> <pre><code>splifft run data/audio/input/3BFTio5296w.flac --config data/config/bs_roformer.json --checkpoint data/models/roformer-fp16.pt\n</code></pre> <p> Console output <pre><code>[00:00:41] INFO     using device=device(type='cuda')                                                 __main__.py:111\n           INFO     loading configuration from                                                       __main__.py:113\n                    config_path=PosixPath('data/config/bs_roformer.json')                                           \n           INFO     loading model metadata `BSRoformer` from module `splifft.models.bs_roformer`     __main__.py:126\n[00:00:42] INFO     loading weights from checkpoint_path=PosixPath('data/models/roformer-fp16.pt')   __main__.py:127\n           INFO     processing audio file:                                                           __main__.py:135\n                    mixture_path=PosixPath('data/audio/input/3BFTio5296w.flac')                                     \n\u2819 processing chunks... \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u257a\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501  25% 0:00:10 (bs=4 \u2022 cuda \u2022 float16)\n[00:00:56] INFO     wrote stem `bass` to data/audio/output/3BFTio5296w/bass.flac                     __main__.py:158\n           INFO     wrote stem `drums` to data/audio/output/3BFTio5296w/drums.flac                   __main__.py:158\n           INFO     wrote stem `other` to data/audio/output/3BFTio5296w/other.flac                   __main__.py:158\n[00:00:57] INFO     wrote stem `vocals` to data/audio/output/3BFTio5296w/vocals.flac                 __main__.py:158\n           INFO     wrote stem `guitar` to data/audio/output/3BFTio5296w/guitar.flac                 __main__.py:158\n           INFO     wrote stem `piano` to data/audio/output/3BFTio5296w/piano.flac                   __main__.py:158\n[00:00:58] INFO     wrote stem `instrum` to data/audio/output/3BFTio5296w/instrum.flac               __main__.py:158\n           INFO     wrote stem `drums_and_bass` to data/audio/output/3BFTio5296w/drums_and_bass.flac __main__.py:158\n</code></pre> <p>To update the tool:</p> <pre><code>uv tool upgrade splifft --force-reinstall\n</code></pre>"},{"location":"#faq","title":"FAQ","text":"<p>Where is my <code>config.json</code>, and which one is actually used?</p> <p>Think of two locations:</p> <ol> <li>Built-in templates bundled in the installed package (<code>src/splifft/data/config/*.json</code> in this repo)</li> <li>Your editable copy after <code>splifft pull {model_id}</code></li> </ol> <p><code>splifft run --model {model_id}</code> uses your cached copy:</p> <ul> <li>Linux: <code>~/.cache/splifft/{model_id}/config.json</code></li> <li>macOS: <code>~/Library/Caches/splifft/{model_id}/config.json</code></li> <li>Windows: <code>%LOCALAPPDATA%\\splifft\\Cache\\{model_id}\\config.json</code></li> </ul> <p>What is the difference between <code>--override-config</code> and editing <code>config.json</code>?</p> <ul> <li><code>--override-config \"inference.batch_size=2\"</code> is temporary for that command only.</li> <li>editing <code>config.json</code> is persistent for all future runs.</li> </ul> <p>Use overrides to experiment quickly, then copy stable values into your config.</p> <p>I hit <code>CUDA out of memory</code>.</p> <p>Reduce memory pressure first:</p> <pre><code>splifft run --override-config \"inference.batch_size=2\"\n</code></pre> <p>Then, if you have a GPU and want to use fp16 mixed precision:</p> <pre><code>splifft run \\\n    --override-config \"inference.batch_size=2\" \\\n    --override-config 'inference.use_autocast_dtype=\"float16\"'\n</code></pre> <p>I only want some outputs (for example one stem).</p> <p>Modify <code>inference.requested_stems</code> in the <code>config.json</code> or:</p> <pre><code>splifft run \\\n    --model bs_roformer-fruit-sw \\\n    --override-config 'inference.requested_stems=[\"piano\"]'\n</code></pre> <p>My config suddenly fails validation after an upgrade.</p> <p>Your cached config may be from an older schema. If you want the latest preset config without redownloading checkpoint weights:</p> <pre><code>splifft pull --force-overwrite-config bs_roformer-fruit-sw\n</code></pre> <p>Note that this discards your previous changes!</p> <p>Where do I find the config contract?</p> <ul> <li>API docs: <code>splifft.config.Config</code></li> <li>JSON schema: <code>src/splifft/data/config.schema.json</code></li> </ul> <p>For example, runtime batch size is <code>inference.batch_size</code>.</p> <p>How do I derive custom outputs (e.g. drumless)?</p> <p>Use <code>derived_stems</code> in config (they will be executed in the order you define it), for example:</p> <pre><code>{\n    // ...\n    \"derived_stems\": {\n        \"drumless\": {\n            \"operation\": \"subtract\",\n            \"stem_name\": \"vocals\",\n            \"by_stem_name\": \"mixture\"\n        },\n        \"drums_and_bass\": {\n            \"operation\": \"add\",\n            \"stem_names\": [\"drums\", \"bass\"]\n        }\n    }\n}\n</code></pre>"},{"location":"#library","title":"Library","text":"<p>Add <code>splifft</code> to your project:</p> <pre><code># latest pypi version\nuv add splifft\n# latest bleeding edge\nuv add git+https://github.com/undef13/splifft.git\n</code></pre> <p>This will install the absolutely minimal core dependencies used under the <code>src/splifft/models</code> directory. Higher level components, e.g. inference, training or CLI components must be installed via optional dependencies, as specified in the <code>project.optional-dependencies</code> section of <code>pyproject.toml</code>, for example:</p> <pre><code># enable the built-in configuration, inference and CLI\nuv add \"splifft[config,inference,cli,web]\"\n</code></pre> <p>This will install <code>splifft</code> in your venv.</p>"},{"location":"#development","title":"Development","text":"<p>If you'd like to make local changes, it is recommended to enable all optional and developer group dependencies:</p> <pre><code>git clone https://github.com/undef13/splifft.git\ncd splifft\nuv venv\nuv sync --all-extras --all-groups\n</code></pre> <p>You may also want to use <code>--editable</code> with <code>sync</code>. Check your code:</p> <pre><code># lint &amp; format\njust fmt\n# build &amp; host documentation\njust docs\n</code></pre> <p>Code style:</p> <ul> <li>Use stdlib dataclasses or pydantic BaseModels instead of untyped dictionaries or <code>ConfigDict</code>. This provides static type safety, runtime data validation, IDE autocompletion, and a single, clear source of truth for all parameters.</li> <li>Avoid complex class hierarchies and inheritance. Use plain data structures and pure, stateless functions.</li> <li>Leverage Python's type system and our built-in types (e.g. <code>splifft.types.ChunkSize</code>) to convey intent. It reduces the needs of verbose docstrings.</li> <li>The core should remain agnostic and not contain any model-specific code other than high-level pre/postprocessing archetypes.</li> </ul> <p>PRs are very welcome!</p>"},{"location":"#registry","title":"Registry","text":"<ul> <li>Source of truth: <code>src/splifft/data/registry.json</code></li> <li>Per-model runtime config: <code>src/splifft/data/config/{config_id}.json</code></li> <li>JSON Schema are generated with <code>uv run scripts/gen_schema.py</code>.</li> <li>Validation gate: pydantic (<code>Registry.from_file</code>, <code>Config.from_file</code>)</li> </ul> <p>If you would like to add a model to the <code>splifft</code> registry:</p> <ul> <li>upload checkpoint (ideally to huggingface), with optional MSST config</li> <li>add registry entry: <code>architecture</code>, <code>purpose</code>, <code>config_id</code>, <code>output</code>, <code>resources[]</code></li> <li>write your own config JSON under <code>data/config</code>, or auto-convert your MSST yaml with <code>uv run scripts/community.py fix-registry-with-msst</code></li> <li>optionally, run <code>uv run scripts/community.py fix-registry</code> to auto generate the <code>created_at</code> /<code>model_size</code>/<code>digest</code> fields using HF/GH metadata and sync outputs from configs.</li> <li>format registry JSON: <code>pnpm run fmt:json src/splifft/data/registry.json</code></li> </ul> <p>Right now, registry + configs are shipped in the package itself, with new model visibility inherently tied to package release/version bump. In the future, we may add a <code>splifft update</code> command.</p>"},{"location":"#roadmap","title":"Roadmap","text":"<p><code>splifft</code> is currently optimised for inferencing and does not yet support training.</p> <p>Near term:</p> <ul> <li><code>torch.jit.script</code></li> <li>ONNX export</li> <li><code>coremltools</code></li> <li>support streaming with ring buffer</li> <li>simple web-based GUI with FastAPI and SolidJS.</li> <li>Jupyter notebook</li> </ul> <p>Long term:</p> <ul> <li>evals: SDR, bleedless, fullness, etc.</li> <li>datasets: MUSDB18-HQ, moises</li> <li>implement a complete, configurable training loop</li> <li>data augmentation</li> </ul>"},{"location":"concepts/","title":"Concepts & Definitions","text":""},{"location":"concepts/#introduction","title":"Introduction","text":"<p>In the digital world, sound is captured as a discrete sequence of samples, a representation of the original continuous audio signal \\(x(t)\\). We refer to this time-domain data as a <code>RawAudioTensor</code>. This digital signal is defined by several key parameters: its sample rate, number of channels, bit rate and file format.</p> <p>The full range of human hearing is approximately 20-20000 Hz, so according to the Nyquist-Shannon sampling theorem, the minimum sample rate to accurately represent this range is 40000 Hz. Common values are 44100 Hz (CD quality), 48000 Hz (professional audio), and 16000 Hz (voice).</p>"},{"location":"concepts/#the-separation-pipeline","title":"The Separation Pipeline","text":""},{"location":"concepts/#normalization","title":"Normalization","text":"<p>Neural networks perform best when their input data has a consistent statistical distribution. To prepare the audio for the model, we first normalize it. This process transforms the <code>RawAudioTensor</code> into a <code>NormalizedAudioTensor</code> with a mean of 0 and a standard deviation of 1. The original statistics (\\(\\mu\\) and \\(\\sigma\\)) are stored in <code>NormalizationStats</code> to be used later for denormalization.</p>"},{"location":"concepts/#time-frequency-transformation","title":"Time-Frequency Transformation","text":"<p>Models usually operates not on raw time-domain samples, but in the time-frequency domain, which reveals the signal's frequency content over time. This is achieved via the Short-Time Fourier Transform (STFT), which converts the 1D audio signal into a 2D complex spectrogram.</p>"},{"location":"concepts/#complex-spectrogram","title":"Complex Spectrogram","text":"<p>The STFT coefficient \\(X[m, k]\\) is a complex number that can be decomposed into:</p> <ul> <li>Magnitude \\(|X[m, k]|\\): Tells us \"how much\" of a frequency is present (i.e., its loudness).</li> <li>Phase \\(\\phi(m, k)\\): Tells us \"how it's aligned\" in time. This is notoriously difficult to model, as it chaotically wraps around from \\(-\\pi\\) to \\(\\pi\\). Human hearing is highly sensitive to phase, which is crucial for sound localization and timbre perception.</li> </ul> <p>Practically, the process involves:</p> <ol> <li>Dividing the audio signal into short, overlapping segments in time (chunks), parameterized by the    hop size \\(H\\)</li> <li>Applying a window function \\(w[n]\\) (e.g.    Hann window) to each chunk to reduce spectral leakage</li> <li>Computing the Fast Fourier Transform (FFT) on each windowed segment to get its complex frequency    spectrum. The FFT size \\(N_\\text{fft}\\) determines the number of frequency    bins.</li> <li>Stacking these spectra to form the 2D complex spectrogram.</li> </ol> <p>This is commonly used as the input to models. The objective of source separation is to approximate an ideal ratio mask or its complex equivalent: \\(\\hat{S}_\\text{source} = M_\\text{complex} \\odot S_\\text{mixture}\\).</p>"},{"location":"concepts/#fft-size","title":"FFT Size","text":"<p>The choice of <code>FftSize</code> presents a fundamental trade-off between the uncertainty in time \\(t\\) and frequency \\(f\\): \\(\\sigma_t \\sigma_f \\ge \\frac{1}{4\\pi}\\)</p> <ul> <li>a short window gives good time resolution, excellent for capturing sharp, percussive sounds (transients).</li> <li>a long window gives good frequency resolution, ideal for separating fine harmonics of tonal instruments.</li> </ul> <p>To address this, some loss functions (e.g. <code>auraloss.MultiResolutionSTFTLoss</code>) calculate the error on spectrograms with multiple FFT sizes, forcing the model to optimize for both transient and tonal accuracy.</p>"},{"location":"concepts/#bands","title":"Bands","text":"<p>Instead of processing every frequency bin independently, we can group them into <code>Bands</code>. This reduces computational complexity and allows the model to learn relationships within frequency regions, which often correspond to musical harmonics. Some models use perceptually-motivated scales like the Mel scale, while others like BS-Roformer use a linear frequency scale and learn their own relevant bandings.</p>"},{"location":"concepts/#chunking-and-inference","title":"Chunking and Inference","text":"<p>Since a full audio track is too large for GPU memory, we process it in overlapping segments. The <code>ChunkSize</code> defines the segment length, while the <code>HopSize</code> dictates the step between them, controlled by the <code>OverlapRatio</code>. This process yields a stream of <code>PaddedChunkedAudioTensor</code> batches, which are fed into the model. The model then outputs a corresponding stream of <code>SeparatedChunkedTensor</code>.</p>"},{"location":"concepts/#stitching-and-post-processing","title":"Stitching and Post-processing","text":"<p>After the model processes each chunk, we must reconstruct the full-length audio. The <code>stitch_chunks</code> function implements the overlap-add method, which applies a <code>WindowTensor</code> to each chunk to ensure an artifact-free reconstruction. The final result is a <code>RawSeparatedTensor</code> for each separated stem.</p> <p>With the separated audio back in the time domain, the final steps are to reverse the normalization using the original <code>NormalizationStats</code> and, optionally, to create new stems (e.g., an \"instrumental\" track) using rules defined in <code>DerivedStemsConfig</code>.</p>"},{"location":"models/","title":"Models","text":"<p><code>splifft</code> supports:</p> <ul> <li>Separation: isolate stems (<code>vocals</code>, <code>drums</code>, <code>bass</code>, ...)</li> <li>Sequence labeling: predict frame-wise musical signals (<code>beat</code>, <code>pitch</code>, <code>onset</code>, ...)</li> </ul> <p>Goal: one runtime, one config style, one cache/registry flow. More information on config shape is in <code>splifft.config.Config</code>.</p>"},{"location":"models/#supported-models","title":"Supported models","text":""},{"location":"models/#bs-roformer-mel-roformer","title":"BS-Roformer / Mel-Roformer","text":"<ul> <li><code>config.model</code></li> <li><code>config.model_type = \"bs_roformer\" or \"mel_roformer\"</code></li> </ul> <p>Use this family when separation quality is the top priority.</p> <p>In <code>splifft</code>, Mel-Roformer is represented through the same model family with <code>splifft.models.bs_roformer.MelBandsConfig</code> enabled.</p>"},{"location":"models/#mdx23c","title":"MDX23C","text":"<ul> <li><code>config.model</code></li> <li><code>config.model_type = \"mdx23c\"</code></li> </ul> <p>This is an older architecture that won the Sound Demixing Challenge 2023 Leaderboard C. It is used in some drum separation checkpoints from the community registry.</p>"},{"location":"models/#beat-this","title":"Beat This","text":"<ul> <li><code>config.model</code></li> <li><code>config.model_type = \"beat_this\"</code></li> </ul> <p>Outputs frame-wise beat/downbeat activations (<code>.npy</code>). We intentionally avoid depending on legacy DBN post-processing stacks so inference stays lightweight and reproducible.</p> Why not add DBN? <p>The reference package <code>beat_this</code> depends on <code>madmom.features.downbeats.DBNDownBeatTrackingProcessor</code> as a preprocessing step, which uses:</p> <ul> <li>a state space to represent progression through a measure. for a 4/4 time signature, states represent grid positions (e.g. \"beat 1, 25% through\")</li> <li>transition model encodes tempo (bpm) and continuity, penalising sudden tempo jumps</li> <li>observation model takes the raw logit values (activations) as input probabilties</li> <li>using a Viterbi-like algorithm to find the most likely path through states</li> </ul> <p>However <code>madmom</code> has been abandoned as of 2024-08-25 and so we do not depend on it.</p>"},{"location":"models/#pesto","title":"PESTO","text":"<ul> <li><code>config.model</code></li> <li><code>config.model_type = \"pesto\"</code></li> </ul> <p>Use PESTO when the input is dominated by a single melodic source (voice, lead, solo line). It is designed for stable frame-level F0 tracking.</p> <p>Outputs <code>pitch</code>, <code>confidence</code>, <code>volume</code>, <code>activations</code> as frame sequences (<code>.npy</code>).</p>"},{"location":"models/#basic-pitch-polyphonic-pitch","title":"Basic Pitch (polyphonic pitch)","text":"<ul> <li><code>config.model</code></li> <li><code>config.model_type = \"basic_pitch\"</code></li> </ul> <p>Use Basic Pitch when multiple notes can be active at once. In <code>splifft</code> we intentionally expose raw outputs and do not support MIDI decoding, so downstream applications can choose their own thresholding/hysteresis policy.</p> <p>Outputs <code>onset</code>, <code>note</code>, and <code>contour</code> activation maps (<code>.npy</code>). Except <code>contour</code> (3 bins per semitone), all others have 1 bin per semitone.</p>"},{"location":"models/#visual-comparisons-mvsep","title":"Visual comparisons (MVSep)","text":"<p>The following are quick comparisons for model quality on MVSep (separation-only).</p> InstrumentalVocals <p> 2026-02-17T02:48:54.689043 image/svg+xml Matplotlib v3.10.8, https://matplotlib.org/ 16.5 17.0 17.5 sdr 35 40 45 bleedless 30 35 40 fullness 38 39 40 l1_freq 16.5 17.0 17.5 sdr 35.0 37.5 40.0 42.5 45.0 47.5 bleedless 27.5 30.0 32.5 35.0 37.5 40.0 fullness 0.0 0.2 0.4 0.6 0.8 1.0 l1_freq Metric Correlations for `instrum` 7534: unwa inst v1e (raw) 7573: MelBand Roformer Kim | segment size 2048 | overlap 2 | UVR 5.6.1 7768: Mel-Roformer Instrumental model F (by Gabox) 8257: Inst_GaboxFv8 (updated) 8303: inst fv8b 8362: Logic Pro BS-RoFormer 8393: Inst Fv9 / Bv4? 9475: BS-Roformer Inst HyperACEv2 9505: Inst_GaboxFV10 9580: BS-Roformer Large v2 Inst r2 </p> <p> 2026-02-17T02:49:05.680983 image/svg+xml Matplotlib v3.10.8, https://matplotlib.org/ 10.5 11.0 11.5 sdr 30 35 40 bleedless 15.0 17.5 20.0 22.5 fullness 38.5 39.0 39.5 l1_freq 10.4 10.6 10.8 11.0 11.2 11.4 sdr 30 32 34 36 38 40 bleedless 14 16 18 20 22 fullness 0.0 0.2 0.4 0.6 0.8 1.0 l1_freq Metric Correlations for `vocals` 7475: Mel-RoFormer Kimberley | Unwa's Big Beta 5e(mphasis fullnes) (by unwa) 7706: mel_band_roformer_vocals (by becruily) 8093: Big Beta 6X 8265: BS-Roformer Revive2 8337: BS-Roformer Revive3e 8377: BS\u00b2 9470: BS-Roformer Voc HyperACEv2 9588: gabox </p>"},{"location":"tutorial/","title":"Library Tutorial","text":"<p>This page targets two audiences:</p> <ul> <li>Users shipping features: use the high-level engine and keep config-driven behavior.</li> <li>Researchers/developers: drop to low-level APIs when you need full control over tensors, transforms, and post-processing.</li> </ul>"},{"location":"tutorial/#basic-inference","title":"Basic inference","text":"<p>Use the <code>splifft.inference.InferenceEngine.from_pretrained</code> for a convenient high level API.</p> inference.pyOutput <pre><code>PATH_MIXTURE = \"data/audio/input/3BFTio5296w.flac\"\n\nfrom splifft.inference import InferenceEngine\n\nengine = InferenceEngine.from_pretrained(\n    config=\"/path/to/config.json\",\n    checkpoint_path=\"/path/to/checkpoint.pt\",\n)\nresult = engine.run(PATH_MIXTURE)\nprint(result)\n\n#\n# or, if you use the default user cache registry\n#\n\nengine = InferenceEngine.from_registry(\"bs_roformer-fruit-sw\")\n\n#\n# and to track progress for long files or on slow hardware:\n#\n\nimport logging\n\nfrom splifft.inference import InferenceOutput\n\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(message)s\", datefmt=\"[%X]\")\nlogger = logging.getLogger(__name__)\n\nfor event in engine.stream(PATH_MIXTURE):\n    if isinstance(event, InferenceOutput):\n        print(event)\n        break\n    logger.info(event)\n</code></pre> <pre><code>[00:00:36] Stage.Started(stage='normalize', total_batches=None)\n[00:00:36] Stage.Completed(stage='normalize')\n[00:00:38] ChunkProcessed(batch_index=1, total_batches=9)\n[00:00:39] ChunkProcessed(batch_index=2, total_batches=9)\n[00:00:41] ChunkProcessed(batch_index=3, total_batches=9)\n[00:00:42] ChunkProcessed(batch_index=4, total_batches=9)\n[00:00:43] ChunkProcessed(batch_index=5, total_batches=9)\n[00:00:44] ChunkProcessed(batch_index=6, total_batches=9)\n[00:00:46] ChunkProcessed(batch_index=7, total_batches=9)\n[00:00:47] ChunkProcessed(batch_index=8, total_batches=9)\n[00:00:47] ChunkProcessed(batch_index=9, total_batches=9)\n[00:00:47] Stage.Started(stage='stitch', total_batches=None)\n[00:00:47] Stage.Completed(stage='stitch')\n[00:00:47] Stage.Started(stage='collect_outputs', total_batches=None)\n[00:00:47] Stage.Completed(stage='collect_outputs')\n[00:00:47] Stage.Started(stage='derive_stems', total_batches=None)\n[00:00:47] Stage.Completed(stage='derive_stems')\nInferenceOutput(\n    outputs={\n        'bass': tensor([[-1.3643e-05, -1.3736e-05, -1.3643e-05,  ..., \n-1.3958e-05,\n        -1.3730e-05, -1.3960e-05],\n        [-1.3811e-05, -1.3586e-05, -1.3811e-05,  ..., -1.3738e-05,\n        -1.3953e-05, -1.3736e-05]], device='cuda:0'),\n        'drums': tensor([[-1.3493e-05, -1.4200e-05, -1.3493e-05,  ..., \n-1.2080e-05,\n        -1.2848e-05, -1.2020e-05],\n        [-1.3936e-05, -1.3758e-05, -1.3936e-05,  ..., -1.1843e-05,\n        -1.2818e-05, -1.1989e-05]], device='cuda:0'),\n        'other': tensor([[-7.5168e-07, -6.3413e-07, -7.5222e-07,  ...,  \n1.9690e-05,\n        -3.3400e-05,  2.5086e-05],\n        [-7.4173e-07, -6.7063e-07, -7.4244e-07,  ...,  3.2220e-05,\n        -3.7293e-05,  2.0826e-05]], device='cuda:0'),\n        'vocals': tensor([[-1.3789e-05, -1.3904e-05, -1.3789e-05,  ..., \n-1.3930e-05,\n        -1.3755e-05, -1.4037e-05],\n        [-1.3860e-05, -1.3833e-05, -1.3860e-05,  ..., -1.3848e-05,\n        -1.3747e-05, -1.3913e-05]], device='cuda:0'),\n        'guitar': tensor([[-1.3846e-05, -1.3846e-05, -1.3846e-05,  ..., \n-1.3928e-05,\n        -1.3760e-05, -1.3928e-05],\n        [-1.3910e-05, -1.3782e-05, -1.3910e-05,  ..., -1.3871e-05,\n        -1.3818e-05, -1.3871e-05]], device='cuda:0'),\n        'piano': tensor([[-1.3789e-05, -1.3902e-05, -1.3789e-05,  ..., \n-1.3933e-05,\n        -1.3759e-05, -1.3933e-05],\n        [-1.3881e-05, -1.3810e-05, -1.3881e-05,  ..., -1.3849e-05,\n        -1.3843e-05, -1.3849e-05]], device='cuda:0'),\n        'instrum': tensor([[ 1.3789e-05,  1.3904e-05,  1.3789e-05,  ...,  \n4.7834e-05,\n        -2.5873e-05,  5.2345e-05],\n        [ 1.3860e-05,  1.3833e-05,  1.3860e-05,  ...,  6.8972e-05,\n        -3.0868e-05,  4.3241e-05]], device='cuda:0')\n    },\n    sample_rate=44100\n)\n</code></pre> <p>This outputs <code>splifft.inference.InferenceOutput</code>, containing:</p> <ul> <li>the dictionary of stem names to tensor (which can be audio or logits)</li> <li>the sample rate of the input tensor (so you can save the audio)</li> </ul>"},{"location":"tutorial/#low-level-inference","title":"Low level inference","text":"inference_low_level.py<pre><code># ruff: noqa: E402\nfrom pathlib import Path\n\nimport torch\n\nPATH_CONFIG = Path(\"data/config/bs_roformer.json\")\nPATH_CKPT = Path(\"data/models/roformer-fp16.pt\")\nPATH_MIXTURE = Path(\"data/audio/input/3BFTio5296w.flac\")\n\n# 1. parse + validate a JSON *without* having to import a particular pytorch model.\nfrom splifft.config import Config\n\nconfig = Config.from_file(PATH_CONFIG)\n\n# 2. we now want to *lock in* the configuration to a specific model.\nfrom splifft.models import ModelMetadata\nfrom splifft.models.bs_roformer import BSRoformer, BSRoformerParams\n\nmetadata = ModelMetadata(model_type=\"bs_roformer\", params=BSRoformerParams, model=BSRoformer)\nmodel_params = config.model.to_concrete(metadata.params)\n\n# 3. `metadata` acts as a model builder\nfrom splifft.io import load_weights\n\nmodel = metadata.model(model_params)\nmodel = load_weights(model, PATH_CKPT, device=\"cpu\")\n\n# 4. load audio and run inference by passing dependencies explicitly.\nfrom splifft.inference import InferenceEngine\nfrom splifft.io import read_audio\n\nmixture = read_audio(\n    PATH_MIXTURE,\n    config.audio_io.target_sample_rate,\n    config.audio_io.force_channels,\n)\nengine = InferenceEngine(\n    config=config,\n    model=model,\n    model_params_concrete=model_params,\n    model_device=next(model.parameters()).device,\n    io_device=torch.device(\"cpu\"),\n)\nresult = engine.run(mixture)\n\nprint(list(result.outputs.keys()))\n</code></pre>"},{"location":"tutorial/#extending-splifft","title":"Extending <code>splifft</code>","text":"<p><code>splifft</code> is designed to be easily extended without modifying its core.</p> <p>Make sure you have added <code>splifft</code> as a dependency. Assuming your library has this structure:</p> tree /path/to/ext_project<pre><code>\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 scripts\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 main.py\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 my_library\n        \u2514\u2500\u2500 models\n            \u251c\u2500\u2500 __init__.py\n            \u2514\u2500\u2500 my_model.py\n</code></pre>"},{"location":"tutorial/#1-define-a-new-model","title":"1. Define a new model","text":"<p>Don't do this</p> <p>A common pattern is to define a model with a huge list of parameters in its <code>__init__</code> method:</p> src/my_library/models/my_model.py<pre><code>from torch import nn\nfrom beartype import beartype\n\nclass MyModel(nn.Module):\n    @beartype\n    def __init__(\n        self,\n        chunk_size: int,\n        output_stem_names: tuple[str, ...],\n        # ... a bunch of args here\n    ):\n        ...\n</code></pre> <p>The problem is that it tightly couples the model's implementation to its configuration. Serializing to/from a JSON file and simultaneously supporting static type checking is a headache.</p> <p>Instead, define a stdlib <code>dataclass</code> separate from the model:</p> src/my_library/models/my_model.py<pre><code>from dataclasses import dataclass\n\nfrom torch import nn\n\nfrom splifft.models import ModelParamsLike\nfrom splifft.types import (\n    ChunkSize,\n    InferenceArchetype,\n    ModelInputType,\n    ModelOutputStemName,\n    ModelOutputType,\n)\n\n\n@dataclass\nclass MyModelParams(ModelParamsLike):  # (1)!\n    chunk_size: ChunkSize\n    output_stem_names: tuple[ModelOutputStemName, ...]\n\n    # ... any other config your model needs\n    @property\n    def input_type(self) -&gt; ModelInputType:\n        return \"waveform\"\n\n    @property\n    def output_type(self) -&gt; ModelOutputType:\n        return \"waveform\"\n\n    @property\n    def inference_archetype(self) -&gt; InferenceArchetype:\n        return \"standard_end_to_end\"\n\n\nclass MyModel(nn.Module):\n    def __init__(self, params: MyModelParams):\n        super().__init__()\n        self.params = params\n</code></pre> <ol> <li><code>ModelParamsLike</code> is not a base class to inherit from, but rather a form of structural typing that signals that <code>MyModelParams</code> is compatible with the <code>splifft</code> configuration system. You can remove it if you don't like it.</li> </ol>"},{"location":"tutorial/#2-register-the-model","title":"2. Register the model","text":"<p>With the model and its config defined, our configuration system needs to understand your model.</p> <p>Don't do this</p> <p>A common solution is to define a \"global\" dictionary of available models:</p> src/my_library/models/__init__.py<pre><code>from my_library.models.my_model import MyModelParams, MyModel\n\nMODEL_REGISTRY = {\n    \"my_model\": (MyModel, MyModelParams),\n    # every other model must be added here\n}\n</code></pre> <p>To add a new model, you'd have to modify this central registry. It also forces the import of all models and unwanted dependencies at once.</p> <p>Instead, our configuration system uses a simple <code>ModelMetadata</code> wrapper struct to act as a \"descriptor\" for your model. Create a factory function that defers the imports until its actually needed:</p> src/my_library/models/__init__.py<pre><code>from splifft.models import ModelMetadata\n\n\ndef my_model_metadata():\n    from .my_model import MyModel, MyModelParams\n\n    return ModelMetadata(model_type=\"my_model\", params=MyModelParams, model=MyModel)\n</code></pre> I need to take a user's input string and dynamically import the model. How? <p><code>ModelMetadata.from_module</code> is an alternative way to load the model metadata. It uses importlib under the hood. In fact, our CLI uses this exact approach.</p> <pre><code>from splifft.models import ModelMetadata\n\nmy_model_metadata = ModelMetadata.from_module(\n    module_name=\"my_library.models.my_model\",\n    model_cls_name=\"MyModel\",\n    model_type=\"my_model\"\n)\n</code></pre>"},{"location":"tutorial/#3-putting-everything-together","title":"3. Putting everything together","text":"<p>First, load in the configuration:</p> scripts/main.py<pre><code>from pathlib import Path\n\nimport torch\n\nfrom splifft.config import Config\n\nconfig = Config.from_file(Path(\"path/to/my_model_config.json\"))\n</code></pre> <p>This validates your JSON and returns a pydantic.BaseModel. Note that at this point, <code>config.model</code> is a lazy model configuration that is not yet fully validated.</p> <p>Next, we need to create the PyTorch model. Concretize the lazy model configuration into the <code>dataclass</code> we defined earlier then instantiate the model:</p> scripts/main.py<pre><code>from my_library.models import my_model_metadata\n\nmetadata = my_model_metadata()\nmy_model_params = config.model.to_concrete(metadata.params)\nmodel = metadata.model(my_model_params)\n</code></pre> <p>Finally, load the weights, input audio and run!</p> scripts/main.py<pre><code>from splifft.inference import InferenceEngine\nfrom splifft.io import load_weights, read_audio\n\ncheckpoint_path = Path(\"path/to/my_model.pt\")\nmodel = load_weights(model, checkpoint_path, device=\"cpu\")\n\nmixture = read_audio(\n    Path(\"path/to/mixture.wav\"), config.audio_io.target_sample_rate, config.audio_io.force_channels\n)\nengine = InferenceEngine(\n    config=config,\n    model=model,\n    model_params_concrete=my_model_params,\n    model_device=next(model.parameters()).device,\n    io_device=torch.device(\"cpu\"),\n)\nresult = engine.run(mixture)\n\nprint(f\"{list(result.outputs.keys())=}\")\n</code></pre>"},{"location":"api/","title":"All","text":""},{"location":"api/#splifft","title":"splifft","text":"<p>Lightweight utilities for music source separation.</p> <p>Modules:</p> Name Description <code>__main__</code> <p>Command line interface for <code>splifft</code>.</p> <code>config</code> <p>Configuration</p> <code>core</code> <p>Reusable, pure algorithmic components for inference and training.</p> <code>inference</code> <p>Public inference APIs.</p> <code>io</code> <p>Operations for reading and writing to disk, and network IO.</p> <code>models</code> <p>Source separation models.</p> <code>training</code> <p>High level orchestrator for model training</p> <code>types</code> <p>Types for documentation and data validation (for use in pydantic).</p> <p>Attributes:</p> Name Type Description <code>DIR_MODULE</code> <code>DIR_DATA</code> <code>DIR_CONFIG_DEFAULT</code> <code>PATH_REGISTRY_DEFAULT</code>"},{"location":"api/#splifft.DIR_MODULE","title":"DIR_MODULE  <code>module-attribute</code>","text":"<pre><code>DIR_MODULE = parent\n</code></pre>"},{"location":"api/#splifft.DIR_DATA","title":"DIR_DATA  <code>module-attribute</code>","text":"<pre><code>DIR_DATA = DIR_MODULE / 'data'\n</code></pre>"},{"location":"api/#splifft.DIR_CONFIG_DEFAULT","title":"DIR_CONFIG_DEFAULT  <code>module-attribute</code>","text":"<pre><code>DIR_CONFIG_DEFAULT = DIR_DATA / 'config'\n</code></pre>"},{"location":"api/#splifft.PATH_REGISTRY_DEFAULT","title":"PATH_REGISTRY_DEFAULT  <code>module-attribute</code>","text":"<pre><code>PATH_REGISTRY_DEFAULT = DIR_DATA / 'registry.json'\n</code></pre>"},{"location":"api/config/","title":"Config","text":""},{"location":"api/config/#splifft.config","title":"config","text":"<p>Configuration</p> <p>Classes:</p> Name Description <code>LazyModelConfig</code> <p>A lazily validated model configuration.</p> <code>StftConfig</code> <p>configuration for the short-time fourier transform.</p> <code>LogMelConfig</code> <p>Configuration for Log-Mel Spectrogram.</p> <code>CqtConfig</code> <p>Configuration for harmonic CQT feature extraction.</p> <code>AudioIOConfig</code> <code>TorchCompileConfig</code> <code>InferenceConfig</code> <code>NormalizationConfig</code> <code>WaveformChunkingConfig</code> <code>SequenceChunkingConfig</code> <code>MaskingConfig</code> <code>SubtractConfig</code> <code>SumConfig</code> <code>OutputConfig</code> <code>ConfigOverrideError</code> <p>Raised when one or more override strings are syntactically invalid.</p> <code>Config</code> <code>Model</code> <code>Metrics</code> <code>Resource</code> <code>Comment</code> <code>Registry</code> <p>Functions:</p> Name Description <code>parse_override_value</code> <p>Parse a CLI override value into a Python object.</p> <code>parse_config_override</code> <p>Parse one override entry of the form <code>&lt;dot.path&gt;=&lt;value&gt;</code>. Empty value</p> <code>set_path_value</code> <p>Set a nested value in-place, creating missing dictionaries as needed.</p> <code>apply_config_overrides</code> <code>load_config_dict</code> <code>into_config</code> <p>Convert various config inputs into a validated [<code>Config</code>].</p> <p>Attributes:</p> Name Type Description <code>TorchDtype</code> <code>TypeAlias</code> <code>Tuple</code> <code>NonEmptyUnique</code> <code>ModelInputStemName</code> <code>TypeAlias</code> <code>ModelOutputStemName</code> <code>TypeAlias</code> <code>FeatureExtractionConfig</code> <code>TypeAlias</code> <code>DerivedStemName</code> <code>TypeAlias</code> <p>The name of a derived stem, e.g. <code>vocals_minus_drums</code>.</p> <code>StemName</code> <code>TypeAlias</code> <p>A name of a stem, either a model output stem or a derived stem.</p> <code>DerivedStemRule</code> <code>TypeAlias</code> <code>DerivedStemsConfig</code> <code>TypeAlias</code> <code>ConfigOverrides</code> <code>TypeAlias</code> <p><code>&lt;dot.path&gt;=&lt;value&gt;</code> form, e.g. <code>inference.batch_size=2</code></p> <code>IntoConfig</code> <code>TypeAlias</code>"},{"location":"api/config/#splifft.config.TorchDtype","title":"TorchDtype  <code>module-attribute</code>","text":"<pre><code>TorchDtype: TypeAlias = Annotated[\n    dtype, GetPydanticSchema(_get_torch_dtype_schema)\n]\n</code></pre>"},{"location":"api/config/#splifft.config.Tuple","title":"Tuple  <code>module-attribute</code>","text":"<pre><code>Tuple = Annotated[\n    tuple[_Item, ...], BeforeValidator(_to_tuple)\n]\n</code></pre>"},{"location":"api/config/#splifft.config.NonEmptyUnique","title":"NonEmptyUnique  <code>module-attribute</code>","text":"<pre><code>NonEmptyUnique = Annotated[\n    _S,\n    Len(min_length=1),\n    AfterValidator(_validate_unique_sequence),\n    Field(json_schema_extra={\"unique_items\": True}),\n]\n</code></pre>"},{"location":"api/config/#splifft.config.ModelInputStemName","title":"ModelInputStemName  <code>module-attribute</code>","text":"<pre><code>ModelInputStemName: TypeAlias = Literal['mixture']\n</code></pre>"},{"location":"api/config/#splifft.config.ModelOutputStemName","title":"ModelOutputStemName  <code>module-attribute</code>","text":"<pre><code>ModelOutputStemName: TypeAlias = Annotated[\n    ModelOutputStemName, StringConstraints(min_length=1)\n]\n</code></pre>"},{"location":"api/config/#splifft.config.LazyModelConfig","title":"LazyModelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>A lazily validated model configuration.</p> <p>Note that it is not guaranteed to be fully valid until <code>to_concrete</code> is called.</p> <p>Methods:</p> Name Description <code>to_concrete</code> <p>Validate against a real set of model parameters and convert to it.</p> <p>Attributes:</p> Name Type Description <code>chunk_size</code> <code>ChunkSize</code> <code>output_stem_names</code> <code>NonEmptyUnique[Tuple[ModelOutputStemName]]</code> <code>stem_names</code> <code>tuple[ModelInputStemName | ModelOutputStemName, ...]</code> <p>Returns the model's input and output stem names.</p> <code>model_config</code>"},{"location":"api/config/#splifft.config.LazyModelConfig.chunk_size","title":"chunk_size  <code>instance-attribute</code>","text":"<pre><code>chunk_size: ChunkSize\n</code></pre>"},{"location":"api/config/#splifft.config.LazyModelConfig.output_stem_names","title":"output_stem_names  <code>instance-attribute</code>","text":"<pre><code>output_stem_names: NonEmptyUnique[\n    Tuple[ModelOutputStemName]\n]\n</code></pre>"},{"location":"api/config/#splifft.config.LazyModelConfig.to_concrete","title":"to_concrete","text":"<pre><code>to_concrete(\n    model_params: type[ModelParamsLikeT],\n    *,\n    pydantic_config: ConfigDict = ConfigDict(\n        extra=\"forbid\"\n    ),\n) -&gt; ModelParamsLikeT\n</code></pre> <p>Validate against a real set of model parameters and convert to it.</p> <p>Raises:</p> Type Description <code>pydantic.ValidationError</code> <p>if extra fields are present in the model parameters that doesn't exist in the concrete model parameters.</p> Source code in <code>src/splifft/config.py</code> <pre><code>def to_concrete(\n    self,\n    model_params: type[ModelParamsLikeT],\n    *,\n    pydantic_config: ConfigDict = ConfigDict(extra=\"forbid\"),\n) -&gt; ModelParamsLikeT:\n    \"\"\"Validate against a real set of model parameters and convert to it.\n\n    :raises pydantic.ValidationError: if extra fields are present in the model parameters\n        that doesn't exist in the concrete model parameters.\n    \"\"\"\n    # input_type and output_type are inconfigurable anyway\n    # TODO: use lru cache to avoid recreating the TypeAdapter in a hot loop but dict isn't hashable\n    ta = TypeAdapter(\n        type(\n            f\"{model_params.__name__}Validator\",\n            (model_params,),\n            {\"__pydantic_config__\": pydantic_config},\n        )  # needed for https://docs.pydantic.dev/latest/errors/usage_errors/#type-adapter-config-unused\n    )  # type: ignore\n    # types defined within `TYPE_CHECKING` blocks will be forward references, so we need rebuild\n    ta.rebuild(_types_namespace={\"TorchDtype\": TorchDtype, \"t\": t})\n    model_params_concrete: ModelParamsLikeT = ta.validate_python(self.model_dump())\n    return model_params_concrete\n</code></pre>"},{"location":"api/config/#splifft.config.LazyModelConfig.stem_names","title":"stem_names  <code>property</code>","text":"<pre><code>stem_names: tuple[\n    ModelInputStemName | ModelOutputStemName, ...\n]\n</code></pre> <p>Returns the model's input and output stem names.</p>"},{"location":"api/config/#splifft.config.LazyModelConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = ConfigDict(strict=True, extra='allow')\n</code></pre>"},{"location":"api/config/#splifft.config.StftConfig","title":"StftConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>configuration for the short-time fourier transform.</p> <p>Attributes:</p> Name Type Description <code>kind</code> <code>Literal['stft']</code> <code>n_fft</code> <code>FftSize</code> <code>hop_length</code> <code>HopSize</code> <code>win_length</code> <code>FftSize</code> <code>window_shape</code> <code>WindowShape</code> <code>normalized</code> <code>bool</code> <code>conv_dtype</code> <code>TorchDtype | None</code> <p>The data type used for the <code>conv1d</code> buffers.</p> <code>model_config</code>"},{"location":"api/config/#splifft.config.StftConfig.kind","title":"kind  <code>instance-attribute</code>","text":"<pre><code>kind: Literal['stft']\n</code></pre>"},{"location":"api/config/#splifft.config.StftConfig.n_fft","title":"n_fft  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_fft: FftSize = 2048\n</code></pre>"},{"location":"api/config/#splifft.config.StftConfig.hop_length","title":"hop_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>hop_length: HopSize = 512\n</code></pre>"},{"location":"api/config/#splifft.config.StftConfig.win_length","title":"win_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>win_length: FftSize = 2048\n</code></pre>"},{"location":"api/config/#splifft.config.StftConfig.window_shape","title":"window_shape  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>window_shape: WindowShape = 'hann'\n</code></pre>"},{"location":"api/config/#splifft.config.StftConfig.normalized","title":"normalized  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>normalized: bool = False\n</code></pre>"},{"location":"api/config/#splifft.config.StftConfig.conv_dtype","title":"conv_dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>conv_dtype: TorchDtype | None = None\n</code></pre> <p>The data type used for the <code>conv1d</code> buffers.</p>"},{"location":"api/config/#splifft.config.StftConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.LogMelConfig","title":"LogMelConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Log-Mel Spectrogram.</p> <p>Attributes:</p> Name Type Description <code>kind</code> <code>Literal['mel']</code> <code>n_fft</code> <code>FftSize</code> <code>hop_length</code> <code>HopSize</code> <code>n_mels</code> <code>Gt0[int]</code> <code>sample_rate</code> <code>SampleRate</code> <code>f_min</code> <code>float</code> <code>f_max</code> <code>float | None</code> <code>mel_scale</code> <code>Literal['htk', 'slaney']</code> <code>normalized</code> <code>bool | Literal['frame_length']</code> <code>power</code> <code>float</code> <code>log_multiplier</code> <code>float</code> <code>model_config</code>"},{"location":"api/config/#splifft.config.LogMelConfig.kind","title":"kind  <code>instance-attribute</code>","text":"<pre><code>kind: Literal['mel']\n</code></pre>"},{"location":"api/config/#splifft.config.LogMelConfig.n_fft","title":"n_fft  <code>instance-attribute</code>","text":"<pre><code>n_fft: FftSize\n</code></pre>"},{"location":"api/config/#splifft.config.LogMelConfig.hop_length","title":"hop_length  <code>instance-attribute</code>","text":"<pre><code>hop_length: HopSize\n</code></pre>"},{"location":"api/config/#splifft.config.LogMelConfig.n_mels","title":"n_mels  <code>instance-attribute</code>","text":"<pre><code>n_mels: Gt0[int]\n</code></pre>"},{"location":"api/config/#splifft.config.LogMelConfig.sample_rate","title":"sample_rate  <code>instance-attribute</code>","text":"<pre><code>sample_rate: SampleRate\n</code></pre>"},{"location":"api/config/#splifft.config.LogMelConfig.f_min","title":"f_min  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>f_min: float = 0.0\n</code></pre>"},{"location":"api/config/#splifft.config.LogMelConfig.f_max","title":"f_max  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>f_max: float | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.LogMelConfig.mel_scale","title":"mel_scale  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mel_scale: Literal['htk', 'slaney'] = 'slaney'\n</code></pre>"},{"location":"api/config/#splifft.config.LogMelConfig.normalized","title":"normalized  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>normalized: bool | Literal['frame_length'] = 'frame_length'\n</code></pre>"},{"location":"api/config/#splifft.config.LogMelConfig.power","title":"power  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>power: float = 1.0\n</code></pre>"},{"location":"api/config/#splifft.config.LogMelConfig.log_multiplier","title":"log_multiplier  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_multiplier: float = 1000.0\n</code></pre>"},{"location":"api/config/#splifft.config.LogMelConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.CqtConfig","title":"CqtConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for harmonic CQT feature extraction.</p> <p>This intentionally matches PESTO's HCQT preprocessor defaults so we can share checkpoints without adding SciPy or nnAudio.</p> <p>Attributes:</p> Name Type Description <code>kind</code> <code>Literal['cqt']</code> <code>hop_size_ms</code> <code>float</code> <code>harmonics</code> <code>NonEmptyUnique[Tuple[Gt0[int]]]</code> <code>fmin</code> <code>float</code> <code>fmax</code> <code>float | None</code> <code>bins_per_semitone</code> <code>Gt0[int]</code> <code>n_bins</code> <code>Gt0[int]</code> <code>center_bins</code> <code>bool</code> <code>gamma</code> <code>float</code> <code>center</code> <code>bool</code> <code>log_epsilon</code> <code>float</code> <code>model_config</code>"},{"location":"api/config/#splifft.config.CqtConfig.kind","title":"kind  <code>instance-attribute</code>","text":"<pre><code>kind: Literal['cqt']\n</code></pre>"},{"location":"api/config/#splifft.config.CqtConfig.hop_size_ms","title":"hop_size_ms  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>hop_size_ms: float = 10.0\n</code></pre>"},{"location":"api/config/#splifft.config.CqtConfig.harmonics","title":"harmonics  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>harmonics: NonEmptyUnique[Tuple[Gt0[int]]] = (1,)\n</code></pre>"},{"location":"api/config/#splifft.config.CqtConfig.fmin","title":"fmin  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fmin: float = 27.5\n</code></pre>"},{"location":"api/config/#splifft.config.CqtConfig.fmax","title":"fmax  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fmax: float | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.CqtConfig.bins_per_semitone","title":"bins_per_semitone  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bins_per_semitone: Gt0[int] = 3\n</code></pre>"},{"location":"api/config/#splifft.config.CqtConfig.n_bins","title":"n_bins  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_bins: Gt0[int] = 251\n</code></pre>"},{"location":"api/config/#splifft.config.CqtConfig.center_bins","title":"center_bins  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>center_bins: bool = True\n</code></pre>"},{"location":"api/config/#splifft.config.CqtConfig.gamma","title":"gamma  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>gamma: float = 7.0\n</code></pre>"},{"location":"api/config/#splifft.config.CqtConfig.center","title":"center  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>center: bool = True\n</code></pre>"},{"location":"api/config/#splifft.config.CqtConfig.log_epsilon","title":"log_epsilon  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_epsilon: float = 1e-08\n</code></pre>"},{"location":"api/config/#splifft.config.CqtConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.FeatureExtractionConfig","title":"FeatureExtractionConfig  <code>module-attribute</code>","text":"<pre><code>FeatureExtractionConfig: TypeAlias = Annotated[\n    Union[StftConfig, LogMelConfig, CqtConfig],\n    Discriminator(\"kind\"),\n]\n</code></pre>"},{"location":"api/config/#splifft.config.AudioIOConfig","title":"AudioIOConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>target_sample_rate</code> <code>SampleRate</code> <code>force_channels</code> <code>Channels | None</code> <p>Whether to force mono or stereo audio input. If None, keep original.</p> <code>model_config</code>"},{"location":"api/config/#splifft.config.AudioIOConfig.target_sample_rate","title":"target_sample_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>target_sample_rate: SampleRate = 44100\n</code></pre>"},{"location":"api/config/#splifft.config.AudioIOConfig.force_channels","title":"force_channels  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>force_channels: Channels | None = 2\n</code></pre> <p>Whether to force mono or stereo audio input. If None, keep original.</p>"},{"location":"api/config/#splifft.config.AudioIOConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.TorchCompileConfig","title":"TorchCompileConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>fullgraph</code> <code>bool</code> <code>dynamic</code> <code>bool</code> <code>mode</code> <code>Literal['default', 'reduce-overhead', 'max-autotune', 'max-autotune-no-cudagraphs']</code>"},{"location":"api/config/#splifft.config.TorchCompileConfig.fullgraph","title":"fullgraph  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fullgraph: bool = True\n</code></pre>"},{"location":"api/config/#splifft.config.TorchCompileConfig.dynamic","title":"dynamic  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dynamic: bool = True\n</code></pre>"},{"location":"api/config/#splifft.config.TorchCompileConfig.mode","title":"mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mode: Literal[\n    \"default\",\n    \"reduce-overhead\",\n    \"max-autotune\",\n    \"max-autotune-no-cudagraphs\",\n] = \"reduce-overhead\"\n</code></pre>"},{"location":"api/config/#splifft.config.InferenceConfig","title":"InferenceConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>BatchSize</code> <code>requested_stems</code> <code>NonEmptyUnique[Tuple[ModelOutputStemName]] | None</code> <p>Optional subset of model output stems to compute and emit.</p> <code>force_weights_dtype</code> <code>TorchDtype | None</code> <code>use_autocast_dtype</code> <code>TorchDtype | None</code> <code>model_device</code> <code>str | None</code> <p>Device used for model forward execution.</p> <code>io_device</code> <code>str | None</code> <p>Device for audio IO and non-model tensor ops (chunking/stitching/output/normalization).</p> <code>compile_model</code> <code>TorchCompileConfig | None</code> <code>model_config</code>"},{"location":"api/config/#splifft.config.InferenceConfig.batch_size","title":"batch_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_size: BatchSize = 8\n</code></pre>"},{"location":"api/config/#splifft.config.InferenceConfig.requested_stems","title":"requested_stems  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>requested_stems: (\n    NonEmptyUnique[Tuple[ModelOutputStemName]] | None\n) = None\n</code></pre> <p>Optional subset of model output stems to compute and emit.</p> <p>When provided, inference only computes/keeps these model outputs in the configured order. Depending on the model architecture, this may allow specific stem-specific weights to be not loaded at all.</p> <p>For example, the BS Roformer architecture has a shared backbone followed by multiple MLP heads, so setting this parameter can effectively patch out the unrelated heads.</p>"},{"location":"api/config/#splifft.config.InferenceConfig.force_weights_dtype","title":"force_weights_dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>force_weights_dtype: TorchDtype | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.InferenceConfig.use_autocast_dtype","title":"use_autocast_dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_autocast_dtype: TorchDtype | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.InferenceConfig.model_device","title":"model_device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_device: str | None = None\n</code></pre> <p>Device used for model forward execution.</p> <p>If <code>None</code>, runtime auto-selects <code>cuda</code> when available, otherwise <code>cpu</code>.</p>"},{"location":"api/config/#splifft.config.InferenceConfig.io_device","title":"io_device  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>io_device: str | None = None\n</code></pre> <p>Device for audio IO and non-model tensor ops (chunking/stitching/output/normalization).</p> <p>If <code>None</code>, runtime auto-selects <code>cuda</code> when available, otherwise <code>cpu</code>.</p> <p>Note that switching <code>io_device</code> to CPU is expected to break bit-identical parity vs <code>io_device=\"cuda\"</code> even with identical weights/seeds.</p> <ul> <li><code>core.normalize_audio</code> output/stat-mean (tiny CPU vs CUDA reduction drift)</li> <li><code>core._get_window_fn</code> (tiny kernel-level float differences)</li> <li><code>core.generate_chunks</code> and <code>core.stitch_chunks</code> may then differ bitwise</li> </ul>"},{"location":"api/config/#splifft.config.InferenceConfig.compile_model","title":"compile_model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>compile_model: TorchCompileConfig | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.InferenceConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.NormalizationConfig","title":"NormalizationConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>enabled</code> <code>bool</code> <code>model_config</code>"},{"location":"api/config/#splifft.config.NormalizationConfig.enabled","title":"enabled  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>enabled: bool = False\n</code></pre>"},{"location":"api/config/#splifft.config.NormalizationConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.WaveformChunkingConfig","title":"WaveformChunkingConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>method</code> <code>Literal['overlap_add_windowed']</code> <code>overlap_ratio</code> <code>OverlapRatio</code> <code>window_shape</code> <code>WindowShape</code> <code>padding_mode</code> <code>PaddingMode</code> <code>model_config</code>"},{"location":"api/config/#splifft.config.WaveformChunkingConfig.method","title":"method  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>method: Literal[\"overlap_add_windowed\"] = (\n    \"overlap_add_windowed\"\n)\n</code></pre>"},{"location":"api/config/#splifft.config.WaveformChunkingConfig.overlap_ratio","title":"overlap_ratio  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap_ratio: OverlapRatio = 0.5\n</code></pre>"},{"location":"api/config/#splifft.config.WaveformChunkingConfig.window_shape","title":"window_shape  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>window_shape: WindowShape = 'hann'\n</code></pre>"},{"location":"api/config/#splifft.config.WaveformChunkingConfig.padding_mode","title":"padding_mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>padding_mode: PaddingMode = 'reflect'\n</code></pre>"},{"location":"api/config/#splifft.config.WaveformChunkingConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.SequenceChunkingConfig","title":"SequenceChunkingConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>trim_margin</code> <code>TrimMargin</code> <code>overlap_mode</code> <code>OverlapMode</code> <code>avoid_short_end</code> <code>bool</code> <code>model_config</code>"},{"location":"api/config/#splifft.config.SequenceChunkingConfig.trim_margin","title":"trim_margin  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trim_margin: TrimMargin = 0\n</code></pre>"},{"location":"api/config/#splifft.config.SequenceChunkingConfig.overlap_mode","title":"overlap_mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overlap_mode: OverlapMode = 'keep_first'\n</code></pre>"},{"location":"api/config/#splifft.config.SequenceChunkingConfig.avoid_short_end","title":"avoid_short_end  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>avoid_short_end: bool = True\n</code></pre>"},{"location":"api/config/#splifft.config.SequenceChunkingConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.MaskingConfig","title":"MaskingConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>add_sub_dtype</code> <code>TorchDtype | None</code> <code>out_dtype</code> <code>TorchDtype | None</code> <code>model_config</code>"},{"location":"api/config/#splifft.config.MaskingConfig.add_sub_dtype","title":"add_sub_dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>add_sub_dtype: TorchDtype | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.MaskingConfig.out_dtype","title":"out_dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>out_dtype: TorchDtype | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.MaskingConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.DerivedStemName","title":"DerivedStemName  <code>module-attribute</code>","text":"<pre><code>DerivedStemName: TypeAlias = Annotated[\n    str, StringConstraints(min_length=1)\n]\n</code></pre> <p>The name of a derived stem, e.g. <code>vocals_minus_drums</code>.</p>"},{"location":"api/config/#splifft.config.StemName","title":"StemName  <code>module-attribute</code>","text":"<pre><code>StemName: TypeAlias = Union[\n    ModelOutputStemName, DerivedStemName\n]\n</code></pre> <p>A name of a stem, either a model output stem or a derived stem.</p>"},{"location":"api/config/#splifft.config.SubtractConfig","title":"SubtractConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>operation</code> <code>Literal['subtract']</code> <code>stem_name</code> <code>StemName</code> <code>by_stem_name</code> <code>StemName</code> <code>model_config</code>"},{"location":"api/config/#splifft.config.SubtractConfig.operation","title":"operation  <code>instance-attribute</code>","text":"<pre><code>operation: Literal['subtract']\n</code></pre>"},{"location":"api/config/#splifft.config.SubtractConfig.stem_name","title":"stem_name  <code>instance-attribute</code>","text":"<pre><code>stem_name: StemName\n</code></pre>"},{"location":"api/config/#splifft.config.SubtractConfig.by_stem_name","title":"by_stem_name  <code>instance-attribute</code>","text":"<pre><code>by_stem_name: StemName\n</code></pre>"},{"location":"api/config/#splifft.config.SubtractConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.SumConfig","title":"SumConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>operation</code> <code>Literal['sum']</code> <code>stem_names</code> <code>NonEmptyUnique[Tuple[StemName]]</code> <code>model_config</code>"},{"location":"api/config/#splifft.config.SumConfig.operation","title":"operation  <code>instance-attribute</code>","text":"<pre><code>operation: Literal['sum']\n</code></pre>"},{"location":"api/config/#splifft.config.SumConfig.stem_names","title":"stem_names  <code>instance-attribute</code>","text":"<pre><code>stem_names: NonEmptyUnique[Tuple[StemName]]\n</code></pre>"},{"location":"api/config/#splifft.config.SumConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.DerivedStemRule","title":"DerivedStemRule  <code>module-attribute</code>","text":"<pre><code>DerivedStemRule: TypeAlias = Annotated[\n    Union[SubtractConfig, SumConfig],\n    Discriminator(\"operation\"),\n]\n</code></pre>"},{"location":"api/config/#splifft.config.DerivedStemsConfig","title":"DerivedStemsConfig  <code>module-attribute</code>","text":"<pre><code>DerivedStemsConfig: TypeAlias = dict[\n    DerivedStemName, DerivedStemRule\n]\n</code></pre>"},{"location":"api/config/#splifft.config.OutputConfig","title":"OutputConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>stem_names</code> <code>Literal['all'] | NonEmptyUnique[Tuple[StemName]]</code> <code>file_format</code> <code>FileFormat</code> <code>bit_rate</code> <code>BitRate | None</code> <p>Output bit rate for lossy formats. The default is chosen by FFmpeg.</p> <code>model_config</code>"},{"location":"api/config/#splifft.config.OutputConfig.stem_names","title":"stem_names  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stem_names: (\n    Literal[\"all\"] | NonEmptyUnique[Tuple[StemName]]\n) = \"all\"\n</code></pre>"},{"location":"api/config/#splifft.config.OutputConfig.file_format","title":"file_format  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>file_format: FileFormat = 'flac'\n</code></pre>"},{"location":"api/config/#splifft.config.OutputConfig.bit_rate","title":"bit_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bit_rate: BitRate | None = None\n</code></pre> <p>Output bit rate for lossy formats. The default is chosen by FFmpeg.</p>"},{"location":"api/config/#splifft.config.OutputConfig.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = _PYDANTIC_STRICT_CONFIG\n</code></pre>"},{"location":"api/config/#splifft.config.ConfigOverrideError","title":"ConfigOverrideError","text":"<p>               Bases: <code>ValueError</code></p> <p>Raised when one or more override strings are syntactically invalid.</p>"},{"location":"api/config/#splifft.config.parse_override_value","title":"parse_override_value","text":"<pre><code>parse_override_value(value: str) -&gt; Any\n</code></pre> <p>Parse a CLI override value into a Python object.</p> <p>Shell-like quoting/escaping semantics beyond what your shell and JSON provide are not supported.</p> Source code in <code>src/splifft/config.py</code> <pre><code>def parse_override_value(value: str) -&gt; Any:\n    \"\"\"Parse a CLI override value into a Python object.\n\n    Shell-like quoting/escaping semantics beyond what your shell and JSON provide\n    are not supported.\n    \"\"\"\n    try:\n        return json.loads(value)\n    except json.JSONDecodeError:\n        return value\n</code></pre>"},{"location":"api/config/#splifft.config.parse_config_override","title":"parse_config_override","text":"<pre><code>parse_config_override(\n    override: str,\n) -&gt; tuple[tuple[str, ...], Any]\n</code></pre> <p>Parse one override entry of the form <code>&lt;dot.path&gt;=&lt;value&gt;</code>. Empty value is interpreted as an empty string.</p> <p>Missing <code>=</code> or empty path segments like <code>a..b=1</code> or <code>.a=1</code> are considered syntax errors</p> Source code in <code>src/splifft/config.py</code> <pre><code>def parse_config_override(override: str) -&gt; tuple[tuple[str, ...], Any]:\n    \"\"\"Parse one override entry of the form `&lt;dot.path&gt;=&lt;value&gt;`. Empty value\n    is interpreted as an empty string.\n\n    Missing `=` or empty path segments like `a..b=1` or `.a=1` are considered syntax errors\n    \"\"\"\n    key, sep, raw_value = override.partition(\"=\")\n    if sep == \"\":\n        raise ConfigOverrideError(\n            \"invalid override `\"\n            f\"{override}`: expected `&lt;dot.path&gt;=&lt;value&gt;`, e.g. `inference.batch_size=2`\"\n        )\n\n    path = tuple(part.strip() for part in key.split(\".\"))\n    if not path or any(part == \"\" for part in path):\n        raise ConfigOverrideError(\n            f\"invalid override path `{key}` in `{override}`: empty path segment is not allowed\"\n        )\n\n    return path, parse_override_value(raw_value)\n</code></pre>"},{"location":"api/config/#splifft.config.set_path_value","title":"set_path_value","text":"<pre><code>set_path_value(\n    mut_config_dict: dict[str, Any],\n    path: tuple[str, ...],\n    value: Any,\n) -&gt; None\n</code></pre> <p>Set a nested value in-place, creating missing dictionaries as needed.</p> Source code in <code>src/splifft/config.py</code> <pre><code>def set_path_value(mut_config_dict: dict[str, Any], path: tuple[str, ...], value: Any) -&gt; None:\n    \"\"\"Set a nested value in-place, creating missing dictionaries as needed.\"\"\"\n    current = mut_config_dict\n    for key in path[:-1]:\n        next_node = current.get(key)\n        if not isinstance(next_node, dict):\n            next_node = {}\n            current[key] = next_node\n        current = next_node\n\n    current[path[-1]] = value\n</code></pre>"},{"location":"api/config/#splifft.config.ConfigOverrides","title":"ConfigOverrides  <code>module-attribute</code>","text":"<pre><code>ConfigOverrides: TypeAlias = Sequence[str]\n</code></pre> <p><code>&lt;dot.path&gt;=&lt;value&gt;</code> form, e.g. <code>inference.batch_size=2</code></p> <ul> <li>automatic creation of missing nested dictionaries while applying overrides</li> <li>validation is performed after all overrides are applied</li> </ul> <p>Not supported: list index addressing in paths (e.g. <code>a.0.b=1</code> is treated as string keys)</p>"},{"location":"api/config/#splifft.config.apply_config_overrides","title":"apply_config_overrides","text":"<pre><code>apply_config_overrides(\n    mut_config_dict: dict[str, Any],\n    overrides: ConfigOverrides,\n) -&gt; dict[str, Any]\n</code></pre> Source code in <code>src/splifft/config.py</code> <pre><code>def apply_config_overrides(\n    mut_config_dict: dict[str, Any], overrides: ConfigOverrides\n) -&gt; dict[str, Any]:\n    for override in overrides:\n        path, value = parse_config_override(override)\n        set_path_value(mut_config_dict, path, value)\n    return mut_config_dict\n</code></pre>"},{"location":"api/config/#splifft.config.load_config_dict","title":"load_config_dict","text":"<pre><code>load_config_dict(\n    path: StrPath | BytesPath,\n) -&gt; dict[str, Any]\n</code></pre> Source code in <code>src/splifft/config.py</code> <pre><code>def load_config_dict(path: t.StrPath | t.BytesPath) -&gt; dict[str, Any]:\n    with open(path, \"rb\") as f:\n        data = json.load(f)\n    if not isinstance(data, dict):\n        raise TypeError(f\"expected top-level JSON object in config file, got {type(data).__name__}\")\n    return data\n</code></pre>"},{"location":"api/config/#splifft.config.Config","title":"Config","text":"<p>               Bases: <code>BaseModel</code></p> <p>Methods:</p> Name Description <code>check_derived_stems</code> <code>check_requested_stems</code> <code>validate_inference_contract</code> <code>from_file</code> <p>Load config JSON from disk, optionally applying CLI-style overrides.</p> <p>Attributes:</p> Name Type Description <code>identifier</code> <code>str</code> <p>Unique identifier for this configuration</p> <code>model_type</code> <code>ModelType</code> <code>model</code> <code>LazyModelConfig</code> <code>transform</code> <code>FeatureExtractionConfig | None</code> <code>audio_io</code> <code>AudioIOConfig</code> <code>inference</code> <code>InferenceConfig</code> <code>normalization</code> <code>NormalizationConfig</code> <code>waveform_chunking</code> <code>WaveformChunkingConfig | None</code> <code>sequence_chunking</code> <code>SequenceChunkingConfig | None</code> <code>masking</code> <code>MaskingConfig</code> <code>derived_stems</code> <code>DerivedStemsConfig | None</code> <code>output</code> <code>OutputConfig</code> <code>experimental</code> <code>dict[str, Any] | None</code> <p>Any extra experimental configurations outside of the <code>splifft</code> core.</p> <code>model_config</code>"},{"location":"api/config/#splifft.config.Config.identifier","title":"identifier  <code>instance-attribute</code>","text":"<pre><code>identifier: str\n</code></pre> <p>Unique identifier for this configuration</p>"},{"location":"api/config/#splifft.config.Config.model_type","title":"model_type  <code>instance-attribute</code>","text":"<pre><code>model_type: ModelType\n</code></pre>"},{"location":"api/config/#splifft.config.Config.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: LazyModelConfig\n</code></pre>"},{"location":"api/config/#splifft.config.Config.transform","title":"transform  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>transform: FeatureExtractionConfig | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.Config.audio_io","title":"audio_io  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>audio_io: AudioIOConfig = Field(\n    default_factory=AudioIOConfig\n)\n</code></pre>"},{"location":"api/config/#splifft.config.Config.inference","title":"inference  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>inference: InferenceConfig = Field(\n    default_factory=InferenceConfig\n)\n</code></pre>"},{"location":"api/config/#splifft.config.Config.normalization","title":"normalization  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>normalization: NormalizationConfig = Field(\n    default_factory=NormalizationConfig\n)\n</code></pre>"},{"location":"api/config/#splifft.config.Config.waveform_chunking","title":"waveform_chunking  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>waveform_chunking: WaveformChunkingConfig | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.Config.sequence_chunking","title":"sequence_chunking  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sequence_chunking: SequenceChunkingConfig | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.Config.masking","title":"masking  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>masking: MaskingConfig = Field(\n    default_factory=MaskingConfig\n)\n</code></pre>"},{"location":"api/config/#splifft.config.Config.derived_stems","title":"derived_stems  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>derived_stems: DerivedStemsConfig | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.Config.output","title":"output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output: OutputConfig = Field(default_factory=OutputConfig)\n</code></pre>"},{"location":"api/config/#splifft.config.Config.experimental","title":"experimental  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>experimental: dict[str, Any] | None = None\n</code></pre> <p>Any extra experimental configurations outside of the <code>splifft</code> core.</p>"},{"location":"api/config/#splifft.config.Config.check_derived_stems","title":"check_derived_stems","text":"<pre><code>check_derived_stems() -&gt; Self\n</code></pre> Source code in <code>src/splifft/config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_derived_stems(self) -&gt; Self:\n    if self.derived_stems is None:\n        return self\n\n    base_output_stems = tuple(self.inference.requested_stems or self.model.output_stem_names)\n    existing_stem_names: list[StemName] = [*_INPUT_STEM_NAMES, *base_output_stems]\n    for derived_stem_name, definition in self.derived_stems.items():\n        if derived_stem_name in existing_stem_names:\n            raise PydanticCustomError(\n                \"derived_stem_name_conflict\",\n                \"Derived stem `{derived_stem_name}` must not conflict with existing stem names: `{existing_stem_names}`\",\n                {\n                    \"derived_stem_name\": derived_stem_name,\n                    \"existing_stem_names\": existing_stem_names,\n                },\n            )\n        required_stems: tuple[StemName, ...] = tuple()\n        if isinstance(definition, SubtractConfig):\n            required_stems = (definition.stem_name, definition.by_stem_name)\n        elif isinstance(definition, SumConfig):\n            required_stems = definition.stem_names\n        for stem_name in required_stems:\n            if stem_name not in existing_stem_names:\n                raise PydanticCustomError(\n                    \"invalid_derived_stem\",\n                    \"Derived stem `{derived_stem_name}` requires stem `{stem_name}` but is not found in `{existing_stem_names}`\",\n                    {\n                        \"derived_stem_name\": derived_stem_name,\n                        \"stem_name\": stem_name,\n                        \"existing_stem_names\": existing_stem_names,\n                    },\n                )\n        existing_stem_names.append(derived_stem_name)\n    return self\n</code></pre>"},{"location":"api/config/#splifft.config.Config.check_requested_stems","title":"check_requested_stems","text":"<pre><code>check_requested_stems() -&gt; Self\n</code></pre> Source code in <code>src/splifft/config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef check_requested_stems(self) -&gt; Self:\n    if self.inference.requested_stems is None:\n        return self\n\n    model_stem_names = set(self.model.output_stem_names)\n    for stem_name in self.inference.requested_stems:\n        if stem_name in model_stem_names:\n            continue\n        raise PydanticCustomError(\n            \"invalid_target_stem\",\n            \"Target stem `{stem_name}` is not found in model output stems: `{model_stem_names}`\",\n            {\n                \"stem_name\": stem_name,\n                \"model_stem_names\": self.model.output_stem_names,\n            },\n        )\n\n    return self\n</code></pre>"},{"location":"api/config/#splifft.config.Config.validate_inference_contract","title":"validate_inference_contract","text":"<pre><code>validate_inference_contract(\n    model_params: ModelParamsLike,\n) -&gt; InferenceArchetype\n</code></pre> Source code in <code>src/splifft/config.py</code> <pre><code>def validate_inference_contract(self, model_params: ModelParamsLike) -&gt; t.InferenceArchetype:\n    archetype = model_params.inference_archetype\n    input_type = model_params.input_type\n    output_type = model_params.output_type\n\n    if archetype == \"standard_end_to_end\":\n        if self.waveform_chunking is None:\n            raise ValueError(\"`waveform_chunking` is required for standard_end_to_end pipeline\")\n        if output_type != \"waveform\":\n            raise ValueError(\n                f\"standard_end_to_end expects waveform model output, got {output_type}\"\n            )\n    elif archetype == \"frequency_masking\":\n        if self.transform is None or self.transform.kind != \"stft\":\n            raise ValueError(\n                '`transform` with `kind=\"stft\"` is required for frequency_masking pipeline'\n            )\n        if self.waveform_chunking is None:\n            raise ValueError(\"`waveform_chunking` is required for frequency_masking pipeline\")\n        if input_type not in {\"spectrogram\", \"waveform_and_spectrogram\"}:\n            raise ValueError(\n                f\"frequency_masking expects spectrogram-like model input, got {input_type}\"\n            )\n        if output_type not in {\"spectrogram_mask\", \"spectrogram\"}:\n            raise ValueError(\n                f\"frequency_masking expects spectrogram-like model output, got {output_type}\"\n            )\n    elif archetype == \"sequence_labeling\":\n        if self.sequence_chunking is None:\n            raise ValueError(\"`sequence_chunking` is required for sequence_labeling pipeline\")\n        if input_type == \"spectrogram\" and self.transform is None:\n            raise ValueError(\"spectrogram sequence models require `transform` to be configured\")\n        if input_type not in {\"spectrogram\", \"waveform\"}:\n            raise ValueError(\n                f\"sequence_labeling expects spectrogram or waveform model input, got {input_type}\"\n            )\n        if output_type not in {\"logits\", \"multi_stream\"}:\n            raise ValueError(\n                f\"sequence_labeling expects logits or multi_stream model output, got {output_type}\"\n            )\n    else:\n        raise ValueError(f\"unknown inference archetype: {archetype}\")\n\n    if output_type in {\"logits\", \"multi_stream\"} and self.output.file_format != \"npy\":\n        raise ValueError(\n            f\"sequence models require output.file_format='npy', got {self.output.file_format!r}\"\n        )\n    if output_type not in {\"logits\", \"multi_stream\"} and self.output.file_format == \"npy\":\n        raise ValueError(\"waveform/spectrogram models cannot use output.file_format='npy'\")\n\n    return archetype\n</code></pre>"},{"location":"api/config/#splifft.config.Config.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(\n    path: StrPath | BytesPath,\n    *,\n    overrides: ConfigOverrides = (),\n) -&gt; Config\n</code></pre> <p>Load config JSON from disk, optionally applying CLI-style overrides.</p> Source code in <code>src/splifft/config.py</code> <pre><code>@classmethod\ndef from_file(\n    cls,\n    path: t.StrPath | t.BytesPath,\n    *,\n    overrides: ConfigOverrides = (),\n) -&gt; Config:\n    \"\"\"Load config JSON from disk, optionally applying CLI-style overrides.\"\"\"\n    config_dict = load_config_dict(path)\n    if overrides:\n        apply_config_overrides(config_dict, overrides)\n    return cls.model_validate(config_dict)\n</code></pre>"},{"location":"api/config/#splifft.config.Config.model_config","title":"model_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_config = ConfigDict(\n    arbitrary_types_allowed=True,\n    strict=True,\n    extra=\"forbid\",\n)\n</code></pre>"},{"location":"api/config/#splifft.config.IntoConfig","title":"IntoConfig  <code>module-attribute</code>","text":"<pre><code>IntoConfig: TypeAlias = (\n    Config | dict[str, Any] | StrPath | BytesPath\n)\n</code></pre>"},{"location":"api/config/#splifft.config.into_config","title":"into_config","text":"<pre><code>into_config(\n    config: IntoConfig, *, overrides: ConfigOverrides = ()\n) -&gt; Config\n</code></pre> <p>Convert various config inputs into a validated [<code>Config</code>].</p> <p>If <code>overrides</code> is non-empty, they are applied before validation. Caller-owned objects are not mutated.</p> Source code in <code>src/splifft/config.py</code> <pre><code>def into_config(config: IntoConfig, *, overrides: ConfigOverrides = ()) -&gt; Config:\n    \"\"\"Convert various config inputs into a validated [`Config`].\n\n    If `overrides` is non-empty, they are applied before validation.\n    Caller-owned objects are not mutated.\n    \"\"\"\n    if isinstance(config, Config):\n        if not overrides:\n            return config\n        config_dict = config.model_dump(mode=\"python\")\n        apply_config_overrides(config_dict, overrides)\n        return Config.model_validate(config_dict)\n\n    if isinstance(config, dict):\n        config_dict = copy.deepcopy(config)\n        if overrides:\n            apply_config_overrides(config_dict, overrides)\n        return Config.model_validate(config_dict)\n\n    return Config.from_file(config, overrides=overrides)\n</code></pre>"},{"location":"api/config/#splifft.config.Model","title":"Model","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>authors</code> <code>list[str]</code> <code>purpose</code> <code>Literal['separation', 'denoise', 'debleed', 'dereverb', 'decrowd', 'beat_tracking'] | str</code> <code>architecture</code> <code>Literal['bs_roformer', 'mel_roformer', 'mdx23c', 'scnet', 'beat_this'] | str</code> <code>config_id</code> <code>str | None</code> <p>The default configuration identifier (filename stem) to use if one is not provided.</p> <code>created_at</code> <code>str | None</code> <p>ISO8601 date, time is optional (e.g. YYYY-MM-DD)</p> <code>output</code> <code>NonEmptyUnique[list[Instrument]]</code> <code>status</code> <code>Literal['tested'] | None</code> <code>metrics</code> <code>list[Metrics]</code> <code>description</code> <code>list[Comment]</code> <code>resources</code> <code>list[Resource]</code> <code>model_size</code> <code>int | None</code> <p>Model size in bytes, if available.</p>"},{"location":"api/config/#splifft.config.Model.authors","title":"authors  <code>instance-attribute</code>","text":"<pre><code>authors: list[str]\n</code></pre>"},{"location":"api/config/#splifft.config.Model.purpose","title":"purpose  <code>instance-attribute</code>","text":"<pre><code>purpose: (\n    Literal[\n        \"separation\",\n        \"denoise\",\n        \"debleed\",\n        \"dereverb\",\n        \"decrowd\",\n        \"beat_tracking\",\n    ]\n    | str\n)\n</code></pre>"},{"location":"api/config/#splifft.config.Model.architecture","title":"architecture  <code>instance-attribute</code>","text":"<pre><code>architecture: (\n    Literal[\n        \"bs_roformer\",\n        \"mel_roformer\",\n        \"mdx23c\",\n        \"scnet\",\n        \"beat_this\",\n    ]\n    | str\n)\n</code></pre>"},{"location":"api/config/#splifft.config.Model.config_id","title":"config_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>config_id: str | None = None\n</code></pre> <p>The default configuration identifier (filename stem) to use if one is not provided. Files are expected to be in <code>data/config</code>.</p> <p>If None, the model is not officially supported.</p>"},{"location":"api/config/#splifft.config.Model.created_at","title":"created_at  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>created_at: str | None = None\n</code></pre> <p>ISO8601 date, time is optional (e.g. YYYY-MM-DD)</p>"},{"location":"api/config/#splifft.config.Model.output","title":"output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output: NonEmptyUnique[list[Instrument]] = Field(\n    default_factory=list\n)\n</code></pre>"},{"location":"api/config/#splifft.config.Model.status","title":"status  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status: Literal['tested'] | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.Model.metrics","title":"metrics  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metrics: list[Metrics] = Field(default_factory=list)\n</code></pre>"},{"location":"api/config/#splifft.config.Model.description","title":"description  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>description: list[Comment] = Field(default_factory=list)\n</code></pre>"},{"location":"api/config/#splifft.config.Model.resources","title":"resources  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>resources: list[Resource] = Field(default_factory=list)\n</code></pre>"},{"location":"api/config/#splifft.config.Model.model_size","title":"model_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_size: int | None = None\n</code></pre> <p>Model size in bytes, if available.</p>"},{"location":"api/config/#splifft.config.Metrics","title":"Metrics","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>values</code> <code>dict[Instrument, dict[Metric, float]]</code> <code>source</code> <code>Literal['mvsep'] | str | None</code>"},{"location":"api/config/#splifft.config.Metrics.values","title":"values  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>values: dict[Instrument, dict[Metric, float]] = Field(\n    default_factory=dict\n)\n</code></pre>"},{"location":"api/config/#splifft.config.Metrics.source","title":"source  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>source: Literal['mvsep'] | str | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.Resource","title":"Resource","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>kind</code> <code>Literal['model_ckpt', 'config_msst', 'arxiv', 'other']</code> <code>url</code> <code>str</code> <code>digest</code> <code>str | None</code>"},{"location":"api/config/#splifft.config.Resource.kind","title":"kind  <code>instance-attribute</code>","text":"<pre><code>kind: Literal[\"model_ckpt\", \"config_msst\", \"arxiv\", \"other\"]\n</code></pre>"},{"location":"api/config/#splifft.config.Resource.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: str\n</code></pre>"},{"location":"api/config/#splifft.config.Resource.digest","title":"digest  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>digest: str | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.Comment","title":"Comment","text":"<p>               Bases: <code>BaseModel</code></p> <p>Attributes:</p> Name Type Description <code>content</code> <code>list[str]</code> <p>Condensed informative points of the model (lowercase)</p> <code>author</code> <code>str | None</code>"},{"location":"api/config/#splifft.config.Comment.content","title":"content  <code>instance-attribute</code>","text":"<pre><code>content: list[str]\n</code></pre> <p>Condensed informative points of the model (lowercase)</p>"},{"location":"api/config/#splifft.config.Comment.author","title":"author  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>author: str | None = None\n</code></pre>"},{"location":"api/config/#splifft.config.Registry","title":"Registry","text":"<p>               Bases: <code>dict[Identifier, Model]</code></p> <p>Methods:</p> Name Description <code>__get_pydantic_core_schema__</code> <code>from_file</code>"},{"location":"api/config/#splifft.config.Registry.__get_pydantic_core_schema__","title":"__get_pydantic_core_schema__  <code>classmethod</code>","text":"<pre><code>__get_pydantic_core_schema__(\n    source_type: Any, handler: GetCoreSchemaHandler\n) -&gt; CoreSchema\n</code></pre> Source code in <code>src/splifft/config.py</code> <pre><code>@classmethod\ndef __get_pydantic_core_schema__(\n    cls, source_type: Any, handler: GetCoreSchemaHandler\n) -&gt; CoreSchema:\n    return core_schema.no_info_after_validator_function(cls, handler(dict[t.Identifier, Model]))\n</code></pre>"},{"location":"api/config/#splifft.config.Registry.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(path: StrPath | BytesPath) -&gt; Registry\n</code></pre> Source code in <code>src/splifft/config.py</code> <pre><code>@classmethod\ndef from_file(cls, path: t.StrPath | t.BytesPath) -&gt; Registry:\n    with open(path, \"r\") as f:\n        data = f.read()\n    ta = TypeAdapter(cls)\n    return ta.validate_json(data)\n</code></pre>"},{"location":"api/core/","title":"Core","text":""},{"location":"api/core/#splifft.core","title":"core","text":"<p>Reusable, pure algorithmic components for inference and training.</p> <p>Classes:</p> Name Description <code>Audio</code> <code>NormalizationStats</code> <p>Statistics for normalizing</p> <code>NormalizedAudio</code> <p>Container for normalized audio and its original stats.</p> <code>ModelWaveformToWaveform</code> <code>LogMelSpect</code> <p>Computes the log-mel spectrogram of a waveform.</p> <code>SequenceFeatureExtractor</code> <p>Protocol for sequence feature extractors.</p> <code>IdentitySequenceFeatureExtractor</code> <code>LogMelSequenceFeatureExtractor</code> <code>CqtSequenceFeatureExtractor</code> <code>CQT</code> <p>Constant-Q transform layer (complex output) implemented via <code>Conv1d</code>.</p> <code>HarmonicCQT</code> <p>Harmonic CQT computed by stacking one CQT per harmonic multiplier.</p> <p>Functions:</p> Name Description <code>normalize_audio</code> <p>Preprocess the raw audio in the time domain to have a mean of 0 and a std of 1</p> <code>denormalize_audio</code> <p>Take the model output and restore them to their original loudness.</p> <code>generate_chunks</code> <p>Generates batches of overlapping chunks from an audio tensor.</p> <code>stitch_chunks</code> <p>Stitches processed audio chunks back together using the overlap-add method.</p> <code>aggregate_logits</code> <p>Stitches time-series logits (split/aggregate strategy).</p> <code>aggregate_sequence_chunks</code> <p>Aggregate generic time-major chunk outputs.</p> <code>pad_dim</code> <p>Pad an arbitrary tensor on a specific dimension.</p> <code>split_sequence_tensor</code> <p>Split a time-major sequence tensor into overlapping chunks.</p> <code>apply_mask</code> <p>Applies a complex mask to a spectrogram.</p> <code>get_model_floating_dtype</code> <p>Infer floating input dtype from the model's first floating parameter.</p> <code>to_model_device</code> <p>Move tensor to model device while preserving model floating dtype compatibility.</p> <code>create_w2w_model</code> <code>to_log_magnitude</code> <p>Convert complex or real spectrogram-like tensors to dB log-magnitude.</p> <code>create_cqt_kernels</code> <p>Create time-domain CQT kernels using only PyTorch ops.</p> <code>create_sequence_feature_extractor</code> <code>derive_stems</code> <p>It is the caller's responsibility to ensure that all tensors are aligned and have the same shape.</p> <code>str_to_torch_dtype</code>"},{"location":"api/core/#splifft.core.Audio","title":"Audio  <code>dataclass</code>","text":"<pre><code>Audio(data: _AudioTensorLike, sample_rate: SampleRate)\n</code></pre> <p>               Bases: <code>Generic[_AudioTensorLike]</code></p> <p>Attributes:</p> Name Type Description <code>data</code> <code>_AudioTensorLike</code> <p>This should either be an raw or a</p> <code>sample_rate</code> <code>SampleRate</code>"},{"location":"api/core/#splifft.core.Audio.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: _AudioTensorLike\n</code></pre> <p>This should either be an raw or a normalized audio tensor.</p>"},{"location":"api/core/#splifft.core.Audio.sample_rate","title":"sample_rate  <code>instance-attribute</code>","text":"<pre><code>sample_rate: SampleRate\n</code></pre>"},{"location":"api/core/#splifft.core.NormalizationStats","title":"NormalizationStats  <code>dataclass</code>","text":"<pre><code>NormalizationStats(\n    mean: float, std: Annotated[float, Gt(0)]\n)\n</code></pre> <p>Statistics for normalizing and denormalizing audio.</p> <p>Attributes:</p> Name Type Description <code>mean</code> <code>float</code> <p>Mean \\(\\mu\\) of the mixture</p> <code>std</code> <code>Annotated[float, Gt(0)]</code> <p>Standard deviation \\(\\sigma\\) of the mixture</p>"},{"location":"api/core/#splifft.core.NormalizationStats.mean","title":"mean  <code>instance-attribute</code>","text":"<pre><code>mean: float\n</code></pre> <p>Mean \\(\\mu\\) of the mixture</p>"},{"location":"api/core/#splifft.core.NormalizationStats.std","title":"std  <code>instance-attribute</code>","text":"<pre><code>std: Annotated[float, Gt(0)]\n</code></pre> <p>Standard deviation \\(\\sigma\\) of the mixture</p>"},{"location":"api/core/#splifft.core.NormalizedAudio","title":"NormalizedAudio  <code>dataclass</code>","text":"<pre><code>NormalizedAudio(\n    audio: Audio[NormalizedAudioTensor],\n    stats: NormalizationStats,\n)\n</code></pre> <p>Container for normalized audio and its original stats.</p> <p>Attributes:</p> Name Type Description <code>audio</code> <code>Audio[NormalizedAudioTensor]</code> <code>stats</code> <code>NormalizationStats</code>"},{"location":"api/core/#splifft.core.NormalizedAudio.audio","title":"audio  <code>instance-attribute</code>","text":"<pre><code>audio: Audio[NormalizedAudioTensor]\n</code></pre>"},{"location":"api/core/#splifft.core.NormalizedAudio.stats","title":"stats  <code>instance-attribute</code>","text":"<pre><code>stats: NormalizationStats\n</code></pre>"},{"location":"api/core/#splifft.core.normalize_audio","title":"normalize_audio","text":"<pre><code>normalize_audio(\n    audio: Audio[RawAudioTensor],\n) -&gt; NormalizedAudio\n</code></pre> <p>Preprocess the raw audio in the time domain to have a mean of 0 and a std of 1 before passing it to the model.</p> <p>Operates on the mean of the channels.</p> Source code in <code>src/splifft/core.py</code> <pre><code>def normalize_audio(audio: Audio[t.RawAudioTensor]) -&gt; NormalizedAudio:\n    \"\"\"Preprocess the raw audio in the time domain to have a mean of 0 and a std of 1\n    before passing it to the model.\n\n    Operates on the mean of the [channels][splifft.types.Channels].\n    \"\"\"\n    mono_audio = audio.data.mean(dim=0)\n    mean = float(mono_audio.mean())\n    std = float(mono_audio.std())\n\n    if std &lt;= 1e-8:  # silent audio\n        return NormalizedAudio(\n            audio=Audio(data=t.NormalizedAudioTensor(audio.data), sample_rate=audio.sample_rate),\n            stats=NormalizationStats(mean, 1.0),\n        )\n\n    normalized_data = (audio.data - mean) / std\n    return NormalizedAudio(\n        audio=Audio(data=t.NormalizedAudioTensor(normalized_data), sample_rate=audio.sample_rate),\n        stats=NormalizationStats(mean, std),\n    )\n</code></pre>"},{"location":"api/core/#splifft.core.denormalize_audio","title":"denormalize_audio","text":"<pre><code>denormalize_audio(\n    audio_data: NormalizedAudioTensor,\n    stats: NormalizationStats,\n) -&gt; RawAudioTensor\n</code></pre> <p>Take the model output and restore them to their original loudness.</p> Source code in <code>src/splifft/core.py</code> <pre><code>def denormalize_audio(\n    audio_data: t.NormalizedAudioTensor, stats: NormalizationStats\n) -&gt; t.RawAudioTensor:\n    \"\"\"Take the model output and restore them to their original loudness.\"\"\"\n    return t.RawAudioTensor((audio_data * stats.std) + stats.mean)\n</code></pre>"},{"location":"api/core/#splifft.core.generate_chunks","title":"generate_chunks","text":"<pre><code>generate_chunks(\n    audio_data: RawAudioTensor | NormalizedAudioTensor,\n    chunk_size: ChunkSize,\n    hop_size: HopSize,\n    batch_size: BatchSize,\n    *,\n    padding_mode: PaddingMode = \"reflect\",\n) -&gt; Iterator[PaddedChunkedAudioTensor]\n</code></pre> <p>Generates batches of overlapping chunks from an audio tensor.</p> <p>Returns:</p> Type Description <code>Iterator[PaddedChunkedAudioTensor]</code> <p>An iterator that yields batches of chunks of shape (B, C, chunk_T).</p> Source code in <code>src/splifft/core.py</code> <pre><code>def generate_chunks(\n    audio_data: t.RawAudioTensor | t.NormalizedAudioTensor,\n    chunk_size: t.ChunkSize,\n    hop_size: t.HopSize,\n    batch_size: t.BatchSize,\n    *,\n    padding_mode: t.PaddingMode = \"reflect\",\n) -&gt; Iterator[t.PaddedChunkedAudioTensor]:\n    \"\"\"Generates batches of overlapping chunks from an audio tensor.\n\n    :return: An iterator that yields batches of chunks of shape (B, C, chunk_T).\n    \"\"\"\n    padding = chunk_size - hop_size\n    padded_audio = F.pad(audio_data, (padding, padding), mode=padding_mode)\n\n    padded_len = padded_audio.shape[-1]\n    rem = (padded_len - chunk_size) % hop_size\n    if rem != 0:\n        final_pad = hop_size - rem\n        padded_audio = F.pad(padded_audio, (0, final_pad), mode=\"constant\", value=0)\n\n    unfolded = padded_audio.unfold(\n        dimension=-1, size=chunk_size, step=hop_size\n    )  # (C, num_chunks, chunk_size)\n\n    num_chunks = unfolded.shape[1]\n    unfolded = unfolded.permute(1, 0, 2)  # (num_chunks, C, chunk_size)\n\n    for i in range(0, num_chunks, batch_size):\n        yield t.PaddedChunkedAudioTensor(unfolded[i : i + batch_size])\n</code></pre>"},{"location":"api/core/#splifft.core.stitch_chunks","title":"stitch_chunks","text":"<pre><code>stitch_chunks(\n    processed_chunks: Sequence[SeparatedChunkedTensor],\n    num_stems: NumModelStems,\n    chunk_size: ChunkSize,\n    hop_size: HopSize,\n    target_num_samples: Samples,\n    *,\n    window: WindowTensor,\n) -&gt; RawSeparatedTensor\n</code></pre> <p>Stitches processed audio chunks back together using the overlap-add method.</p> <p>Reconstructs the full audio signal from a sequence of overlapping, processed chunks. Ensures that the sum of all overlapping windows is constant at every time step: \\(\\sum_{m=-\\infty}^{\\infty} w[n - mH] = C\\) where \\(H\\) is the hop size.</p> Source code in <code>src/splifft/core.py</code> <pre><code>def stitch_chunks(\n    processed_chunks: Sequence[t.SeparatedChunkedTensor],\n    num_stems: t.NumModelStems,\n    chunk_size: t.ChunkSize,\n    hop_size: t.HopSize,\n    target_num_samples: t.Samples,\n    *,\n    window: t.WindowTensor,\n) -&gt; t.RawSeparatedTensor:\n    r\"\"\"Stitches processed audio chunks back together using the [overlap-add method](https://en.wikipedia.org/wiki/Overlap%E2%80%93add_method).\n\n    Reconstructs the full audio signal from a sequence of overlapping, processed chunks. Ensures\n    that the sum of all overlapping windows is constant at every time step:\n    $\\sum_{m=-\\infty}^{\\infty} w[n - mH] = C$ where $H$ is the [hop size][splifft.types.HopSize].\n    \"\"\"\n    all_chunks = torch.cat(tuple(processed_chunks), dim=0)\n    total_chunks, _N, num_channels, _chunk_T = all_chunks.shape\n    windowed_chunks = all_chunks * window.view(1, 1, 1, -1)\n\n    # folding: (B, N * C * chunk_T) -&gt; (1, N * C * chunk_T, total_chunks)\n    reshaped_for_fold = windowed_chunks.permute(1, 2, 3, 0).reshape(\n        1, num_stems * num_channels * chunk_size, total_chunks\n    )\n\n    total_length = (total_chunks - 1) * hop_size + chunk_size\n\n    folded = F.fold(\n        reshaped_for_fold,\n        output_size=(1, total_length),\n        kernel_size=(1, chunk_size),\n        stride=(1, hop_size),\n    )  # (1, N * C, 1, total_length)\n    stitched = folded.view(num_stems, num_channels, total_length)\n\n    # normalization for overlap-add\n    windows_to_fold = window.expand(total_chunks, 1, chunk_size)\n    reshaped_windows_for_fold = windows_to_fold.permute(1, 2, 0).reshape(\n        1, chunk_size, total_chunks\n    )\n    norm_window = F.fold(\n        reshaped_windows_for_fold,\n        output_size=(1, total_length),\n        kernel_size=(1, chunk_size),\n        stride=(1, hop_size),\n    ).squeeze(0)\n\n    norm_window.clamp_min_(1e-8)  # for edges where the window sum might be zero\n    stitched /= norm_window\n\n    padding = chunk_size - hop_size\n    if padding &gt; 0:\n        stitched = stitched[..., padding:-padding]\n\n    return t.RawSeparatedTensor(stitched[..., :target_num_samples])\n</code></pre>"},{"location":"api/core/#splifft.core.aggregate_logits","title":"aggregate_logits","text":"<pre><code>aggregate_logits(\n    processed_chunks: Sequence[LogitsTensor],\n    starts: Sequence[int],\n    full_size: int,\n    chunk_size: int,\n    num_stems: int,\n    *,\n    trim_margin: int = 0,\n    overlap_mode: OverlapMode = \"keep_first\",\n) -&gt; LogitsTensor\n</code></pre> <p>Stitches time-series logits (split/aggregate strategy).</p> <p>This is a 1:1 map of beat_this's aggregation behavior: - trim <code>trim_margin</code> frames from each chunk side - write into a full-size buffer - in <code>keep_first</code> mode, process chunks in reverse so earlier chunks   overwrite later ones</p> Source code in <code>src/splifft/core.py</code> <pre><code>def aggregate_logits(\n    processed_chunks: Sequence[t.LogitsTensor],\n    starts: Sequence[int],\n    full_size: int,\n    chunk_size: int,\n    num_stems: int,\n    *,\n    trim_margin: int = 0,\n    overlap_mode: t.OverlapMode = \"keep_first\",\n) -&gt; t.LogitsTensor:\n    \"\"\"Stitches time-series logits (split/aggregate strategy).\n\n    This is a 1:1 map of beat_this's aggregation behavior:\n    - trim `trim_margin` frames from each chunk side\n    - write into a full-size buffer\n    - in `keep_first` mode, process chunks in reverse so earlier chunks\n      overwrite later ones\n    \"\"\"\n    all_chunks = torch.cat(tuple(processed_chunks), dim=0)\n    total_chunks, _, chunk_len_frames = all_chunks.shape\n\n    if len(starts) != total_chunks:\n        raise ValueError(f\"expected {total_chunks=} starts, got {len(starts)}\")\n    if chunk_len_frames != chunk_size:\n        raise ValueError(f\"expected {chunk_size=} but got chunk length {chunk_len_frames}\")\n    if trim_margin * 2 &gt;= chunk_len_frames:\n        raise ValueError(f\"{trim_margin=} is too large for {chunk_len_frames=}\")\n\n    buffer = torch.full(\n        (num_stems, full_size), -1000.0, device=all_chunks.device, dtype=all_chunks.dtype\n    )\n\n    if overlap_mode == \"keep_first\":\n        indices = range(total_chunks - 1, -1, -1)\n    elif overlap_mode == \"keep_last\":\n        indices = range(total_chunks)\n    else:\n        assert_never(overlap_mode)\n\n    for i in indices:\n        chunk = all_chunks[i]\n        chunk_valid = chunk[:, trim_margin : chunk_len_frames - trim_margin]\n        start = starts[i] + trim_margin\n        end = starts[i] + chunk_size - trim_margin\n\n        clipped_start = max(0, start)\n        clipped_end = min(end, full_size)\n        if clipped_start &gt;= clipped_end:\n            continue\n\n        src_start = clipped_start - start\n        src_end = src_start + (clipped_end - clipped_start)\n        buffer[:, clipped_start:clipped_end] = chunk_valid[:, src_start:src_end]\n\n    return t.LogitsTensor(buffer)\n</code></pre>"},{"location":"api/core/#splifft.core.aggregate_sequence_chunks","title":"aggregate_sequence_chunks","text":"<pre><code>aggregate_sequence_chunks(\n    processed_chunks: Sequence[Tensor],\n    starts: Sequence[int],\n    full_size: int,\n    chunk_size: int,\n    *,\n    trim_margin: int = 0,\n    overlap_mode: OverlapMode = \"keep_first\",\n) -&gt; Tensor\n</code></pre> <p>Aggregate generic time-major chunk outputs.</p> <p>Each <code>processed_chunks[i]</code> must have shape <code>(chunk_time, ...)</code> where <code>...</code> can contain any additional feature dimensions (for example bins for <code>activations</code>).</p> Source code in <code>src/splifft/core.py</code> <pre><code>def aggregate_sequence_chunks(\n    processed_chunks: Sequence[Tensor],\n    starts: Sequence[int],\n    full_size: int,\n    chunk_size: int,\n    *,\n    trim_margin: int = 0,\n    overlap_mode: t.OverlapMode = \"keep_first\",\n) -&gt; Tensor:\n    \"\"\"Aggregate generic time-major chunk outputs.\n\n    Each `processed_chunks[i]` must have shape `(chunk_time, ...)` where `...` can\n    contain any additional feature dimensions (for example bins for `activations`).\n    \"\"\"\n    if not processed_chunks:\n        raise ValueError(\"expected at least one chunk\")\n\n    chunk_len_frames = int(processed_chunks[0].shape[0])\n    if trim_margin * 2 &gt;= chunk_len_frames:\n        raise ValueError(f\"{trim_margin=} is too large for {chunk_len_frames=}\")\n    if len(starts) != len(processed_chunks):\n        raise ValueError(f\"expected {len(processed_chunks)=} starts, got {len(starts)}\")\n    if chunk_len_frames != chunk_size:\n        raise ValueError(f\"expected {chunk_size=} but got chunk length {chunk_len_frames}\")\n\n    tail_shape = tuple(processed_chunks[0].shape[1:])\n    buffer = processed_chunks[0].new_zeros((full_size, *tail_shape))\n\n    if overlap_mode == \"keep_first\":\n        indices = range(len(processed_chunks) - 1, -1, -1)\n    elif overlap_mode == \"keep_last\":\n        indices = range(len(processed_chunks))\n    else:\n        assert_never(overlap_mode)\n\n    for i in indices:\n        chunk = processed_chunks[i]\n        if tuple(chunk.shape[1:]) != tail_shape:\n            raise ValueError(\n                f\"all stream chunks must have identical non-time dimensions, got {chunk.shape[1:]} and {tail_shape}\"\n            )\n\n        chunk_valid = chunk[trim_margin : chunk_len_frames - trim_margin]\n        start = starts[i] + trim_margin\n        end = starts[i] + chunk_size - trim_margin\n\n        clipped_start = max(0, start)\n        clipped_end = min(end, full_size)\n        if clipped_start &gt;= clipped_end:\n            continue\n\n        src_start = clipped_start - start\n        src_end = src_start + (clipped_end - clipped_start)\n        buffer[clipped_start:clipped_end] = chunk_valid[src_start:src_end]\n\n    return buffer\n</code></pre>"},{"location":"api/core/#splifft.core.pad_dim","title":"pad_dim","text":"<pre><code>pad_dim(\n    tensor: Tensor,\n    *,\n    dim: int,\n    pad: tuple[int, int],\n    value: float = 0.0,\n) -&gt; Tensor\n</code></pre> <p>Pad an arbitrary tensor on a specific dimension.</p> <p>This avoids relying on <code>F.pad</code>'s reverse-dimension argument ordering.</p> Source code in <code>src/splifft/core.py</code> <pre><code>def pad_dim(tensor: Tensor, *, dim: int, pad: tuple[int, int], value: float = 0.0) -&gt; Tensor:\n    \"\"\"Pad an arbitrary tensor on a specific dimension.\n\n    This avoids relying on `F.pad`'s reverse-dimension argument ordering.\n    \"\"\"\n    left, right = pad\n    if left &lt; 0 or right &lt; 0:\n        raise ValueError(f\"expected non-negative pad widths, got left={left}, right={right}\")\n    if left == 0 and right == 0:\n        return tensor\n\n    rank = tensor.ndim\n    resolved_dim = dim if dim &gt;= 0 else rank + dim\n    if resolved_dim &lt; 0 or resolved_dim &gt;= rank:\n        raise IndexError(f\"dim out of range for rank-{rank} tensor: {dim}\")\n\n    pieces: list[Tensor] = []\n    if left:\n        left_shape = list(tensor.shape)\n        left_shape[resolved_dim] = left\n        pieces.append(tensor.new_full(left_shape, fill_value=value))\n\n    pieces.append(tensor)\n\n    if right:\n        right_shape = list(tensor.shape)\n        right_shape[resolved_dim] = right\n        pieces.append(tensor.new_full(right_shape, fill_value=value))\n\n    return torch.cat(pieces, dim=resolved_dim)\n</code></pre>"},{"location":"api/core/#splifft.core.split_sequence_tensor","title":"split_sequence_tensor","text":"<pre><code>split_sequence_tensor(\n    sequence: Tensor,\n    chunk_size: int,\n    *,\n    trim_margin: int = 0,\n    avoid_short_end: bool = True,\n) -&gt; tuple[list[Tensor], list[int]]\n</code></pre> <p>Split a time-major sequence tensor into overlapping chunks.</p> <p><code>sequence</code> must be shaped <code>(time, ...)</code>, where <code>...</code> can be any feature tail.</p> Source code in <code>src/splifft/core.py</code> <pre><code>def split_sequence_tensor(\n    sequence: Tensor,\n    chunk_size: int,\n    *,\n    trim_margin: int = 0,\n    avoid_short_end: bool = True,\n) -&gt; tuple[list[Tensor], list[int]]:\n    \"\"\"Split a time-major sequence tensor into overlapping chunks.\n\n    `sequence` must be shaped `(time, ...)`, where `...` can be any feature tail.\n    \"\"\"\n    full_size = sequence.shape[0]\n    if (step := chunk_size - 2 * trim_margin) &lt;= 0:\n        raise ValueError(\n            f\"expected chunk_size - 2*trim_margin &gt; 0, got {chunk_size=}, {trim_margin=}\"\n        )\n    if not (starts := list(range(-trim_margin, full_size - trim_margin, step))):\n        starts = [-trim_margin]\n    if avoid_short_end and full_size &gt; step:\n        starts[-1] = full_size - (chunk_size - trim_margin)\n\n    chunks: list[Tensor] = []\n    for start in starts:\n        src_start = max(start, 0)\n        src_end = min(start + chunk_size, full_size)\n        left = max(0, -start)\n        right = max(0, start + chunk_size - full_size)\n\n        chunk = sequence[src_start:src_end]\n        if left &gt; 0 or right &gt; 0:\n            chunk = pad_dim(chunk, dim=0, pad=(left, right), value=0.0)\n        chunks.append(chunk)\n\n    return chunks, starts\n</code></pre>"},{"location":"api/core/#splifft.core.apply_mask","title":"apply_mask","text":"<pre><code>apply_mask(\n    spec_for_masking: ComplexSpectrogram,\n    mask_batch: ComplexSpectrogram,\n    mask_add_sub_dtype: dtype | None,\n    mask_out_dtype: dtype | None,\n) -&gt; SeparatedSpectrogramTensor\n</code></pre> <p>Applies a complex mask to a spectrogram.</p> <p>While this can be simply replaced by a complex multiplication and <code>torch.view_as_complex</code>, CoreML does not support it: https://github.com/apple/coremltools/issues/2003 so we handroll our own.</p> Source code in <code>src/splifft/core.py</code> <pre><code>def apply_mask(\n    spec_for_masking: t.ComplexSpectrogram,\n    mask_batch: t.ComplexSpectrogram,\n    mask_add_sub_dtype: torch.dtype | None,\n    mask_out_dtype: torch.dtype | None,\n) -&gt; t.SeparatedSpectrogramTensor:\n    \"\"\"Applies a complex mask to a spectrogram.\n\n    While this can be simply replaced by a complex multiplication and `torch.view_as_complex`,\n    CoreML does not support it: https://github.com/apple/coremltools/issues/2003 so we handroll our\n    own.\n    \"\"\"\n    spec_real = spec_for_masking[..., 0]\n    spec_imag = spec_for_masking[..., 1]\n    mask_real = mask_batch[..., 0]\n    mask_imag = mask_batch[..., 1]\n\n    # see: 14385, 14401, 14392, 14408\n    ac = spec_real * mask_real\n    bd = spec_imag * mask_imag\n    ad = spec_real * mask_imag\n    bc = spec_imag * mask_real\n\n    # see: 509, 506, 505, 504, 741, 747\n    out_real = ac.to(mask_add_sub_dtype) - bd.to(mask_add_sub_dtype)\n    out_imag = ad.to(mask_add_sub_dtype) + bc.to(mask_add_sub_dtype)\n\n    # see: 503, 501\n    separated_spec = torch.stack([out_real, out_imag], dim=-1).to(mask_out_dtype)\n    return t.SeparatedSpectrogramTensor(separated_spec)\n</code></pre>"},{"location":"api/core/#splifft.core.get_model_floating_dtype","title":"get_model_floating_dtype","text":"<pre><code>get_model_floating_dtype(model: Module) -&gt; dtype | None\n</code></pre> <p>Infer floating input dtype from the model's first floating parameter.</p> Source code in <code>src/splifft/core.py</code> <pre><code>def get_model_floating_dtype(model: nn.Module) -&gt; torch.dtype | None:\n    \"\"\"Infer floating input dtype from the model's first floating parameter.\"\"\"\n    first_param = next(model.parameters(), None)\n    if first_param is None:\n        return None\n    return first_param.dtype if first_param.is_floating_point() else None\n</code></pre>"},{"location":"api/core/#splifft.core.to_model_device","title":"to_model_device","text":"<pre><code>to_model_device(\n    tensor: Tensor,\n    *,\n    model_device: device,\n    model_floating_dtype: dtype | None,\n) -&gt; Tensor\n</code></pre> <p>Move tensor to model device while preserving model floating dtype compatibility.</p> Source code in <code>src/splifft/core.py</code> <pre><code>def to_model_device(\n    tensor: Tensor,\n    *,\n    model_device: torch.device,\n    model_floating_dtype: torch.dtype | None,\n) -&gt; Tensor:\n    \"\"\"Move tensor to model device while preserving model floating dtype compatibility.\"\"\"\n    if model_floating_dtype is not None and tensor.is_floating_point():\n        return tensor.to(device=model_device, dtype=model_floating_dtype)\n    return tensor.to(device=model_device)\n</code></pre>"},{"location":"api/core/#splifft.core.ModelWaveformToWaveform","title":"ModelWaveformToWaveform","text":"<pre><code>ModelWaveformToWaveform(\n    model: Module,\n    preprocess: PreprocessFn,\n    postprocess: PostprocessFn,\n    *,\n    io_device: device,\n    model_device: device,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>model</code> <code>preprocess</code> <code>postprocess</code> <code>io_device</code> <code>model_device</code> <code>model_input_dtype</code> Source code in <code>src/splifft/core.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    preprocess: t.PreprocessFn,\n    postprocess: t.PostprocessFn,\n    *,\n    io_device: torch.device,\n    model_device: torch.device,\n):\n    super().__init__()\n    self.model = model\n    self.preprocess = preprocess\n    self.postprocess = postprocess\n    self.io_device = io_device\n    self.model_device = model_device\n    self.model_input_dtype = get_model_floating_dtype(self.model)\n</code></pre>"},{"location":"api/core/#splifft.core.ModelWaveformToWaveform.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model = model\n</code></pre>"},{"location":"api/core/#splifft.core.ModelWaveformToWaveform.preprocess","title":"preprocess  <code>instance-attribute</code>","text":"<pre><code>preprocess = preprocess\n</code></pre>"},{"location":"api/core/#splifft.core.ModelWaveformToWaveform.postprocess","title":"postprocess  <code>instance-attribute</code>","text":"<pre><code>postprocess = postprocess\n</code></pre>"},{"location":"api/core/#splifft.core.ModelWaveformToWaveform.io_device","title":"io_device  <code>instance-attribute</code>","text":"<pre><code>io_device = io_device\n</code></pre>"},{"location":"api/core/#splifft.core.ModelWaveformToWaveform.model_device","title":"model_device  <code>instance-attribute</code>","text":"<pre><code>model_device = model_device\n</code></pre>"},{"location":"api/core/#splifft.core.ModelWaveformToWaveform.model_input_dtype","title":"model_input_dtype  <code>instance-attribute</code>","text":"<pre><code>model_input_dtype = get_model_floating_dtype(model)\n</code></pre>"},{"location":"api/core/#splifft.core.ModelWaveformToWaveform.forward","title":"forward","text":"<pre><code>forward(\n    waveform_chunk: RawAudioTensor | NormalizedAudioTensor,\n) -&gt; SeparatedChunkedTensor | LogitsTensor\n</code></pre> Source code in <code>src/splifft/core.py</code> <pre><code>def forward(\n    self, waveform_chunk: t.RawAudioTensor | t.NormalizedAudioTensor\n) -&gt; t.SeparatedChunkedTensor | t.LogitsTensor:\n    model_waveform_chunk = cast(\n        t.RawAudioTensor | t.NormalizedAudioTensor,\n        to_model_device(\n            waveform_chunk,\n            model_device=self.model_device,\n            model_floating_dtype=self.model_input_dtype,\n        ),\n    )\n    preprocessed_input = self.preprocess(model_waveform_chunk)\n    model_output = self.model(*preprocessed_input)\n    postprocessed = self.postprocess(model_output, *preprocessed_input)\n    if isinstance(postprocessed, Tensor):\n        return cast(\n            t.SeparatedChunkedTensor | t.LogitsTensor,\n            postprocessed.to(self.io_device),\n        )\n    return postprocessed\n</code></pre>"},{"location":"api/core/#splifft.core.create_w2w_model","title":"create_w2w_model","text":"<pre><code>create_w2w_model(\n    model: Module,\n    model_input_type: ModelInputType,\n    model_output_type: ModelOutputType,\n    stft_cfg: StftConfig | None,\n    num_channels: Channels,\n    chunk_size: ChunkSize,\n    masking_cfg: MaskingConfig,\n    *,\n    io_device: device,\n    model_device: device,\n) -&gt; ModelWaveformToWaveform\n</code></pre> Source code in <code>src/splifft/core.py</code> <pre><code>def create_w2w_model(\n    model: nn.Module,\n    model_input_type: t.ModelInputType,\n    model_output_type: t.ModelOutputType,\n    stft_cfg: StftConfig | None,\n    num_channels: t.Channels,\n    chunk_size: t.ChunkSize,\n    masking_cfg: MaskingConfig,\n    *,\n    io_device: torch.device,\n    model_device: torch.device,\n) -&gt; ModelWaveformToWaveform:\n    needs_stft = model_input_type == \"spectrogram\" or model_input_type == \"waveform_and_spectrogram\"\n    needs_istft = model_output_type == \"spectrogram_mask\" or model_output_type == \"spectrogram\"\n\n    if (needs_stft or needs_istft) and stft_cfg is None:\n        raise ValueError(\n            \"expected stft config for models that operate on spectrograms, but found `None`.\"\n        )\n\n    preprocess: t.PreprocessFn = lambda chunk: (chunk,)  # noqa: E731\n    postprocess: t.PostprocessFn = lambda model_output, *_: model_output  # noqa: E731\n\n    if needs_stft:\n        assert stft_cfg is not None\n        conv_dtype = stft_cfg.conv_dtype\n        if model_device.type == \"cpu\" and conv_dtype == torch.float16:\n            conv_dtype = torch.float32\n\n        stft_module = Stft(\n            n_fft=stft_cfg.n_fft,\n            hop_length=stft_cfg.hop_length,\n            win_length=stft_cfg.win_length,\n            window_fn=lambda win_len: _get_window_fn(stft_cfg.window_shape, win_len, model_device),\n            conv_dtype=conv_dtype,\n        ).to(model_device)\n        if model_input_type == \"spectrogram\":\n            preprocess = _create_stft_preprocessor(stft_module)\n        elif model_input_type == \"waveform_and_spectrogram\":\n            preprocess = _create_hybrid_preprocessor(stft_module)\n        else:\n            raise NotImplementedError(f\"unsupported input type for stft: {model_input_type}\")\n\n    if needs_istft:\n        assert stft_cfg is not None\n        istft_module = IStft(\n            n_fft=stft_cfg.n_fft,\n            hop_length=stft_cfg.hop_length,\n            win_length=stft_cfg.win_length,\n            window_fn=lambda win_len: _get_window_fn(stft_cfg.window_shape, win_len, model_device),\n        ).to(model_device)\n\n        add_sub_dtype = masking_cfg.add_sub_dtype\n        out_dtype = masking_cfg.out_dtype\n        if model_device.type == \"cpu\":\n            if add_sub_dtype == torch.float16:\n                add_sub_dtype = torch.float32\n            if out_dtype == torch.float16:\n                out_dtype = torch.float32\n\n        postprocess = _create_spec_postprocessor(\n            istft_module,\n            num_channels,\n            chunk_size,\n            add_sub_dtype,\n            out_dtype,\n            model_output_type,\n        )\n    return ModelWaveformToWaveform(\n        model,\n        preprocess,\n        postprocess,\n        io_device=io_device,\n        model_device=model_device,\n    )\n</code></pre>"},{"location":"api/core/#splifft.core.LogMelSpect","title":"LogMelSpect","text":"<pre><code>LogMelSpect(\n    sample_rate: int,\n    n_fft: int,\n    hop_length: int,\n    n_mels: int,\n    f_min: float = 0.0,\n    f_max: float | None = None,\n    mel_scale: str = \"slaney\",\n    normalized: bool | str = \"frame_length\",\n    power: float = 1.0,\n    log_multiplier: float = 1000.0,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Computes the log-mel spectrogram of a waveform.</p> <p>Methods:</p> Name Description <code>forward</code> <p>:param x: Waveform tensor of shape (batch, channels, time) or (batch, time)</p> <p>Attributes:</p> Name Type Description <code>spect_class</code> <code>log_multiplier</code> Source code in <code>src/splifft/core.py</code> <pre><code>def __init__(\n    self,\n    sample_rate: int,\n    n_fft: int,\n    hop_length: int,\n    n_mels: int,\n    f_min: float = 0.0,\n    f_max: float | None = None,\n    mel_scale: str = \"slaney\",\n    normalized: bool | str = \"frame_length\",\n    power: float = 1.0,\n    log_multiplier: float = 1000.0,\n):\n    super().__init__()\n    import torchaudio.transforms as T\n\n    self.spect_class = T.MelSpectrogram(\n        sample_rate=sample_rate,\n        n_fft=n_fft,\n        hop_length=hop_length,\n        f_min=f_min,\n        f_max=f_max,\n        n_mels=n_mels,\n        mel_scale=mel_scale,\n        normalized=normalized,\n        power=power,\n    )\n    self.log_multiplier = log_multiplier\n</code></pre>"},{"location":"api/core/#splifft.core.LogMelSpect.spect_class","title":"spect_class  <code>instance-attribute</code>","text":"<pre><code>spect_class = MelSpectrogram(\n    sample_rate=sample_rate,\n    n_fft=n_fft,\n    hop_length=hop_length,\n    f_min=f_min,\n    f_max=f_max,\n    n_mels=n_mels,\n    mel_scale=mel_scale,\n    normalized=normalized,\n    power=power,\n)\n</code></pre>"},{"location":"api/core/#splifft.core.LogMelSpect.log_multiplier","title":"log_multiplier  <code>instance-attribute</code>","text":"<pre><code>log_multiplier = log_multiplier\n</code></pre>"},{"location":"api/core/#splifft.core.LogMelSpect.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; LogMelSpectrogram\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Waveform tensor of shape (batch, channels, time) or (batch, time)</p> required <p>Returns:</p> Type Description <code>LogMelSpectrogram</code> <p>Log-Mel spectrogram of shape (batch, channels, n_mels, time)</p> Source code in <code>src/splifft/core.py</code> <pre><code>def forward(self, x: Tensor) -&gt; t.LogMelSpectrogram:\n    \"\"\"\n    :param x: Waveform tensor of shape (batch, channels, time) or (batch, time)\n    :return: Log-Mel spectrogram of shape (batch, channels, n_mels, time)\n    \"\"\"\n    if x.ndim == 2:\n        x = x.unsqueeze(1)\n    mel_spec = self.spect_class(x)\n    return torch.log1p(self.log_multiplier * mel_spec)  # type: ignore\n</code></pre>"},{"location":"api/core/#splifft.core.to_log_magnitude","title":"to_log_magnitude","text":"<pre><code>to_log_magnitude(\n    x: Tensor, *, epsilon: float = 1e-08\n) -&gt; Tensor\n</code></pre> <p>Convert complex or real spectrogram-like tensors to dB log-magnitude.</p> Source code in <code>src/splifft/core.py</code> <pre><code>def to_log_magnitude(x: Tensor, *, epsilon: float = 1e-8) -&gt; Tensor:\n    \"\"\"Convert complex or real spectrogram-like tensors to dB log-magnitude.\"\"\"\n    if x.shape[-1] == 2:\n        x = torch.sqrt(x[..., 0] ** 2 + x[..., 1] ** 2)\n    else:\n        x = x.abs()\n    return x.clamp_min(epsilon).log10().mul(20)\n</code></pre>"},{"location":"api/core/#splifft.core.create_cqt_kernels","title":"create_cqt_kernels","text":"<pre><code>create_cqt_kernels(\n    *,\n    Q: float,\n    fs: int,\n    fmin: float,\n    n_bins: int,\n    bins_per_octave: int,\n    norm: int = 1,\n    window: str = \"hann\",\n    fmax: float | None = None,\n    gamma: float = 0.0,\n    device: device,\n    dtype: dtype = float32,\n) -&gt; tuple[Tensor, int, Tensor, Tensor]\n</code></pre> <p>Create time-domain CQT kernels using only PyTorch ops.</p> <p>This mirrors the nnAudio-style kernel generation used by PESTO but avoids SciPy so <code>splifft</code> can keep a minimal dependency surface.</p> Source code in <code>src/splifft/core.py</code> <pre><code>def create_cqt_kernels(\n    *,\n    Q: float,\n    fs: int,\n    fmin: float,\n    n_bins: int,\n    bins_per_octave: int,\n    norm: int = 1,\n    window: str = \"hann\",\n    fmax: float | None = None,\n    gamma: float = 0.0,\n    device: torch.device,\n    dtype: torch.dtype = torch.float32,\n) -&gt; tuple[Tensor, int, Tensor, Tensor]:\n    \"\"\"Create time-domain CQT kernels using only PyTorch ops.\n\n    This mirrors the nnAudio-style kernel generation used by PESTO but avoids\n    SciPy so `splifft` can keep a minimal dependency surface.\n    \"\"\"\n    del fmax\n    freq_indices = torch.arange(n_bins, device=device, dtype=dtype)\n    freqs = fmin * (2.0 ** (freq_indices / float(bins_per_octave)))\n\n    alpha = 2.0 ** (1.0 / bins_per_octave) - 1.0\n    lengths = torch.ceil(Q * fs / (freqs + gamma / alpha))\n    max_len = int(lengths.max().item())\n    fft_len = 1 &lt;&lt; int(math.ceil(math.log2(max_len)))\n\n    lengths_i = lengths.to(dtype=torch.int64)\n    half_center = torch.full((n_bins,), fft_len / 2.0, device=device, dtype=dtype)\n    starts = torch.ceil(half_center - lengths / 2.0).to(torch.int64) - (lengths_i % 2)\n\n    time_index = torch.arange(fft_len, device=device, dtype=torch.int64).unsqueeze(0)\n    local_index = time_index - starts.unsqueeze(1)\n    valid = (local_index &gt;= 0) &amp; (local_index &lt; lengths_i.unsqueeze(1))\n\n    window_vals = _cqt_window_values(\n        window=window,\n        local_index=local_index,\n        lengths=lengths_i,\n        dtype=dtype,\n    )\n    window_vals = torch.where(valid, window_vals, torch.zeros_like(window_vals))\n\n    neg_half = torch.div(-lengths_i, 2, rounding_mode=\"floor\").unsqueeze(1)\n    centered_n = (local_index + neg_half).to(dtype=dtype)\n    phase = (2 * torch.pi / float(fs)) * freqs.unsqueeze(1) * centered_n\n    signal = window_vals * torch.exp(1j * phase) / lengths.unsqueeze(1)\n    signal = torch.where(valid, signal, torch.zeros_like(signal))\n\n    if norm:\n        denom = torch.linalg.vector_norm(signal, ord=norm, dim=1, keepdim=True).clamp_min(1e-12)\n        signal = signal / denom\n\n    kernels = signal.to(dtype=torch.complex64)\n    return kernels, fft_len, lengths.sqrt().unsqueeze(-1), freqs\n</code></pre>"},{"location":"api/core/#splifft.core.SequenceFeatureExtractor","title":"SequenceFeatureExtractor","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for sequence feature extractors.</p> <p>Required contract: - input: <code>(B, C, T)</code> - output: <code>(B, seq_len, feature_dim)</code></p> <p>Methods:</p> Name Description <code>__call__</code> <p>Attributes:</p> Name Type Description <code>hop_length_samples</code> <code>int</code> <code>stage_name</code> <code>str</code>"},{"location":"api/core/#splifft.core.SequenceFeatureExtractor.hop_length_samples","title":"hop_length_samples  <code>instance-attribute</code>","text":"<pre><code>hop_length_samples: int\n</code></pre>"},{"location":"api/core/#splifft.core.SequenceFeatureExtractor.stage_name","title":"stage_name  <code>instance-attribute</code>","text":"<pre><code>stage_name: str\n</code></pre>"},{"location":"api/core/#splifft.core.SequenceFeatureExtractor.__call__","title":"__call__","text":"<pre><code>__call__(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/core.py</code> <pre><code>def __call__(self, x: Tensor) -&gt; Tensor: ...\n</code></pre>"},{"location":"api/core/#splifft.core.IdentitySequenceFeatureExtractor","title":"IdentitySequenceFeatureExtractor","text":"<p>               Bases: <code>Module</code>, <code>SequenceFeatureExtractor</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>hop_length_samples</code> <code>stage_name</code>"},{"location":"api/core/#splifft.core.IdentitySequenceFeatureExtractor.hop_length_samples","title":"hop_length_samples  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>hop_length_samples = 1\n</code></pre>"},{"location":"api/core/#splifft.core.IdentitySequenceFeatureExtractor.stage_name","title":"stage_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stage_name = 'sequence_features'\n</code></pre>"},{"location":"api/core/#splifft.core.IdentitySequenceFeatureExtractor.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/core.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    if x.ndim != 3:\n        raise ValueError(f\"expected shape (B,C,T), got {tuple(x.shape)}\")\n    if x.shape[1] != 1:\n        raise ValueError(\n            f\"identity sequence extractor expects mono input with C=1, got shape={tuple(x.shape)}\"\n        )\n    return _ensure_btf(x.transpose(1, 2), source=\"identity extractor\")\n</code></pre>"},{"location":"api/core/#splifft.core.LogMelSequenceFeatureExtractor","title":"LogMelSequenceFeatureExtractor","text":"<pre><code>LogMelSequenceFeatureExtractor(\n    mel: LogMelSpect, *, hop_length_samples: int\n)\n</code></pre> <p>               Bases: <code>Module</code>, <code>SequenceFeatureExtractor</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>stage_name</code> <code>mel</code> <code>hop_length_samples</code> Source code in <code>src/splifft/core.py</code> <pre><code>def __init__(self, mel: LogMelSpect, *, hop_length_samples: int):\n    super().__init__()\n    self.mel = mel\n    self.hop_length_samples = hop_length_samples\n</code></pre>"},{"location":"api/core/#splifft.core.LogMelSequenceFeatureExtractor.stage_name","title":"stage_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stage_name = 'mel'\n</code></pre>"},{"location":"api/core/#splifft.core.LogMelSequenceFeatureExtractor.mel","title":"mel  <code>instance-attribute</code>","text":"<pre><code>mel = mel\n</code></pre>"},{"location":"api/core/#splifft.core.LogMelSequenceFeatureExtractor.hop_length_samples","title":"hop_length_samples  <code>instance-attribute</code>","text":"<pre><code>hop_length_samples = hop_length_samples\n</code></pre>"},{"location":"api/core/#splifft.core.LogMelSequenceFeatureExtractor.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/core.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    if x.ndim != 3:\n        raise ValueError(f\"expected shape (B,C,T), got {tuple(x.shape)}\")\n    if x.shape[1] != 1:\n        raise ValueError(\n            f\"mel extractor expects mono input with C=1, got shape={tuple(x.shape)}\"\n        )\n    # TODO dont do this\n    x_mono = x.mean(dim=1, keepdim=True)\n    mel = self.mel(x_mono).squeeze(1)\n    return _ensure_btf(mel.transpose(1, 2), source=\"mel extractor\")\n</code></pre>"},{"location":"api/core/#splifft.core.CqtSequenceFeatureExtractor","title":"CqtSequenceFeatureExtractor","text":"<pre><code>CqtSequenceFeatureExtractor(\n    hcqt: HarmonicCQT,\n    *,\n    hop_length_samples: int,\n    log_epsilon: float,\n)\n</code></pre> <p>               Bases: <code>Module</code>, <code>SequenceFeatureExtractor</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>stage_name</code> <code>hcqt</code> <code>hop_length_samples</code> <code>log_epsilon</code> Source code in <code>src/splifft/core.py</code> <pre><code>def __init__(\n    self,\n    hcqt: HarmonicCQT,\n    *,\n    hop_length_samples: int,\n    log_epsilon: float,\n):\n    super().__init__()\n    self.hcqt = hcqt\n    self.hop_length_samples = hop_length_samples\n    self.log_epsilon = log_epsilon\n</code></pre>"},{"location":"api/core/#splifft.core.CqtSequenceFeatureExtractor.stage_name","title":"stage_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stage_name = 'cqt'\n</code></pre>"},{"location":"api/core/#splifft.core.CqtSequenceFeatureExtractor.hcqt","title":"hcqt  <code>instance-attribute</code>","text":"<pre><code>hcqt = hcqt\n</code></pre>"},{"location":"api/core/#splifft.core.CqtSequenceFeatureExtractor.hop_length_samples","title":"hop_length_samples  <code>instance-attribute</code>","text":"<pre><code>hop_length_samples = hop_length_samples\n</code></pre>"},{"location":"api/core/#splifft.core.CqtSequenceFeatureExtractor.log_epsilon","title":"log_epsilon  <code>instance-attribute</code>","text":"<pre><code>log_epsilon = log_epsilon\n</code></pre>"},{"location":"api/core/#splifft.core.CqtSequenceFeatureExtractor.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/core.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    if x.ndim != 3:\n        raise ValueError(f\"expected shape (B,C,T), got {tuple(x.shape)}\")\n    if x.shape[1] != 1:\n        raise ValueError(\n            f\"cqt extractor expects mono input with C=1, got shape={tuple(x.shape)}\"\n        )\n    x_mono = x.mean(dim=1)\n    hcqt_output = self.hcqt(x_mono)\n    cqt = to_log_magnitude(hcqt_output, epsilon=self.log_epsilon).permute(0, 3, 1, 2)\n    b, t_len, harmonics, bins = cqt.shape\n    return _ensure_btf(cqt.reshape(b, t_len, harmonics * bins), source=\"cqt extractor\")\n</code></pre>"},{"location":"api/core/#splifft.core.create_sequence_feature_extractor","title":"create_sequence_feature_extractor","text":"<pre><code>create_sequence_feature_extractor(\n    feature_cfg: FeatureExtractionConfig | None,\n    *,\n    sample_rate: SampleRate,\n    device: device,\n) -&gt; SequenceFeatureExtractor\n</code></pre> Source code in <code>src/splifft/core.py</code> <pre><code>def create_sequence_feature_extractor(\n    feature_cfg: FeatureExtractionConfig | None,\n    *,\n    sample_rate: t.SampleRate,\n    device: torch.device,\n) -&gt; SequenceFeatureExtractor:\n    if feature_cfg is None:\n        return IdentitySequenceFeatureExtractor()\n\n    if feature_cfg.kind == \"mel\":\n        mel = LogMelSpect(\n            sample_rate=feature_cfg.sample_rate,\n            n_fft=feature_cfg.n_fft,\n            hop_length=feature_cfg.hop_length,\n            n_mels=feature_cfg.n_mels,\n            f_min=feature_cfg.f_min,\n            f_max=feature_cfg.f_max,\n            mel_scale=feature_cfg.mel_scale,\n            normalized=feature_cfg.normalized,\n            power=feature_cfg.power,\n            log_multiplier=feature_cfg.log_multiplier,\n        ).to(device)\n        return LogMelSequenceFeatureExtractor(mel=mel, hop_length_samples=feature_cfg.hop_length)\n\n    if feature_cfg.kind == \"cqt\":\n        hop_length_samples = int(round(feature_cfg.hop_size_ms * sample_rate / 1000))\n        hcqt = HarmonicCQT(\n            sr=sample_rate,\n            hop_length=hop_length_samples,\n            harmonics=feature_cfg.harmonics,\n            fmin=feature_cfg.fmin,\n            fmax=feature_cfg.fmax,\n            bins_per_semitone=feature_cfg.bins_per_semitone,\n            n_bins=feature_cfg.n_bins,\n            center_bins=feature_cfg.center_bins,\n            gamma=feature_cfg.gamma,\n            center=feature_cfg.center,\n        ).to(device)\n        return CqtSequenceFeatureExtractor(\n            hcqt=hcqt,\n            hop_length_samples=hop_length_samples,\n            log_epsilon=feature_cfg.log_epsilon,\n        )\n\n    raise ValueError(f\"unsupported feature extractor kind: {feature_cfg.kind}\")\n</code></pre>"},{"location":"api/core/#splifft.core.CQT","title":"CQT","text":"<pre><code>CQT(\n    *,\n    sr: SampleRate,\n    hop_length: int,\n    fmin: float,\n    fmax: float | None,\n    n_bins: int,\n    bins_per_octave: int,\n    gamma: float,\n    center: bool,\n    window: str = \"hann\",\n    norm: int = 1,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Constant-Q transform layer (complex output) implemented via <code>Conv1d</code>.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>n_bins</code> <code>conv</code> Source code in <code>src/splifft/core.py</code> <pre><code>def __init__(\n    self,\n    *,\n    sr: t.SampleRate,\n    hop_length: int,\n    fmin: float,\n    fmax: float | None,\n    n_bins: int,\n    bins_per_octave: int,\n    gamma: float,\n    center: bool,\n    window: str = \"hann\",\n    norm: int = 1,\n):\n    super().__init__()\n    self.n_bins = n_bins\n\n    Q = 1.0 / (2 ** (1 / bins_per_octave) - 1)\n    kernels, kernel_width, sqrt_lengths, _freqs = create_cqt_kernels(\n        Q=Q,\n        fs=sr,\n        fmin=fmin,\n        n_bins=n_bins,\n        bins_per_octave=bins_per_octave,\n        norm=norm,\n        window=window,\n        fmax=fmax,\n        gamma=gamma,\n        device=torch.device(\"cpu\"),\n    )\n\n    self.register_buffer(\"sqrt_lengths\", sqrt_lengths, persistent=False)\n    self.register_buffer(\"kernel_real_imag\", kernels, persistent=False)\n\n    padding = kernel_width // 2 if center else 0\n    self.conv = nn.Conv1d(\n        1,\n        2 * n_bins,\n        kernel_size=kernel_width,\n        stride=hop_length,\n        padding=padding,\n        padding_mode=\"reflect\",\n        bias=False,\n    )\n    self._init_weights()\n</code></pre>"},{"location":"api/core/#splifft.core.CQT.n_bins","title":"n_bins  <code>instance-attribute</code>","text":"<pre><code>n_bins = n_bins\n</code></pre>"},{"location":"api/core/#splifft.core.CQT.conv","title":"conv  <code>instance-attribute</code>","text":"<pre><code>conv = Conv1d(\n    1,\n    2 * n_bins,\n    kernel_size=kernel_width,\n    stride=hop_length,\n    padding=padding,\n    padding_mode=\"reflect\",\n    bias=False,\n)\n</code></pre>"},{"location":"api/core/#splifft.core.CQT.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/core.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    if x.ndim == 1:\n        x = x.unsqueeze(0)\n    if x.ndim == 2:\n        x = x.unsqueeze(1)\n    if x.ndim != 3:\n        raise ValueError(f\"expected shape (batch,time) or (batch,1,time), got {tuple(x.shape)}\")\n\n    cqt = self.conv(x).view(x.size(0), 2, self.n_bins, -1)\n    cqt = cqt * self.sqrt_lengths.to(cqt.device)\n    return cast(Tensor, cqt.permute(0, 2, 3, 1))\n</code></pre>"},{"location":"api/core/#splifft.core.HarmonicCQT","title":"HarmonicCQT","text":"<pre><code>HarmonicCQT(\n    *,\n    harmonics: Sequence[int],\n    sr: SampleRate,\n    hop_length: int,\n    fmin: float,\n    fmax: float | None,\n    bins_per_semitone: int,\n    n_bins: int,\n    center_bins: bool,\n    gamma: float,\n    center: bool,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Harmonic CQT computed by stacking one CQT per harmonic multiplier.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>cqt_kernels</code> Source code in <code>src/splifft/core.py</code> <pre><code>def __init__(\n    self,\n    *,\n    harmonics: Sequence[int],\n    sr: t.SampleRate,\n    hop_length: int,\n    fmin: float,\n    fmax: float | None,\n    bins_per_semitone: int,\n    n_bins: int,\n    center_bins: bool,\n    gamma: float,\n    center: bool,\n):\n    super().__init__()\n    if center_bins:\n        fmin = fmin / 2 ** ((bins_per_semitone - 1) / (12 * bins_per_semitone))\n\n    self.cqt_kernels = nn.ModuleList(\n        [\n            CQT(\n                sr=sr,\n                hop_length=hop_length,\n                fmin=h * fmin,\n                fmax=fmax,\n                n_bins=n_bins,\n                bins_per_octave=12 * bins_per_semitone,\n                gamma=gamma,\n                center=center,\n            )\n            for h in harmonics\n        ]\n    )\n</code></pre>"},{"location":"api/core/#splifft.core.HarmonicCQT.cqt_kernels","title":"cqt_kernels  <code>instance-attribute</code>","text":"<pre><code>cqt_kernels = ModuleList(\n    [\n        (\n            CQT(\n                sr=sr,\n                hop_length=hop_length,\n                fmin=h * fmin,\n                fmax=fmax,\n                n_bins=n_bins,\n                bins_per_octave=12 * bins_per_semitone,\n                gamma=gamma,\n                center=center,\n            )\n        )\n        for h in harmonics\n    ]\n)\n</code></pre>"},{"location":"api/core/#splifft.core.HarmonicCQT.forward","title":"forward","text":"<pre><code>forward(audio_waveforms: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/core.py</code> <pre><code>def forward(self, audio_waveforms: Tensor) -&gt; Tensor:\n    return torch.stack([cqt(audio_waveforms) for cqt in self.cqt_kernels], dim=1)\n</code></pre>"},{"location":"api/core/#splifft.core.derive_stems","title":"derive_stems","text":"<pre><code>derive_stems(\n    separated_stems: Mapping[\n        ModelOutputStemName, RawAudioTensor\n    ],\n    mixture_input: RawAudioTensor,\n    stem_rules: DerivedStemsConfig,\n) -&gt; dict[StemName, RawAudioTensor]\n</code></pre> <p>It is the caller's responsibility to ensure that all tensors are aligned and have the same shape.</p> <p>Note</p> <p>Mixture input and separated stems must first be denormalized.</p> Source code in <code>src/splifft/core.py</code> <pre><code>def derive_stems(\n    separated_stems: Mapping[t.ModelOutputStemName, t.RawAudioTensor],\n    mixture_input: t.RawAudioTensor,\n    stem_rules: DerivedStemsConfig,\n) -&gt; dict[StemName, t.RawAudioTensor]:\n    \"\"\"\n    It is the caller's responsibility to ensure that all tensors are aligned and have the same shape.\n\n    !!! note\n        Mixture input and separated stems must first be [denormalized][splifft.core.denormalize_audio].\n    \"\"\"\n    stems = {\n        \"mixture\": t.RawAudioTensor(mixture_input),  # for subtraction\n        **separated_stems,\n    }\n\n    for derived_name, rule in stem_rules.items():\n        if rule.operation == \"subtract\":\n            # pydantic should have already validated that the stem names exist so safe to index directly\n            minuend = stems[rule.stem_name]\n            subtrahend = stems[rule.by_stem_name]\n            stems[derived_name] = t.RawAudioTensor(minuend - subtrahend)\n        elif rule.operation == \"sum\":\n            to_sum = tuple(stems[s] for s in rule.stem_names)\n            stems[derived_name] = t.RawAudioTensor(torch.stack(to_sum).sum(dim=0))\n\n    stems.pop(\"mixture\", None)\n    return stems\n</code></pre>"},{"location":"api/core/#splifft.core.str_to_torch_dtype","title":"str_to_torch_dtype","text":"<pre><code>str_to_torch_dtype(value: Any) -&gt; dtype\n</code></pre> Source code in <code>src/splifft/core.py</code> <pre><code>def str_to_torch_dtype(value: Any) -&gt; torch.dtype:\n    if not isinstance(value, str):\n        raise TypeError(f\"expected dtype to be a string, got {value} (type {type(value)})\")\n    try:\n        dtype = getattr(torch, value)\n    except AttributeError:\n        raise ValueError(f\"`{value}` cannot be found under the `torch` namespace\")\n    if not isinstance(dtype, torch.dtype):\n        raise TypeError(f\"expected {dtype} to be a dtype but it is a {type(dtype)}\")\n    return dtype\n</code></pre>"},{"location":"api/inference/","title":"Inference","text":""},{"location":"api/inference/#splifft.inference","title":"inference","text":"<p>Public inference APIs.</p> <p>Classes:</p> Name Description <code>ChunkProcessed</code> <code>Stage</code> <code>InferenceOutput</code> <code>InferenceEngine</code> <p>Functions:</p> Name Description <code>resolve_model_entrypoint</code> <p>Attributes:</p> Name Type Description <code>SUPPORTED_MODELS</code> <code>dict[str, tuple[str, str]]</code> <code>InferenceEvent</code> <code>TypeAlias</code>"},{"location":"api/inference/#splifft.inference.SUPPORTED_MODELS","title":"SUPPORTED_MODELS  <code>module-attribute</code>","text":"<pre><code>SUPPORTED_MODELS: dict[str, tuple[str, str]] = {\n    \"bs_roformer\": (\n        \"splifft.models.bs_roformer\",\n        \"BSRoformer\",\n    ),\n    \"mel_roformer\": (\n        \"splifft.models.bs_roformer\",\n        \"BSRoformer\",\n    ),\n    \"mdx23c\": (\"splifft.models.mdx23c\", \"MDX23C\"),\n    \"beat_this\": (\"splifft.models.beat_this\", \"BeatThis\"),\n    \"pesto\": (\"splifft.models.pesto\", \"Pesto\"),\n    \"basic_pitch\": (\n        \"splifft.models.basic_pitch\",\n        \"BasicPitch\",\n    ),\n}\n</code></pre>"},{"location":"api/inference/#splifft.inference.resolve_model_entrypoint","title":"resolve_model_entrypoint","text":"<pre><code>resolve_model_entrypoint(\n    model_type: ModelType,\n    module_name: str | None,\n    class_name: str | None,\n) -&gt; tuple[str, str]\n</code></pre> Source code in <code>src/splifft/inference.py</code> <pre><code>def resolve_model_entrypoint(\n    model_type: t.ModelType,\n    module_name: str | None,\n    class_name: str | None,\n) -&gt; tuple[str, str]:\n    if module_name is not None and class_name is not None:\n        return module_name, class_name\n    try:\n        return SUPPORTED_MODELS[model_type]\n    except KeyError as e:\n        raise ValueError(\n            f\"could not resolve model entrypoint for model_type={model_type!r}; \"\n            \"provide both module and class explicitly\"\n        ) from e\n</code></pre>"},{"location":"api/inference/#splifft.inference.ChunkProcessed","title":"ChunkProcessed  <code>dataclass</code>","text":"<pre><code>ChunkProcessed(batch_index: int, total_batches: int)\n</code></pre> <p>Attributes:</p> Name Type Description <code>batch_index</code> <code>int</code> <code>total_batches</code> <code>int</code>"},{"location":"api/inference/#splifft.inference.ChunkProcessed.batch_index","title":"batch_index  <code>instance-attribute</code>","text":"<pre><code>batch_index: int\n</code></pre>"},{"location":"api/inference/#splifft.inference.ChunkProcessed.total_batches","title":"total_batches  <code>instance-attribute</code>","text":"<pre><code>total_batches: int\n</code></pre>"},{"location":"api/inference/#splifft.inference.Stage","title":"Stage  <code>dataclass</code>","text":"<pre><code>Stage(stage: str, *, total_batches: int | None = None)\n</code></pre> <p>Classes:</p> Name Description <code>Started</code> <code>Completed</code> <p>Methods:</p> Name Description <code>__enter__</code> <code>__exit__</code> <p>Attributes:</p> Name Type Description <code>stage</code> <code>str</code> <code>total_batches</code> <code>int | None</code> <code>started</code> <code>Started</code> <code>completed</code> <code>Completed</code>"},{"location":"api/inference/#splifft.inference.Stage.stage","title":"stage  <code>instance-attribute</code>","text":"<pre><code>stage: str\n</code></pre>"},{"location":"api/inference/#splifft.inference.Stage.total_batches","title":"total_batches  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>total_batches: int | None = field(\n    kw_only=True, default=None\n)\n</code></pre>"},{"location":"api/inference/#splifft.inference.Stage.Started","title":"Started  <code>dataclass</code>","text":"<pre><code>Started(stage: str, total_batches: int | None = None)\n</code></pre> <p>Attributes:</p> Name Type Description <code>stage</code> <code>str</code> <code>total_batches</code> <code>int | None</code>"},{"location":"api/inference/#splifft.inference.Stage.Started.stage","title":"stage  <code>instance-attribute</code>","text":"<pre><code>stage: str\n</code></pre>"},{"location":"api/inference/#splifft.inference.Stage.Started.total_batches","title":"total_batches  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>total_batches: int | None = None\n</code></pre>"},{"location":"api/inference/#splifft.inference.Stage.Completed","title":"Completed  <code>dataclass</code>","text":"<pre><code>Completed(stage: str)\n</code></pre> <p>Attributes:</p> Name Type Description <code>stage</code> <code>str</code>"},{"location":"api/inference/#splifft.inference.Stage.Completed.stage","title":"stage  <code>instance-attribute</code>","text":"<pre><code>stage: str\n</code></pre>"},{"location":"api/inference/#splifft.inference.Stage.started","title":"started  <code>property</code>","text":"<pre><code>started: Started\n</code></pre>"},{"location":"api/inference/#splifft.inference.Stage.completed","title":"completed  <code>property</code>","text":"<pre><code>completed: Completed\n</code></pre>"},{"location":"api/inference/#splifft.inference.Stage.__enter__","title":"__enter__","text":"<pre><code>__enter__() -&gt; Stage\n</code></pre> Source code in <code>src/splifft/inference.py</code> <pre><code>def __enter__(self) -&gt; Stage:\n    return self\n</code></pre>"},{"location":"api/inference/#splifft.inference.Stage.__exit__","title":"__exit__","text":"<pre><code>__exit__(*_: object) -&gt; None\n</code></pre> Source code in <code>src/splifft/inference.py</code> <pre><code>def __exit__(self, *_: object) -&gt; None:\n    return None\n</code></pre>"},{"location":"api/inference/#splifft.inference.InferenceOutput","title":"InferenceOutput  <code>dataclass</code>","text":"<pre><code>InferenceOutput(\n    outputs: dict[StemName, RawAudioTensor]\n    | dict[str, Tensor],\n    sample_rate: SampleRate,\n)\n</code></pre> <p>Attributes:</p> Name Type Description <code>outputs</code> <code>dict[StemName, RawAudioTensor] | dict[str, Tensor]</code> <code>sample_rate</code> <code>SampleRate</code>"},{"location":"api/inference/#splifft.inference.InferenceOutput.outputs","title":"outputs  <code>instance-attribute</code>","text":"<pre><code>outputs: dict[StemName, RawAudioTensor] | dict[str, Tensor]\n</code></pre>"},{"location":"api/inference/#splifft.inference.InferenceOutput.sample_rate","title":"sample_rate  <code>instance-attribute</code>","text":"<pre><code>sample_rate: SampleRate\n</code></pre>"},{"location":"api/inference/#splifft.inference.InferenceEvent","title":"InferenceEvent  <code>module-attribute</code>","text":"<pre><code>InferenceEvent: TypeAlias = (\n    Started | ChunkProcessed | Completed | InferenceOutput\n)\n</code></pre>"},{"location":"api/inference/#splifft.inference.InferenceEngine","title":"InferenceEngine  <code>dataclass</code>","text":"<pre><code>InferenceEngine(\n    config: Config,\n    model: Module,\n    model_params_concrete: ModelParamsLike,\n    model_device: device,\n    io_device: device,\n    model_input_dtype: dtype | None,\n)\n</code></pre> <p>Methods:</p> Name Description <code>from_pretrained</code> <code>from_registry</code> <code>to_audio_tensor</code> <code>run</code> <code>stream</code> <p>Attributes:</p> Name Type Description <code>config</code> <code>Config</code> <code>model</code> <code>Module</code> <code>model_params_concrete</code> <code>ModelParamsLike</code> <code>model_device</code> <code>device</code> <code>io_device</code> <code>device</code> <code>model_input_dtype</code> <code>dtype | None</code>"},{"location":"api/inference/#splifft.inference.InferenceEngine.config","title":"config  <code>instance-attribute</code>","text":"<pre><code>config: Config\n</code></pre>"},{"location":"api/inference/#splifft.inference.InferenceEngine.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: Module\n</code></pre>"},{"location":"api/inference/#splifft.inference.InferenceEngine.model_params_concrete","title":"model_params_concrete  <code>instance-attribute</code>","text":"<pre><code>model_params_concrete: ModelParamsLike\n</code></pre>"},{"location":"api/inference/#splifft.inference.InferenceEngine.model_device","title":"model_device  <code>instance-attribute</code>","text":"<pre><code>model_device: device\n</code></pre>"},{"location":"api/inference/#splifft.inference.InferenceEngine.io_device","title":"io_device  <code>instance-attribute</code>","text":"<pre><code>io_device: device\n</code></pre>"},{"location":"api/inference/#splifft.inference.InferenceEngine.model_input_dtype","title":"model_input_dtype  <code>instance-attribute</code>","text":"<pre><code>model_input_dtype: dtype | None\n</code></pre>"},{"location":"api/inference/#splifft.inference.InferenceEngine.from_pretrained","title":"from_pretrained  <code>classmethod</code>","text":"<pre><code>from_pretrained(\n    *,\n    config: IntoConfig,\n    checkpoint_path: StrPath,\n    overrides: ConfigOverrides = (),\n    model_device: device | str | None = None,\n    io_device: device | str | None = None,\n    module_name: str | None = None,\n    class_name: str | None = None,\n    package_name: str | None = None,\n) -&gt; InferenceEngine\n</code></pre> Source code in <code>src/splifft/inference.py</code> <pre><code>@classmethod\ndef from_pretrained(\n    cls,\n    *,\n    config: IntoConfig,\n    checkpoint_path: t.StrPath,\n    overrides: ConfigOverrides = (),\n    model_device: torch.device | str | None = None,\n    io_device: torch.device | str | None = None,\n    module_name: str | None = None,\n    class_name: str | None = None,\n    package_name: str | None = None,\n) -&gt; InferenceEngine:\n    from .config import into_config\n    from .io import load_weights\n    from .models import ModelMetadata\n\n    config = into_config(config, overrides=overrides)\n\n    model_device_resolved = _resolve_device(\n        model_device or config.inference.model_device,\n        field_name=\"inference.model_device\",\n    )\n    io_device_resolved = _resolve_device(\n        io_device or config.inference.io_device,\n        field_name=\"inference.io_device\",\n    )\n    resolved_module, resolved_class = resolve_model_entrypoint(\n        config.model_type, module_name, class_name\n    )\n    metadata = ModelMetadata.from_module(\n        module_name=resolved_module,\n        model_cls_name=resolved_class,\n        model_type=config.model_type,\n        package=package_name,\n    )\n\n    model_params = config.model.to_concrete(metadata.params)\n    full_output_stems = tuple(config.model.output_stem_names)\n    requested_stems = tuple(config.inference.requested_stems or full_output_stems)\n\n    state_dict_transform = None\n    if requested_stems != full_output_stems:\n        # optional model-level optimization contract: models can choose to\n        # provide a stem-selection plan that may mutate params and checkpoint\n        # loading. if absent, we keep full model outputs and discard\n        # unrelated stems immediately after each forward pass.\n        from .models import SupportsStemSelection\n\n        if isinstance(metadata.model, SupportsStemSelection):\n            plan = metadata.model.__splifft_stem_selection_plan__(model_params, requested_stems)\n            model_params = plan.model_params\n            state_dict_transform = plan.state_dict_transform\n\n    model = metadata.model(model_params)\n    if (forced_dtype := config.inference.force_weights_dtype) is not None:\n        model = model.to(_resolve_runtime_dtype(forced_dtype, device=model_device_resolved))\n    model = load_weights(\n        model,\n        checkpoint_path,\n        device=model_device_resolved,\n        state_dict_transform=state_dict_transform,\n    ).eval()\n\n    # maybe we should to an explicit try_compile() method while emitting events but eh.\n    # we shuold probably log since it can take extremely long\n    if (compile_cfg := config.inference.compile_model) is not None:\n        compiled_model = torch.compile(\n            model,\n            fullgraph=compile_cfg.fullgraph,\n            dynamic=compile_cfg.dynamic,\n            mode=compile_cfg.mode,\n        )\n        model = cast(nn.Module, compiled_model)\n\n    return cls(\n        config=config,\n        model=model,\n        model_params_concrete=model_params,\n        model_device=model_device_resolved,\n        io_device=io_device_resolved,\n        model_input_dtype=core.get_model_floating_dtype(model),\n    )\n</code></pre>"},{"location":"api/inference/#splifft.inference.InferenceEngine.from_registry","title":"from_registry  <code>classmethod</code>","text":"<pre><code>from_registry(\n    model_id: str,\n    *,\n    model_device: device | str | None = None,\n    io_device: device | str | None = None,\n    overrides: ConfigOverrides = (),\n    fetch_if_missing: bool = True,\n    force_overwrite_config: bool = False,\n    force_overwrite_model: bool = False,\n    registry_path: Path = PATH_REGISTRY_DEFAULT,\n) -&gt; InferenceEngine\n</code></pre> Source code in <code>src/splifft/inference.py</code> <pre><code>@classmethod\ndef from_registry(\n    cls,\n    model_id: str,\n    *,\n    model_device: torch.device | str | None = None,\n    io_device: torch.device | str | None = None,\n    overrides: ConfigOverrides = (),\n    fetch_if_missing: bool = True,\n    force_overwrite_config: bool = False,\n    force_overwrite_model: bool = False,\n    registry_path: Path = PATH_REGISTRY_DEFAULT,\n) -&gt; InferenceEngine:\n    from .config import Registry\n    from .io import get_model_paths\n\n    model_paths = get_model_paths(\n        model_id,\n        fetch_if_missing=fetch_if_missing,\n        force_overwrite_config=force_overwrite_config,\n        force_overwrite_model=force_overwrite_model,\n        registry=Registry.from_file(registry_path),\n    )\n    return cls.from_pretrained(\n        config=model_paths.path_config,\n        checkpoint_path=model_paths.path_checkpoint,\n        overrides=overrides,\n        model_device=model_device,\n        io_device=io_device,\n    )\n</code></pre>"},{"location":"api/inference/#splifft.inference.InferenceEngine.to_audio_tensor","title":"to_audio_tensor","text":"<pre><code>to_audio_tensor(\n    mixture: StrPath\n    | BytesPath\n    | RawAudioTensor\n    | Audio[RawAudioTensor],\n) -&gt; Audio[RawAudioTensor]\n</code></pre> Source code in <code>src/splifft/inference.py</code> <pre><code>def to_audio_tensor(\n    self,\n    mixture: t.StrPath | t.BytesPath | t.RawAudioTensor | core.Audio[t.RawAudioTensor],\n) -&gt; core.Audio[t.RawAudioTensor]:\n    if isinstance(mixture, core.Audio):\n        return mixture\n    elif isinstance(mixture, torch.Tensor):\n        return core.Audio(\n            data=t.RawAudioTensor(mixture),\n            sample_rate=self.config.audio_io.target_sample_rate,\n        )\n    else:\n        from .io import read_audio\n\n        return read_audio(\n            mixture,  # type: ignore[arg-type]\n            self.config.audio_io.target_sample_rate,\n            self.config.audio_io.force_channels,\n            device=self.io_device,\n        )\n</code></pre>"},{"location":"api/inference/#splifft.inference.InferenceEngine.run","title":"run","text":"<pre><code>run(\n    mixture: StrPath\n    | BytesPath\n    | RawAudioTensor\n    | Audio[RawAudioTensor],\n) -&gt; InferenceOutput\n</code></pre> Source code in <code>src/splifft/inference.py</code> <pre><code>def run(\n    self,\n    mixture: t.StrPath | t.BytesPath | t.RawAudioTensor | core.Audio[t.RawAudioTensor],\n) -&gt; InferenceOutput:\n    for event in self.stream(mixture):\n        if isinstance(event, InferenceOutput):\n            return event\n    raise RuntimeError(\"inference stream finished without outputs\")\n</code></pre>"},{"location":"api/inference/#splifft.inference.InferenceEngine.stream","title":"stream","text":"<pre><code>stream(\n    mixture: StrPath\n    | BytesPath\n    | RawAudioTensor\n    | Audio[RawAudioTensor],\n) -&gt; Generator[InferenceEvent, None, None]\n</code></pre> Source code in <code>src/splifft/inference.py</code> <pre><code>def stream(\n    self,\n    mixture: t.StrPath | t.BytesPath | t.RawAudioTensor | core.Audio[t.RawAudioTensor],\n) -&gt; Generator[InferenceEvent, None, None]:\n    archetype = self.config.validate_inference_contract(self.model_params_concrete)\n\n    audio_tensor = self.to_audio_tensor(mixture)\n    raw_mixture_data = t.RawAudioTensor(audio_tensor.data.to(self.io_device))\n    mixture_data: t.RawAudioTensor | t.NormalizedAudioTensor = raw_mixture_data\n    mixture_stats: core.NormalizationStats | None = None\n\n    if self.config.normalization.enabled:\n        with Stage(\"normalize\") as s:\n            yield s.started\n            normalized = core.normalize_audio(\n                core.Audio(data=raw_mixture_data, sample_rate=audio_tensor.sample_rate)\n            )\n            mixture_data = normalized.audio.data\n            mixture_stats = normalized.stats\n            yield s.completed\n\n    mixture_data = yield from self._adapt_input_channels(mixture_data)\n\n    if archetype == \"sequence_labeling\":\n        requested_stems = self._requested_output_stem_names()\n        requested_stem_indices = self._requested_output_stem_indices()\n        sequence_outputs = yield from self._stream_sequence_labeling(\n            mixture_data,\n            requested_stems=requested_stems,\n            output_indices=requested_stem_indices,\n        )\n        yield InferenceOutput(outputs=sequence_outputs, sample_rate=audio_tensor.sample_rate)\n        return\n\n    requested_stems = self._requested_output_stem_names()\n    requested_stem_indices = self._requested_output_stem_indices()\n    separated_data = yield from self._stream_waveform_pipeline(\n        mixture_data,\n        archetype,\n        output_indices=requested_stem_indices,\n        num_stems=len(requested_stems),\n    )\n\n    denormalized_stems: dict[t.ModelOutputStemName, t.RawAudioTensor] = {}\n    with Stage(\"collect_outputs\") as s:\n        yield s.started\n        for i, stem_name in enumerate(requested_stems):\n            stem_data = separated_data[i, ...]\n            if mixture_stats is not None:\n                stem_data = core.denormalize_audio(\n                    audio_data=t.NormalizedAudioTensor(stem_data),\n                    stats=mixture_stats,\n                )\n            denormalized_stems[stem_name] = t.RawAudioTensor(stem_data)\n        yield s.completed\n\n    output_stems: dict[StemName, t.RawAudioTensor] = denormalized_stems\n    if derived_stems_cfg := self.config.derived_stems:\n        with Stage(\"derive_stems\") as s:\n            yield s.started\n            output_stems = core.derive_stems(\n                denormalized_stems,\n                raw_mixture_data,\n                derived_stems_cfg,\n            )\n            yield s.completed\n\n    yield InferenceOutput(outputs=output_stems, sample_rate=audio_tensor.sample_rate)\n</code></pre>"},{"location":"api/io/","title":"IO","text":""},{"location":"api/io/#splifft.io","title":"io","text":"<p>Operations for reading and writing to disk, and network IO.</p> <p>All side effects should go here.</p> <p>Classes:</p> Name Description <code>ModelSupportStatus</code> <code>LocalModelPaths</code> <p>Functions:</p> Name Description <code>read_audio</code> <p>Loads, resamples and converts channels.</p> <code>load_weights</code> <p>Load the weights from a checkpoint into the given model.</p> <code>get_model_cache_dir</code> <code>is_model_cached</code> <p>Checks if the model's config and checkpoint exist in the cache.</p> <code>is_model_supported</code> <p>Return support status based on the presence of a default config in package data.</p> <code>delete_model_from_cache</code> <code>get_model_paths</code> <code>download_file</code> <p>Attributes:</p> Name Type Description <code>logger</code>"},{"location":"api/io/#splifft.io.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/io/#splifft.io.read_audio","title":"read_audio","text":"<pre><code>read_audio(\n    file: str | Path | RawIOBase | BufferedReader | bytes,\n    target_sr: SampleRate,\n    target_channels: int | None,\n    device: device | None = None,\n) -&gt; Audio[RawAudioTensor]\n</code></pre> <p>Loads, resamples and converts channels.</p> Source code in <code>src/splifft/io.py</code> <pre><code>def read_audio(\n    file: str | Path | io.RawIOBase | io.BufferedReader | bytes,\n    target_sr: t.SampleRate,\n    target_channels: int | None,\n    device: torch.device | None = None,\n) -&gt; Audio[t.RawAudioTensor]:\n    \"\"\"Loads, resamples and converts channels.\"\"\"\n    decoder = AudioDecoder(source=file, sample_rate=target_sr, num_channels=target_channels)\n    samples = decoder.get_all_samples()\n    waveform = samples.data.to(device)\n\n    return Audio(t.RawAudioTensor(waveform), samples.sample_rate)\n</code></pre>"},{"location":"api/io/#splifft.io.load_weights","title":"load_weights","text":"<pre><code>load_weights(\n    model: ModelT,\n    checkpoint_file: FileLike,\n    device: device | str,\n    *,\n    strict: bool = False,\n    state_dict_transform: Callable[\n        [dict[str, Tensor]], dict[str, Tensor]\n    ]\n    | None = None,\n) -&gt; ModelT\n</code></pre> <p>Load the weights from a checkpoint into the given model.</p> <p>Handles standard PyTorch checkpoints and PyTorch Lightning checkpoints (stripping <code>model.</code> prefix).</p> Source code in <code>src/splifft/io.py</code> <pre><code>def load_weights(\n    model: ModelT,\n    checkpoint_file: torch.types.FileLike,\n    device: torch.device | str,\n    *,\n    strict: bool = False,\n    state_dict_transform: Callable[[dict[str, torch.Tensor]], dict[str, torch.Tensor]]\n    | None = None,\n) -&gt; ModelT:\n    \"\"\"Load the weights from a checkpoint into the given model.\n\n    Handles standard PyTorch checkpoints and PyTorch Lightning checkpoints (stripping `model.` prefix).\n    \"\"\"\n\n    loaded_obj: object = torch.load(checkpoint_file, map_location=device, weights_only=True)\n    if isinstance(loaded_obj, dict) and \"state_dict\" in loaded_obj:\n        loaded_obj = loaded_obj[\"state_dict\"]\n    if not isinstance(loaded_obj, dict):\n        raise TypeError(f\"expected checkpoint dict, got {type(loaded_obj).__name__}\")\n\n    state_dict: dict[str, torch.Tensor] = {}\n    for key, value in loaded_obj.items():\n        if not isinstance(value, torch.Tensor):\n            continue\n        state_dict[key] = value\n\n    new_state_dict = {}\n    for key, value in state_dict.items():\n        if key.startswith(\"model.\"):\n            new_state_dict[key[6:]] = value\n        else:\n            new_state_dict[key] = value\n    state_dict = new_state_dict\n\n    if state_dict_transform is not None:\n        state_dict = state_dict_transform(state_dict)\n\n    # TODO: DataParallel and `module.` prefix\n    model.load_state_dict(state_dict, strict=strict)\n    # NOTE: do not torch.compile here!\n\n    return model.to(device)\n</code></pre>"},{"location":"api/io/#splifft.io.get_model_cache_dir","title":"get_model_cache_dir","text":"<pre><code>get_model_cache_dir(model_id: str) -&gt; Path\n</code></pre> Source code in <code>src/splifft/io.py</code> <pre><code>def get_model_cache_dir(model_id: str) -&gt; Path:\n    try:\n        import platformdirs  # noqa: F401\n        import pydantic  # noqa: F401\n    except ImportError:\n        _raise_missing_feature(extra=\"config\", feature=\"caching\")\n    from platformdirs import user_cache_dir\n\n    cache_dir = Path(user_cache_dir(\"splifft\", appauthor=False)) / model_id\n    return cache_dir\n</code></pre>"},{"location":"api/io/#splifft.io.is_model_cached","title":"is_model_cached","text":"<pre><code>is_model_cached(model_id: str) -&gt; bool\n</code></pre> <p>Checks if the model's config and checkpoint exist in the cache.</p> Source code in <code>src/splifft/io.py</code> <pre><code>def is_model_cached(model_id: str) -&gt; bool:\n    \"\"\"Checks if the model's config and checkpoint exist in the cache.\"\"\"\n    # NOTE: not validating the hash for speed\n    try:\n        cache_dir = get_model_cache_dir(model_id)\n        return (cache_dir / \"config.json\").exists() and (cache_dir / \"model.ckpt\").exists()\n    except ImportError:\n        return False\n</code></pre>"},{"location":"api/io/#splifft.io.ModelSupportStatus","title":"ModelSupportStatus","text":"<p>               Bases: <code>Enum</code></p> <p>Attributes:</p> Name Type Description <code>MISSING</code> <code>UNTESTED</code> <code>AVAILABLE</code>"},{"location":"api/io/#splifft.io.ModelSupportStatus.MISSING","title":"MISSING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MISSING = 'missing'\n</code></pre>"},{"location":"api/io/#splifft.io.ModelSupportStatus.UNTESTED","title":"UNTESTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UNTESTED = 'untested'\n</code></pre>"},{"location":"api/io/#splifft.io.ModelSupportStatus.AVAILABLE","title":"AVAILABLE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>AVAILABLE = 'available'\n</code></pre>"},{"location":"api/io/#splifft.io.is_model_supported","title":"is_model_supported","text":"<pre><code>is_model_supported(\n    model_id: str, registry: Registry\n) -&gt; ModelSupportStatus\n</code></pre> <p>Return support status based on the presence of a default config in package data.</p> Source code in <code>src/splifft/io.py</code> <pre><code>def is_model_supported(model_id: str, registry: Registry) -&gt; ModelSupportStatus:\n    \"\"\"Return support status based on the presence of a default config in package data.\"\"\"\n    if model_id not in registry:\n        return ModelSupportStatus.MISSING\n\n    if not (config_id := registry[model_id].config_id):\n        return ModelSupportStatus.MISSING\n\n    if not (DIR_CONFIG_DEFAULT / f\"{config_id}.json\").exists():  # TODO dont hardcode\n        return ModelSupportStatus.MISSING\n\n    if config_id.startswith(\".\"):\n        return ModelSupportStatus.UNTESTED\n\n    return ModelSupportStatus.AVAILABLE\n</code></pre>"},{"location":"api/io/#splifft.io.delete_model_from_cache","title":"delete_model_from_cache","text":"<pre><code>delete_model_from_cache(model_id: str) -&gt; bool\n</code></pre> Source code in <code>src/splifft/io.py</code> <pre><code>def delete_model_from_cache(model_id: str) -&gt; bool:\n    try:\n        if (cache_dir := get_model_cache_dir(model_id)).exists():\n            shutil.rmtree(cache_dir)\n            return True\n        return False\n    except ImportError:\n        return False\n</code></pre>"},{"location":"api/io/#splifft.io.LocalModelPaths","title":"LocalModelPaths  <code>dataclass</code>","text":"<pre><code>LocalModelPaths(\n    *, path_config: Path, path_checkpoint: Path\n)\n</code></pre> <p>Attributes:</p> Name Type Description <code>path_config</code> <code>Path</code> <code>path_checkpoint</code> <code>Path</code>"},{"location":"api/io/#splifft.io.LocalModelPaths.path_config","title":"path_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>path_config: Path = field(kw_only=True)\n</code></pre>"},{"location":"api/io/#splifft.io.LocalModelPaths.path_checkpoint","title":"path_checkpoint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>path_checkpoint: Path = field(kw_only=True)\n</code></pre>"},{"location":"api/io/#splifft.io.get_model_paths","title":"get_model_paths","text":"<pre><code>get_model_paths(\n    model_id: str,\n    *,\n    fetch_if_missing: bool = False,\n    force_overwrite_config: bool = False,\n    force_overwrite_model: bool = False,\n    registry: Registry,\n) -&gt; LocalModelPaths\n</code></pre> Source code in <code>src/splifft/io.py</code> <pre><code>def get_model_paths(\n    model_id: str,\n    *,\n    fetch_if_missing: bool = False,\n    force_overwrite_config: bool = False,\n    force_overwrite_model: bool = False,\n    registry: Registry,\n) -&gt; LocalModelPaths:\n    if model_id not in registry:\n        matches = difflib.get_close_matches(model_id, list(registry))\n        suggestions = \"\\n\".join(map(lambda m: f\"- {m!r}\", matches))\n        suggestion = f\" did you mean:\\n{suggestions}\\n\" if matches else \"\"\n        raise ValueError(\n            f\"model '{model_id}' not found in registry.{suggestion}\\n\"\n            \"help: use `splifft ls` to see downloaded models in the registry\"\n        )\n\n    model_info = registry[model_id]\n    cache_dir = get_model_cache_dir(model_id)\n    cached_config = cache_dir / \"config.json\"\n    cached_ckpt = cache_dir / \"model.ckpt\"\n\n    is_config_present = cached_config.exists()\n    is_ckpt_present = cached_ckpt.exists()\n\n    if force_overwrite_config or not is_config_present:\n        if not model_info.config_id:\n            raise ValueError(\n                f\"model '{model_id}' does not specify a default configuration identifier.\\n\"\n                \"help: you must provide a config file manually.\"\n            )\n\n        if not (source_config := DIR_CONFIG_DEFAULT / f\"{model_info.config_id}.json\").exists():\n            raise FileNotFoundError(\n                f\"default config '{model_info.config_id}.json' not found in package data ({DIR_CONFIG_DEFAULT}).\\n\"\n                \"help: the registry entry should point to a config that exists\"\n            )\n\n        cache_dir.mkdir(parents=True, exist_ok=True)\n        shutil.copy2(source_config, cached_config)\n        logger.info(f\"wrote config for '{model_id}' at {cached_config}\")\n\n    if force_overwrite_model or (not is_ckpt_present and fetch_if_missing):\n        # only download the first? or try the second if not?\n        if not (\n            ckpt_resource := next((r for r in model_info.resources if r.kind == \"model_ckpt\"), None)\n        ):\n            raise ValueError(f\"model '{model_id}' has no `model_ckpt` resource URL in registry\")\n\n        logger.info(f\"pulling weights for '{model_id}'\")\n        download_file(\n            ckpt_resource.url,\n            cached_ckpt,\n            expected_digest=ckpt_resource.digest,\n        )\n\n    return LocalModelPaths(path_config=cached_config, path_checkpoint=cached_ckpt)\n</code></pre>"},{"location":"api/io/#splifft.io.download_file","title":"download_file","text":"<pre><code>download_file(\n    url: str, dest: Path, expected_digest: str | None = None\n) -&gt; None\n</code></pre> Source code in <code>src/splifft/io.py</code> <pre><code>def download_file(url: str, dest: Path, expected_digest: str | None = None) -&gt; None:\n    try:\n        import httpx  # noqa: F401\n    except ImportError:\n        _raise_missing_feature(extra=\"web\", feature=\"download\")\n\n    try:\n        from rich.progress import (\n            BarColumn,\n            DownloadColumn,\n            Progress,\n            TextColumn,\n            TimeRemainingColumn,\n            TransferSpeedColumn,\n        )\n\n        rich_progress = Progress(\n            TextColumn(\"downloading [bold blue]{task.fields[filename]}\"),\n            BarColumn(),\n            DownloadColumn(),\n            TransferSpeedColumn(),\n            TimeRemainingColumn(),\n            transient=True,\n        )\n    except ImportError:\n        rich_progress = None\n\n    dest.parent.mkdir(parents=True, exist_ok=True)\n    dest_tmp = dest.with_suffix(\".tmp\")\n\n    if expected_digest is None:\n        logger.warning(f\"no digest found for {url}, file integrity will not be verified.\")\n    else:\n        if not expected_digest.startswith(\"sha256:\"):\n            logger.warning(f\"unsupported digest format: {expected_digest}, skipping verification.\")\n            expected_digest = None\n\n    hasher = hashlib.sha256() if expected_digest else None\n\n    try:\n        # TODO hoist httpx client up. and aiolimiter?\n        with httpx.Client(http2=True, follow_redirects=True) as client:\n            with client.stream(\"GET\", url) as response:\n                response.raise_for_status()\n                content_length = response.headers.get(\"content-length\")\n                total = int(content_length) if content_length is not None else None\n\n                def download(*, callback: Callable[[bytes], None]) -&gt; None:\n                    with open(dest_tmp, \"wb\") as f:\n                        for chunk in response.iter_bytes():\n                            f.write(chunk)\n                            if hasher is not None:\n                                hasher.update(chunk)\n                            callback(chunk)\n\n                if rich_progress is None:\n                    logger.info(f\"downloading {dest.name} ({total or 'unknown'} bytes)...\")\n                    download(callback=lambda *args: None)\n                else:\n                    with rich_progress as p:\n                        task = p.add_task(\"download\", filename=dest.name, total=total)\n                        download(callback=lambda chunk: p.update(task, advance=len(chunk)))\n\n        if expected_digest and hasher:\n            if (actual_digest := f\"sha256:{hasher.hexdigest()}\") != expected_digest:\n                raise RuntimeError(\n                    f\"digest mismatch for {url}:\\n\"\n                    f\"  expected: '{expected_digest}'\\n\"\n                    f\"  actual:   '{actual_digest}'\"\n                )\n            else:\n                logger.info(f\"verified digest '{expected_digest}'\")\n\n        dest_tmp.replace(dest)  # atomic to ensure we dont have corrupted files if interrupted\n    except Exception as e:\n        if dest_tmp.exists():\n            os.remove(dest_tmp)\n        raise RuntimeError(f\"failed to download {url}: {e}\") from e\n</code></pre>"},{"location":"api/models/","title":"Models","text":""},{"location":"api/models/#splifft.models","title":"models","text":"<p>Source separation models.</p> <p>Modules:</p> Name Description <code>basic_pitch</code> <p>ICASSP 2022 Basic Pitch. Raw multi-stream outputs only, no symbolic decoding.</p> <code>beat_this</code> <p>Beat This! Beat Tracker.</p> <code>bs_roformer</code> <p>Band-Split RoPE Transformer</p> <code>mdx23c</code> <p>MDX23C.</p> <code>pesto</code> <p>PESTO: Pitch Estimation with Self-supervised Transposition-equivariant Objective.</p> <code>utils</code> <p>Classes:</p> Name Description <code>ModelParamsLike</code> <p>A trait that must be implemented to be considered a model parameter.</p> <code>StemSelectionPlan</code> <p>Optional model-specific plan for selective stem inference.</p> <code>SupportsStemSelection</code> <code>ModelMetadata</code> <p>Metadata about a model, including its type, parameter class, and model class.</p> <p>Attributes:</p> Name Type Description <code>ModelT</code> <code>ModelParamsLikeT</code> <code>StateDictTransform</code> <code>TypeAlias</code>"},{"location":"api/models/#splifft.models.ModelParamsLike","title":"ModelParamsLike","text":"<p>               Bases: <code>Protocol</code></p> <p>A trait that must be implemented to be considered a model parameter. Note that <code>input_type</code> and <code>output_type</code> belong to a model's definition and does not allow modification via the configuration dictionary.</p> <p>Attributes:</p> Name Type Description <code>chunk_size</code> <code>ChunkSize</code> <code>output_stem_names</code> <code>tuple[ModelOutputStemName, ...]</code> <code>input_channels</code> <code>ModelInputChannels</code> <code>input_type</code> <code>ModelInputType</code> <code>output_type</code> <code>ModelOutputType</code> <code>inference_archetype</code> <code>InferenceArchetype</code>"},{"location":"api/models/#splifft.models.ModelParamsLike.chunk_size","title":"chunk_size  <code>instance-attribute</code>","text":"<pre><code>chunk_size: ChunkSize\n</code></pre>"},{"location":"api/models/#splifft.models.ModelParamsLike.output_stem_names","title":"output_stem_names  <code>instance-attribute</code>","text":"<pre><code>output_stem_names: tuple[ModelOutputStemName, ...]\n</code></pre>"},{"location":"api/models/#splifft.models.ModelParamsLike.input_channels","title":"input_channels  <code>property</code>","text":"<pre><code>input_channels: ModelInputChannels\n</code></pre>"},{"location":"api/models/#splifft.models.ModelParamsLike.input_type","title":"input_type  <code>property</code>","text":"<pre><code>input_type: ModelInputType\n</code></pre>"},{"location":"api/models/#splifft.models.ModelParamsLike.output_type","title":"output_type  <code>property</code>","text":"<pre><code>output_type: ModelOutputType\n</code></pre>"},{"location":"api/models/#splifft.models.ModelParamsLike.inference_archetype","title":"inference_archetype  <code>property</code>","text":"<pre><code>inference_archetype: InferenceArchetype\n</code></pre>"},{"location":"api/models/#splifft.models.ModelT","title":"ModelT  <code>module-attribute</code>","text":"<pre><code>ModelT = TypeVar('ModelT', bound=Module)\n</code></pre>"},{"location":"api/models/#splifft.models.ModelParamsLikeT","title":"ModelParamsLikeT  <code>module-attribute</code>","text":"<pre><code>ModelParamsLikeT = TypeVar(\n    \"ModelParamsLikeT\", bound=ModelParamsLike\n)\n</code></pre>"},{"location":"api/models/#splifft.models.StateDictTransform","title":"StateDictTransform  <code>module-attribute</code>","text":"<pre><code>StateDictTransform: TypeAlias = Callable[\n    [dict[str, Tensor]], dict[str, Tensor]\n]\n</code></pre>"},{"location":"api/models/#splifft.models.StemSelectionPlan","title":"StemSelectionPlan  <code>dataclass</code>","text":"<pre><code>StemSelectionPlan(\n    model_params: ModelParamsLikeT,\n    output_stem_names: tuple[ModelOutputStemName, ...],\n    state_dict_transform: StateDictTransform | None = None,\n)\n</code></pre> <p>               Bases: <code>Generic[ModelParamsLikeT]</code></p> <p>Optional model-specific plan for selective stem inference.</p> <p>Models can provide this plan to: - instantiate a stem-reduced parameter set, and/or - return a checkpoint state-dict transformer that drops/remaps unrelated heads.</p> <p><code>output_stem_names</code> defines the output ordering produced by the instantiated model after applying the plan.</p> <p>Attributes:</p> Name Type Description <code>model_params</code> <code>ModelParamsLikeT</code> <code>output_stem_names</code> <code>tuple[ModelOutputStemName, ...]</code> <code>state_dict_transform</code> <code>StateDictTransform | None</code>"},{"location":"api/models/#splifft.models.StemSelectionPlan.model_params","title":"model_params  <code>instance-attribute</code>","text":"<pre><code>model_params: ModelParamsLikeT\n</code></pre>"},{"location":"api/models/#splifft.models.StemSelectionPlan.output_stem_names","title":"output_stem_names  <code>instance-attribute</code>","text":"<pre><code>output_stem_names: tuple[ModelOutputStemName, ...]\n</code></pre>"},{"location":"api/models/#splifft.models.StemSelectionPlan.state_dict_transform","title":"state_dict_transform  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>state_dict_transform: StateDictTransform | None = None\n</code></pre>"},{"location":"api/models/#splifft.models.SupportsStemSelection","title":"SupportsStemSelection","text":"<p>               Bases: <code>Protocol[ModelParamsLikeT]</code></p> <p>Methods:</p> Name Description <code>__splifft_stem_selection_plan__</code>"},{"location":"api/models/#splifft.models.SupportsStemSelection.__splifft_stem_selection_plan__","title":"__splifft_stem_selection_plan__  <code>classmethod</code>","text":"<pre><code>__splifft_stem_selection_plan__(\n    model_params: ModelParamsLikeT,\n    output_stem_names: tuple[ModelOutputStemName, ...],\n) -&gt; StemSelectionPlan[ModelParamsLikeT]\n</code></pre> Source code in <code>src/splifft/models/__init__.py</code> <pre><code>@classmethod\ndef __splifft_stem_selection_plan__(\n    cls,\n    model_params: ModelParamsLikeT,\n    output_stem_names: tuple[t.ModelOutputStemName, ...],\n) -&gt; StemSelectionPlan[ModelParamsLikeT]: ...\n</code></pre>"},{"location":"api/models/#splifft.models.ModelMetadata","title":"ModelMetadata  <code>dataclass</code>","text":"<pre><code>ModelMetadata(\n    model_type: ModelType,\n    params: type[ModelParamsLikeT],\n    model: type[ModelT],\n)\n</code></pre> <p>               Bases: <code>Generic[ModelT, ModelParamsLikeT]</code></p> <p>Metadata about a model, including its type, parameter class, and model class.</p> <p>Methods:</p> Name Description <code>from_module</code> <p>Dynamically import a model named <code>X</code> and its parameter dataclass <code>XParams</code> under a</p> <p>Attributes:</p> Name Type Description <code>model_type</code> <code>ModelType</code> <code>params</code> <code>type[ModelParamsLikeT]</code> <code>model</code> <code>type[ModelT]</code>"},{"location":"api/models/#splifft.models.ModelMetadata.model_type","title":"model_type  <code>instance-attribute</code>","text":"<pre><code>model_type: ModelType\n</code></pre>"},{"location":"api/models/#splifft.models.ModelMetadata.params","title":"params  <code>instance-attribute</code>","text":"<pre><code>params: type[ModelParamsLikeT]\n</code></pre>"},{"location":"api/models/#splifft.models.ModelMetadata.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: type[ModelT]\n</code></pre>"},{"location":"api/models/#splifft.models.ModelMetadata.from_module","title":"from_module  <code>classmethod</code>","text":"<pre><code>from_module(\n    module_name: str,\n    model_cls_name: str,\n    *,\n    model_type: ModelType,\n    package: str | None = None,\n) -&gt; ModelMetadata[Module, ModelParamsLike]\n</code></pre> <p>Dynamically import a model named <code>X</code> and its parameter dataclass <code>XParams</code> under a given module name (e.g. <code>splifft.models.bs_roformer</code>).</p> <p>Parameters:</p> Name Type Description Default <code>model_cls_name</code> <code>str</code> <p>The name of the model class to import, e.g. <code>BSRoformer</code>.</p> required <code>module_name</code> <code>str</code> <p>The name of the module to import, e.g. <code>splifft.models.bs_roformer</code>.</p> required <code>model_type</code> <code>ModelType</code> <p>The type of the model, e.g. <code>bs_roformer</code>.</p> required <code>package</code> <code>str | None</code> <p>The package to use as the anchor point from which to resolve the relative import. to an absolute import. This is only required when performing a relative import.</p> <code>None</code> Source code in <code>src/splifft/models/__init__.py</code> <pre><code>@classmethod\ndef from_module(\n    cls,\n    module_name: str,\n    model_cls_name: str,\n    *,\n    model_type: t.ModelType,\n    package: str | None = None,\n) -&gt; ModelMetadata[nn.Module, ModelParamsLike]:\n    \"\"\"\n    Dynamically import a model named `X` and its parameter dataclass `XParams` under a\n    given module name (e.g. `splifft.models.bs_roformer`).\n\n    :param model_cls_name: The name of the model class to import, e.g. `BSRoformer`.\n    :param module_name: The name of the module to import, e.g. `splifft.models.bs_roformer`.\n    :param model_type: The type of the model, e.g. `bs_roformer`.\n    :param package: The package to use as the anchor point from which to resolve the relative import.\n    to an absolute import. This is only required when performing a relative import.\n    \"\"\"\n    _loc = f\"{module_name=} under {package=}\"\n    try:\n        module = importlib.import_module(module_name, package)\n    except ImportError as e:\n        raise ValueError(f\"failed to find or import module for {_loc}\") from e\n\n    params_cls_name = f\"{model_cls_name}Params\"\n    model_cls = getattr(module, model_cls_name, None)\n    params_cls = getattr(module, params_cls_name, None)\n    if model_cls is None or params_cls is None:\n        raise AttributeError(\n            f\"expected to find a class named `{params_cls_name}` in {_loc}, but it was not found.\"\n        )\n\n    return ModelMetadata(\n        model_type=model_type,\n        model=model_cls,\n        params=params_cls,\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.pesto","title":"pesto","text":"<p>PESTO: Pitch Estimation with Self-supervised Transposition-equivariant Objective.</p> <p>See: https://github.com/SonyCSLParis/pesto, https://arxiv.org/abs/2309.02265</p> <p>Classes:</p> Name Description <code>PestoParams</code> <code>ToeplitzLinear</code> <code>Resnet1d</code> <p>Compact 1D CNN used by PESTO to decode HCQT frames into activations.</p> <code>ConfidenceClassifier</code> <p>Frame-level voiced/unvoiced confidence head.</p> <code>Pesto</code> <p>PESTO inference head over externally computed HCQT features.</p> <p>Functions:</p> Name Description <code>reduce_activations</code> <p>Reduce per-bin probabilities to scalar pitch per frame.</p>"},{"location":"api/models/#splifft.models.pesto.PestoParams","title":"PestoParams  <code>dataclass</code>","text":"<pre><code>PestoParams(\n    chunk_size: ChunkSize,\n    output_stem_names: tuple[ModelOutputStemName, ...],\n    reduction: Literal[\"argmax\", \"mean\", \"alwa\"] = \"alwa\",\n    convert_to_freq: bool = True,\n    crop_freq_bins_bottom: Ge0[int] = 16,\n    crop_freq_bins_top: Ge0[int] = 16,\n    n_chan_input: Gt0[int] = 1,\n    n_chan_layers: tuple[Gt0[int], ...] = (\n        40,\n        30,\n        30,\n        10,\n        3,\n    ),\n    n_prefilt_layers: Gt0[int] = 3,\n    prefilt_kernel_size: Gt0[int] = 39,\n    residual: bool = True,\n    n_bins_in: Gt0[int] = 219,\n    output_dim: Gt0[int] = 384,\n    activation_fn: Literal[\n        \"relu\", \"silu\", \"leaky\"\n    ] = \"leaky\",\n    a_lrelu: Ge0[float] = 0.3,\n    p_dropout: Dropout = 0.2,\n    bins_per_semitone: Gt0[int] = 3,\n)\n</code></pre> <p>               Bases: <code>ModelParamsLike</code></p> <p>Attributes:</p> Name Type Description <code>chunk_size</code> <code>ChunkSize</code> <code>output_stem_names</code> <code>tuple[ModelOutputStemName, ...]</code> <code>reduction</code> <code>Literal['argmax', 'mean', 'alwa']</code> <code>convert_to_freq</code> <code>bool</code> <code>crop_freq_bins_bottom</code> <code>Ge0[int]</code> <code>crop_freq_bins_top</code> <code>Ge0[int]</code> <code>n_chan_input</code> <code>Gt0[int]</code> <code>n_chan_layers</code> <code>tuple[Gt0[int], ...]</code> <code>n_prefilt_layers</code> <code>Gt0[int]</code> <code>prefilt_kernel_size</code> <code>Gt0[int]</code> <code>residual</code> <code>bool</code> <code>n_bins_in</code> <code>Gt0[int]</code> <code>output_dim</code> <code>Gt0[int]</code> <code>activation_fn</code> <code>Literal['relu', 'silu', 'leaky']</code> <code>a_lrelu</code> <code>Ge0[float]</code> <code>p_dropout</code> <code>Dropout</code> <code>bins_per_semitone</code> <code>Gt0[int]</code> <code>input_channels</code> <code>ModelInputChannels</code> <code>input_type</code> <code>ModelInputType</code> <code>output_type</code> <code>ModelOutputType</code> <code>inference_archetype</code> <code>InferenceArchetype</code>"},{"location":"api/models/#splifft.models.pesto.PestoParams.chunk_size","title":"chunk_size  <code>instance-attribute</code>","text":"<pre><code>chunk_size: ChunkSize\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.output_stem_names","title":"output_stem_names  <code>instance-attribute</code>","text":"<pre><code>output_stem_names: tuple[ModelOutputStemName, ...]\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.reduction","title":"reduction  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>reduction: Literal['argmax', 'mean', 'alwa'] = 'alwa'\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.convert_to_freq","title":"convert_to_freq  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>convert_to_freq: bool = True\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.crop_freq_bins_bottom","title":"crop_freq_bins_bottom  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>crop_freq_bins_bottom: Ge0[int] = 16\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.crop_freq_bins_top","title":"crop_freq_bins_top  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>crop_freq_bins_top: Ge0[int] = 16\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.n_chan_input","title":"n_chan_input  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_chan_input: Gt0[int] = 1\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.n_chan_layers","title":"n_chan_layers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_chan_layers: tuple[Gt0[int], ...] = (40, 30, 30, 10, 3)\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.n_prefilt_layers","title":"n_prefilt_layers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_prefilt_layers: Gt0[int] = 3\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.prefilt_kernel_size","title":"prefilt_kernel_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prefilt_kernel_size: Gt0[int] = 39\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.residual","title":"residual  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>residual: bool = True\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.n_bins_in","title":"n_bins_in  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_bins_in: Gt0[int] = 219\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.output_dim","title":"output_dim  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_dim: Gt0[int] = 384\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.activation_fn","title":"activation_fn  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>activation_fn: Literal['relu', 'silu', 'leaky'] = 'leaky'\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.a_lrelu","title":"a_lrelu  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>a_lrelu: Ge0[float] = 0.3\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.p_dropout","title":"p_dropout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>p_dropout: Dropout = 0.2\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.bins_per_semitone","title":"bins_per_semitone  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bins_per_semitone: Gt0[int] = 3\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.input_channels","title":"input_channels  <code>property</code>","text":"<pre><code>input_channels: ModelInputChannels\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.input_type","title":"input_type  <code>property</code>","text":"<pre><code>input_type: ModelInputType\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.output_type","title":"output_type  <code>property</code>","text":"<pre><code>output_type: ModelOutputType\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.PestoParams.inference_archetype","title":"inference_archetype  <code>property</code>","text":"<pre><code>inference_archetype: InferenceArchetype\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.ToeplitzLinear","title":"ToeplitzLinear","text":"<pre><code>ToeplitzLinear(in_features: int, out_features: int)\n</code></pre> <p>               Bases: <code>Conv1d</code></p> <p>Methods:</p> Name Description <code>forward</code> Source code in <code>src/splifft/models/pesto.py</code> <pre><code>def __init__(self, in_features: int, out_features: int):\n    super().__init__(\n        in_channels=1,\n        out_channels=1,\n        kernel_size=in_features + out_features - 1,\n        padding=out_features - 1,\n        bias=False,\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.ToeplitzLinear.forward","title":"forward","text":"<pre><code>forward(input: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/pesto.py</code> <pre><code>def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n    return super().forward(input.unsqueeze(-2)).squeeze(-2)\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.Resnet1d","title":"Resnet1d","text":"<pre><code>Resnet1d(\n    *,\n    n_chan_input: int = 1,\n    n_chan_layers: tuple[int, ...] = (40, 30, 30, 10, 3),\n    n_prefilt_layers: int = 3,\n    prefilt_kernel_size: int = 39,\n    residual: bool = True,\n    n_bins_in: int = 219,\n    output_dim: int = 384,\n    activation_fn: Literal[\n        \"relu\", \"silu\", \"leaky\"\n    ] = \"leaky\",\n    a_lrelu: float = 0.3,\n    p_dropout: float = 0.2,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Compact 1D CNN used by PESTO to decode HCQT frames into activations.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>layernorm</code> <code>conv1</code> <code>n_prefilt_layers</code> <code>prefilt_layers</code> <code>residual</code> <code>conv_layers</code> <code>flatten</code> <code>fc</code> <code>final_norm</code> Source code in <code>src/splifft/models/pesto.py</code> <pre><code>def __init__(\n    self,\n    *,\n    n_chan_input: int = 1,\n    n_chan_layers: tuple[int, ...] = (40, 30, 30, 10, 3),\n    n_prefilt_layers: int = 3,\n    prefilt_kernel_size: int = 39,\n    residual: bool = True,\n    n_bins_in: int = 219,\n    output_dim: int = 384,\n    activation_fn: Literal[\"relu\", \"silu\", \"leaky\"] = \"leaky\",\n    a_lrelu: float = 0.3,\n    p_dropout: float = 0.2,\n):\n    super().__init__()\n\n    activation_layer: Callable[[], nn.Module]\n    if activation_fn == \"relu\":\n        activation_layer = nn.ReLU\n    elif activation_fn == \"silu\":\n        activation_layer = nn.SiLU\n    elif activation_fn == \"leaky\":\n        activation_layer = partial(nn.LeakyReLU, negative_slope=a_lrelu)\n    else:\n        raise ValueError(f\"unsupported activation_fn={activation_fn!r}\")\n\n    n_ch = list(n_chan_layers)\n    if len(n_ch) &lt; 5:\n        n_ch.append(1)\n\n    self.layernorm = nn.LayerNorm(normalized_shape=[n_chan_input, n_bins_in])\n\n    prefilt_padding = prefilt_kernel_size // 2\n    self.conv1 = nn.Sequential(\n        nn.Conv1d(\n            in_channels=n_chan_input,\n            out_channels=n_ch[0],\n            kernel_size=prefilt_kernel_size,\n            padding=prefilt_padding,\n            stride=1,\n        ),\n        activation_layer(),\n        nn.Dropout(p=p_dropout),\n    )\n    self.n_prefilt_layers = n_prefilt_layers\n    self.prefilt_layers = nn.ModuleList(\n        [\n            nn.Sequential(\n                nn.Conv1d(\n                    in_channels=n_ch[0],\n                    out_channels=n_ch[0],\n                    kernel_size=prefilt_kernel_size,\n                    padding=prefilt_padding,\n                    stride=1,\n                ),\n                activation_layer(),\n                nn.Dropout(p=p_dropout),\n            )\n            for _ in range(n_prefilt_layers - 1)\n        ]\n    )\n    self.residual = residual\n\n    conv_layers: list[nn.Module] = []\n    for i in range(len(n_ch) - 1):\n        conv_layers.extend(\n            [\n                nn.Conv1d(\n                    in_channels=n_ch[i],\n                    out_channels=n_ch[i + 1],\n                    kernel_size=1,\n                    padding=0,\n                    stride=1,\n                ),\n                activation_layer(),\n                nn.Dropout(p=p_dropout),\n            ]\n        )\n    self.conv_layers = nn.Sequential(*conv_layers)\n\n    self.flatten = nn.Flatten(start_dim=1)\n    self.fc = ToeplitzLinear(n_bins_in * n_ch[-1], output_dim)\n    self.final_norm = nn.Softmax(dim=-1)\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.Resnet1d.layernorm","title":"layernorm  <code>instance-attribute</code>","text":"<pre><code>layernorm = LayerNorm(\n    normalized_shape=[n_chan_input, n_bins_in]\n)\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.Resnet1d.conv1","title":"conv1  <code>instance-attribute</code>","text":"<pre><code>conv1 = Sequential(\n    Conv1d(\n        in_channels=n_chan_input,\n        out_channels=n_ch[0],\n        kernel_size=prefilt_kernel_size,\n        padding=prefilt_padding,\n        stride=1,\n    ),\n    activation_layer(),\n    Dropout(p=p_dropout),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.Resnet1d.n_prefilt_layers","title":"n_prefilt_layers  <code>instance-attribute</code>","text":"<pre><code>n_prefilt_layers = n_prefilt_layers\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.Resnet1d.prefilt_layers","title":"prefilt_layers  <code>instance-attribute</code>","text":"<pre><code>prefilt_layers = ModuleList(\n    [\n        (\n            Sequential(\n                Conv1d(\n                    in_channels=n_ch[0],\n                    out_channels=n_ch[0],\n                    kernel_size=prefilt_kernel_size,\n                    padding=prefilt_padding,\n                    stride=1,\n                ),\n                activation_layer(),\n                Dropout(p=p_dropout),\n            )\n        )\n        for _ in (range(n_prefilt_layers - 1))\n    ]\n)\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.Resnet1d.residual","title":"residual  <code>instance-attribute</code>","text":"<pre><code>residual = residual\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.Resnet1d.conv_layers","title":"conv_layers  <code>instance-attribute</code>","text":"<pre><code>conv_layers = Sequential(*conv_layers)\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.Resnet1d.flatten","title":"flatten  <code>instance-attribute</code>","text":"<pre><code>flatten = Flatten(start_dim=1)\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.Resnet1d.fc","title":"fc  <code>instance-attribute</code>","text":"<pre><code>fc = ToeplitzLinear(n_bins_in * n_ch[-1], output_dim)\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.Resnet1d.final_norm","title":"final_norm  <code>instance-attribute</code>","text":"<pre><code>final_norm = Softmax(dim=-1)\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.Resnet1d.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/pesto.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    x = self.layernorm(x)\n\n    x = self.conv1(x)\n    for i in range(0, self.n_prefilt_layers - 1):\n        prefilt_layer = self.prefilt_layers[i]\n        if self.residual:\n            x = prefilt_layer(x) + x\n        else:\n            x = prefilt_layer(x)\n\n    x = self.conv_layers(x)\n    x = self.flatten(x)\n    y_pred = self.fc(x)\n    return cast(torch.Tensor, self.final_norm(y_pred))\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.ConfidenceClassifier","title":"ConfidenceClassifier","text":"<pre><code>ConfidenceClassifier()\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Frame-level voiced/unvoiced confidence head.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>conv</code> <code>linear</code> Source code in <code>src/splifft/models/pesto.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n    self.conv = nn.Conv1d(1, 1, 39, stride=3)\n    self.linear = nn.Linear(72, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.ConfidenceClassifier.conv","title":"conv  <code>instance-attribute</code>","text":"<pre><code>conv = Conv1d(1, 1, 39, stride=3)\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.ConfidenceClassifier.linear","title":"linear  <code>instance-attribute</code>","text":"<pre><code>linear = Linear(72, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.ConfidenceClassifier.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/pesto.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    geometric_mean = x.log().mean(dim=-1, keepdim=True).exp()\n    arithmetic_mean = x.mean(dim=-1, keepdim=True).clamp_(min=1e-8)\n    flatness = geometric_mean / arithmetic_mean\n\n    x = F.relu(self.conv(x.unsqueeze(1)).squeeze(1))\n    return torch.sigmoid(self.linear(torch.cat((x, flatness), dim=-1))).squeeze(-1)\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.reduce_activations","title":"reduce_activations","text":"<pre><code>reduce_activations(\n    activations: Tensor, reduction: str = \"alwa\"\n) -&gt; Tensor\n</code></pre> <p>Reduce per-bin probabilities to scalar pitch per frame.</p> Source code in <code>src/splifft/models/pesto.py</code> <pre><code>def reduce_activations(activations: torch.Tensor, reduction: str = \"alwa\") -&gt; torch.Tensor:\n    \"\"\"Reduce per-bin probabilities to scalar pitch per frame.\"\"\"\n    device = activations.device\n    num_bins = int(activations.size(-1))\n\n    bps, rem = divmod(num_bins, 128)\n    if rem != 0:\n        raise ValueError(f\"expected output_dim to be divisible by 128, got {num_bins}\")\n\n    if reduction == \"argmax\":\n        pred = activations.argmax(dim=-1)\n        return pred.float() / bps\n\n    all_pitches = torch.arange(num_bins, dtype=torch.float, device=device).div_(bps)\n    if reduction == \"mean\":\n        return torch.matmul(activations, all_pitches)\n\n    if reduction == \"alwa\":\n        center_bin = activations.argmax(dim=-1, keepdim=True)\n        window = torch.arange(1, 2 * bps, device=device) - bps\n        indices = (center_bin + window).clamp_(min=0, max=num_bins - 1)\n        cropped_activations = activations.gather(-1, indices)\n        cropped_pitches = all_pitches.unsqueeze(0).expand_as(activations).gather(-1, indices)\n        return (cropped_activations * cropped_pitches).sum(dim=-1) / cropped_activations.sum(dim=-1)\n\n    raise ValueError(f\"unknown reduction={reduction!r}\")\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.Pesto","title":"Pesto","text":"<pre><code>Pesto(cfg: PestoParams)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>PESTO inference head over externally computed HCQT features.</p> <p>Input contract: tensor of shape <code>(batch, time, feature_dim)</code> where <code>feature_dim = harmonics * freq_bins</code> in dB log-magnitude HCQT.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>cfg</code> <code>encoder</code> <code>confidence</code> Source code in <code>src/splifft/models/pesto.py</code> <pre><code>def __init__(self, cfg: PestoParams):\n    super().__init__()\n    self.cfg = cfg\n    self.encoder = Resnet1d(\n        n_chan_input=cfg.n_chan_input,\n        n_chan_layers=cfg.n_chan_layers,\n        n_prefilt_layers=cfg.n_prefilt_layers,\n        prefilt_kernel_size=cfg.prefilt_kernel_size,\n        residual=cfg.residual,\n        n_bins_in=cfg.n_bins_in,\n        output_dim=cfg.output_dim,\n        activation_fn=cfg.activation_fn,\n        a_lrelu=cfg.a_lrelu,\n        p_dropout=cfg.p_dropout,\n    )\n    self.confidence = ConfidenceClassifier()\n\n    self.register_buffer(\"shift\", torch.zeros((), dtype=torch.float), persistent=True)\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.Pesto.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = cfg\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.Pesto.encoder","title":"encoder  <code>instance-attribute</code>","text":"<pre><code>encoder = Resnet1d(\n    n_chan_input=n_chan_input,\n    n_chan_layers=n_chan_layers,\n    n_prefilt_layers=n_prefilt_layers,\n    prefilt_kernel_size=prefilt_kernel_size,\n    residual=residual,\n    n_bins_in=n_bins_in,\n    output_dim=output_dim,\n    activation_fn=activation_fn,\n    a_lrelu=a_lrelu,\n    p_dropout=p_dropout,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.Pesto.confidence","title":"confidence  <code>instance-attribute</code>","text":"<pre><code>confidence = ConfidenceClassifier()\n</code></pre>"},{"location":"api/models/#splifft.models.pesto.Pesto.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; dict[str, Tensor]\n</code></pre> Source code in <code>src/splifft/models/pesto.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; dict[str, torch.Tensor]:\n    if x.ndim != 3:\n        raise ValueError(\n            f\"expected `(batch,time,feature_dim)` input, got shape={tuple(x.shape)}\"\n        )\n\n    total_bins = (\n        self.cfg.n_bins_in + self.cfg.crop_freq_bins_bottom + self.cfg.crop_freq_bins_top\n    )\n    expected_feature_dim = self.cfg.n_chan_input * total_bins\n    if x.shape[-1] != expected_feature_dim:\n        raise ValueError(\n            \"invalid PESTO feature dimension: \"\n            f\"expected {expected_feature_dim} (= n_chan_input * (n_bins_in + crop_bottom + crop_top)), \"\n            f\"got {x.shape[-1]}\"\n        )\n\n    batch_size, num_frames, _feature_dim = x.shape\n    x = x.view(batch_size, num_frames, self.cfg.n_chan_input, total_bins)\n    x = x.flatten(0, 1)  # (B*T, H, F)\n\n    # match reference implementation: convert dB back to linear energy and derive\n    # confidence + volume from pre-crop HCQT bins.\n    energy = x.mul(torch.log(torch.tensor(10.0, device=x.device, dtype=x.dtype)) / 10.0).exp()\n    confidence_energy = energy.mean(dim=1)\n    volume = energy.sum(dim=-1).mean(dim=-1)\n    confidence = self.confidence(confidence_energy)\n\n    x = self._crop_cqt(x)\n    activations = self.encoder(x)\n\n    activations = activations.view(batch_size, num_frames, activations.size(-1))\n    confidence = confidence.view(batch_size, num_frames)\n    volume = volume.view(batch_size, num_frames)\n\n    shift_tensor = cast(torch.Tensor, self.shift)\n    shift_bins = int(torch.round(shift_tensor * self.cfg.bins_per_semitone).item())\n    activations = activations.roll(-shift_bins, dims=-1)\n\n    pitch = reduce_activations(activations, reduction=self.cfg.reduction)\n    if self.cfg.convert_to_freq:\n        pitch = 440 * 2 ** ((pitch - 69) / 12)\n\n    return {\n        \"pitch\": pitch,\n        \"confidence\": confidence,\n        \"volume\": volume,\n        \"activations\": activations,\n    }\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c","title":"mdx23c","text":"<p>MDX23C.</p> <p>See: https://arxiv.org/pdf/2306.09382</p> <p>Classes:</p> Name Description <code>MDX23CParams</code> <code>Upscale</code> <code>Downscale</code> <code>MDX23C</code> <p>Functions:</p> Name Description <code>get_norm</code> <code>get_act</code> <code>build_tfc_tdf</code>"},{"location":"api/models/#splifft.models.mdx23c.MDX23CParams","title":"MDX23CParams  <code>dataclass</code>","text":"<pre><code>MDX23CParams(\n    chunk_size: ChunkSize,\n    output_stem_names: tuple[ModelOutputStemName, ...],\n    dim_f: Gt0[int],\n    num_subbands: Gt0[int],\n    num_scales: Gt0[int],\n    scale: tuple[Gt0[int], ...],\n    num_blocks_per_scale: Gt0[int],\n    hidden_channels: Gt0[int],\n    growth: Gt0[int],\n    bottleneck_factor: Gt0[int],\n    norm_type: Literal[\"BatchNorm\", \"InstanceNorm\"]\n    | str = \"InstanceNorm\",\n    act_type: Literal[\"gelu\", \"relu\", \"elu\"] | str = \"gelu\",\n    stereo: bool = True,\n)\n</code></pre> <p>               Bases: <code>ModelParamsLike</code></p> <p>Attributes:</p> Name Type Description <code>chunk_size</code> <code>ChunkSize</code> <code>output_stem_names</code> <code>tuple[ModelOutputStemName, ...]</code> <code>dim_f</code> <code>Gt0[int]</code> <p>The size of the frequency dimension fed into the network. </p> <code>num_subbands</code> <code>Gt0[int]</code> <code>num_scales</code> <code>Gt0[int]</code> <code>scale</code> <code>tuple[Gt0[int], ...]</code> <p>Downscaling factor per scale.</p> <code>num_blocks_per_scale</code> <code>Gt0[int]</code> <code>hidden_channels</code> <code>Gt0[int]</code> <p>Base number of channels.</p> <code>growth</code> <code>Gt0[int]</code> <p>Channel growth per scale.</p> <code>bottleneck_factor</code> <code>Gt0[int]</code> <code>norm_type</code> <code>Literal['BatchNorm', 'InstanceNorm'] | str</code> <code>act_type</code> <code>Literal['gelu', 'relu', 'elu'] | str</code> <code>stereo</code> <code>bool</code> <code>input_channels</code> <code>ModelInputChannels</code> <code>input_type</code> <code>ModelInputType</code> <code>output_type</code> <code>ModelOutputType</code> <code>inference_archetype</code> <code>InferenceArchetype</code>"},{"location":"api/models/#splifft.models.mdx23c.MDX23CParams.chunk_size","title":"chunk_size  <code>instance-attribute</code>","text":"<pre><code>chunk_size: ChunkSize\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23CParams.output_stem_names","title":"output_stem_names  <code>instance-attribute</code>","text":"<pre><code>output_stem_names: tuple[ModelOutputStemName, ...]\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23CParams.dim_f","title":"dim_f  <code>instance-attribute</code>","text":"<pre><code>dim_f: Gt0[int]\n</code></pre> <p>The size of the frequency dimension fed into the network.  Usually smaller than <code>n_fft // 2 + 1</code>.</p>"},{"location":"api/models/#splifft.models.mdx23c.MDX23CParams.num_subbands","title":"num_subbands  <code>instance-attribute</code>","text":"<pre><code>num_subbands: Gt0[int]\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23CParams.num_scales","title":"num_scales  <code>instance-attribute</code>","text":"<pre><code>num_scales: Gt0[int]\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23CParams.scale","title":"scale  <code>instance-attribute</code>","text":"<pre><code>scale: tuple[Gt0[int], ...]\n</code></pre> <p>Downscaling factor per scale.</p>"},{"location":"api/models/#splifft.models.mdx23c.MDX23CParams.num_blocks_per_scale","title":"num_blocks_per_scale  <code>instance-attribute</code>","text":"<pre><code>num_blocks_per_scale: Gt0[int]\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23CParams.hidden_channels","title":"hidden_channels  <code>instance-attribute</code>","text":"<pre><code>hidden_channels: Gt0[int]\n</code></pre> <p>Base number of channels.</p>"},{"location":"api/models/#splifft.models.mdx23c.MDX23CParams.growth","title":"growth  <code>instance-attribute</code>","text":"<pre><code>growth: Gt0[int]\n</code></pre> <p>Channel growth per scale.</p>"},{"location":"api/models/#splifft.models.mdx23c.MDX23CParams.bottleneck_factor","title":"bottleneck_factor  <code>instance-attribute</code>","text":"<pre><code>bottleneck_factor: Gt0[int]\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23CParams.norm_type","title":"norm_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>norm_type: Literal[\"BatchNorm\", \"InstanceNorm\"] | str = (\n    \"InstanceNorm\"\n)\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23CParams.act_type","title":"act_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>act_type: Literal['gelu', 'relu', 'elu'] | str = 'gelu'\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23CParams.stereo","title":"stereo  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stereo: bool = True\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23CParams.input_channels","title":"input_channels  <code>property</code>","text":"<pre><code>input_channels: ModelInputChannels\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23CParams.input_type","title":"input_type  <code>property</code>","text":"<pre><code>input_type: ModelInputType\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23CParams.output_type","title":"output_type  <code>property</code>","text":"<pre><code>output_type: ModelOutputType\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23CParams.inference_archetype","title":"inference_archetype  <code>property</code>","text":"<pre><code>inference_archetype: InferenceArchetype\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.get_norm","title":"get_norm","text":"<pre><code>get_norm(norm_type: str, channels: int) -&gt; Module\n</code></pre> Source code in <code>src/splifft/models/mdx23c.py</code> <pre><code>def get_norm(norm_type: str, channels: int) -&gt; nn.Module:\n    if norm_type == \"BatchNorm\":\n        return nn.BatchNorm2d(channels)\n    elif norm_type == \"InstanceNorm\":\n        return nn.InstanceNorm2d(channels, affine=True)\n    elif \"GroupNorm\" in norm_type:\n        g = int(norm_type.replace(\"GroupNorm\", \"\"))\n        return nn.GroupNorm(num_groups=g, num_channels=channels)\n    return nn.Identity()\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.get_act","title":"get_act","text":"<pre><code>get_act(act_type: str) -&gt; Module\n</code></pre> Source code in <code>src/splifft/models/mdx23c.py</code> <pre><code>def get_act(act_type: str) -&gt; nn.Module:\n    if act_type == \"gelu\":\n        return nn.GELU()\n    elif act_type == \"relu\":\n        return nn.ReLU()\n    elif act_type.startswith(\"elu\"):\n        try:\n            alpha = float(act_type.replace(\"elu\", \"\"))\n        except ValueError:\n            alpha = 1.0\n        return nn.ELU(alpha)\n    raise ValueError(f\"unknown activation: {act_type}\")\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.build_tfc_tdf","title":"build_tfc_tdf","text":"<pre><code>build_tfc_tdf(\n    in_c: int,\n    c: int,\n    blocks_per_scale: int,\n    f: int,\n    bn: int,\n    norm_type: str,\n    act_type: str,\n) -&gt; TfcTdf\n</code></pre> Source code in <code>src/splifft/models/mdx23c.py</code> <pre><code>def build_tfc_tdf(\n    in_c: int,\n    c: int,\n    blocks_per_scale: int,\n    f: int,\n    bn: int,\n    norm_type: str,\n    act_type: str,\n) -&gt; TfcTdf:\n    return TfcTdf(\n        in_channels=in_c,\n        out_channels=c,\n        num_blocks=blocks_per_scale,\n        f_bins=f,\n        bottleneck_factor=bn,\n        norm_factory=lambda channels: get_norm(norm_type, channels),\n        act_factory=lambda: get_act(act_type),\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.Upscale","title":"Upscale","text":"<pre><code>Upscale(\n    in_c: int,\n    out_c: int,\n    scale: tuple[int, int],\n    norm_type: str,\n    act_type: str,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>conv</code> Source code in <code>src/splifft/models/mdx23c.py</code> <pre><code>def __init__(\n    self,\n    in_c: int,\n    out_c: int,\n    scale: tuple[int, int],\n    norm_type: str,\n    act_type: str,\n):\n    super().__init__()\n    self.conv = nn.Sequential(\n        get_norm(norm_type, in_c),\n        get_act(act_type),\n        nn.ConvTranspose2d(\n            in_channels=in_c,\n            out_channels=out_c,\n            kernel_size=scale,\n            stride=scale,\n            bias=False,\n        ),\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.Upscale.conv","title":"conv  <code>instance-attribute</code>","text":"<pre><code>conv = Sequential(\n    get_norm(norm_type, in_c),\n    get_act(act_type),\n    ConvTranspose2d(\n        in_channels=in_c,\n        out_channels=out_c,\n        kernel_size=scale,\n        stride=scale,\n        bias=False,\n    ),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.Upscale.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/mdx23c.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    return self.conv(x)  # type: ignore\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.Downscale","title":"Downscale","text":"<pre><code>Downscale(\n    in_c: int,\n    out_c: int,\n    scale: tuple[int, int],\n    norm_type: str,\n    act_type: str,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>conv</code> Source code in <code>src/splifft/models/mdx23c.py</code> <pre><code>def __init__(\n    self,\n    in_c: int,\n    out_c: int,\n    scale: tuple[int, int],\n    norm_type: str,\n    act_type: str,\n):\n    super().__init__()\n    self.conv = nn.Sequential(\n        get_norm(norm_type, in_c),\n        get_act(act_type),\n        nn.Conv2d(\n            in_channels=in_c,\n            out_channels=out_c,\n            kernel_size=scale,\n            stride=scale,\n            bias=False,\n        ),\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.Downscale.conv","title":"conv  <code>instance-attribute</code>","text":"<pre><code>conv = Sequential(\n    get_norm(norm_type, in_c),\n    get_act(act_type),\n    Conv2d(\n        in_channels=in_c,\n        out_channels=out_c,\n        kernel_size=scale,\n        stride=scale,\n        bias=False,\n    ),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.Downscale.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/mdx23c.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    return self.conv(x)  # type: ignore\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23C","title":"MDX23C","text":"<pre><code>MDX23C(cfg: MDX23CParams)\n</code></pre> <p>               Bases: <code>Module</code>, <code>SupportsStemSelection[MDX23CParams]</code></p> <p>Methods:</p> Name Description <code>cac2cws</code> <code>cws2cac</code> <code>forward</code> <p>:param x: input spectrogram (B, F*S, T, 2)</p> <code>__splifft_stem_selection_plan__</code> <p>Slice <code>final_conv.2</code> per requested stem.</p> <p>Attributes:</p> Name Type Description <code>cfg</code> <code>num_target_instruments</code> <code>audio_channels</code> <code>num_subbands</code> <code>first_conv</code> <code>encoder_blocks</code> <code>bottleneck_block</code> <code>decoder_blocks</code> <code>final_conv</code> Source code in <code>src/splifft/models/mdx23c.py</code> <pre><code>def __init__(self, cfg: MDX23CParams):\n    super().__init__()\n    self.cfg = cfg\n    if len(cfg.scale) != 2:\n        raise ValueError(f\"expected `scale` to have 2 elements, got {cfg.scale}\")\n    scale_2d = (cfg.scale[0], cfg.scale[1])\n    self.num_target_instruments = len(cfg.output_stem_names)\n    self.audio_channels = 2 if cfg.stereo else 1\n    self.num_subbands = cfg.num_subbands\n\n    dim_c = self.num_subbands * self.audio_channels * 2\n    n = cfg.num_scales\n    blocks_per_scale = cfg.num_blocks_per_scale\n    c = cfg.hidden_channels\n    g = cfg.growth\n    bn = cfg.bottleneck_factor\n    f = cfg.dim_f // self.num_subbands\n\n    self.first_conv = nn.Conv2d(dim_c, c, 1, 1, 0, bias=False)\n\n    self.encoder_blocks = nn.ModuleList()\n    for _ in range(n):\n        block = nn.Module()\n        block.tfc_tdf = build_tfc_tdf(\n            c, c, blocks_per_scale, f, bn, cfg.norm_type, cfg.act_type\n        )\n        block.downscale = Downscale(c, c + g, scale_2d, cfg.norm_type, cfg.act_type)\n        f = f // scale_2d[1]\n        c += g\n        self.encoder_blocks.append(block)\n\n    self.bottleneck_block = build_tfc_tdf(\n        c, c, blocks_per_scale, f, bn, cfg.norm_type, cfg.act_type\n    )\n\n    self.decoder_blocks = nn.ModuleList()\n    for _ in range(n):\n        block = nn.Module()\n        block.upscale = Upscale(c, c - g, scale_2d, cfg.norm_type, cfg.act_type)\n        f = f * scale_2d[1]\n        c -= g\n        block.tfc_tdf = build_tfc_tdf(\n            2 * c, c, blocks_per_scale, f, bn, cfg.norm_type, cfg.act_type\n        )\n        self.decoder_blocks.append(block)\n\n    self.final_conv = nn.Sequential(\n        nn.Conv2d(c + dim_c, c, 1, 1, 0, bias=False),\n        get_act(cfg.act_type),\n        nn.Conv2d(c, self.num_target_instruments * dim_c, 1, 1, 0, bias=False),\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23C.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = cfg\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23C.num_target_instruments","title":"num_target_instruments  <code>instance-attribute</code>","text":"<pre><code>num_target_instruments = len(output_stem_names)\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23C.audio_channels","title":"audio_channels  <code>instance-attribute</code>","text":"<pre><code>audio_channels = 2 if stereo else 1\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23C.num_subbands","title":"num_subbands  <code>instance-attribute</code>","text":"<pre><code>num_subbands = num_subbands\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23C.first_conv","title":"first_conv  <code>instance-attribute</code>","text":"<pre><code>first_conv = Conv2d(dim_c, c, 1, 1, 0, bias=False)\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23C.encoder_blocks","title":"encoder_blocks  <code>instance-attribute</code>","text":"<pre><code>encoder_blocks = ModuleList()\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23C.bottleneck_block","title":"bottleneck_block  <code>instance-attribute</code>","text":"<pre><code>bottleneck_block = build_tfc_tdf(\n    c, c, blocks_per_scale, f, bn, norm_type, act_type\n)\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23C.decoder_blocks","title":"decoder_blocks  <code>instance-attribute</code>","text":"<pre><code>decoder_blocks = ModuleList()\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23C.final_conv","title":"final_conv  <code>instance-attribute</code>","text":"<pre><code>final_conv = Sequential(\n    Conv2d(c + dim_c, c, 1, 1, 0, bias=False),\n    get_act(act_type),\n    Conv2d(\n        c,\n        num_target_instruments * dim_c,\n        1,\n        1,\n        0,\n        bias=False,\n    ),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23C.cac2cws","title":"cac2cws","text":"<pre><code>cac2cws(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/mdx23c.py</code> <pre><code>def cac2cws(self, x: Tensor) -&gt; Tensor:\n    return rearrange(x, \"b c (k f) t -&gt; b (c k) f t\", k=self.num_subbands)\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23C.cws2cac","title":"cws2cac","text":"<pre><code>cws2cac(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/mdx23c.py</code> <pre><code>def cws2cac(self, x: Tensor) -&gt; Tensor:\n    return rearrange(x, \"b (c k) f t -&gt; b c (k f) t\", k=self.num_subbands)\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23C.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input spectrogram (B, F*S, T, 2)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output spectrogram (B, N, F*S, T, 2)</p> Source code in <code>src/splifft/models/mdx23c.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    :param x: input spectrogram (B, F*S, T, 2)\n    :return: output spectrogram (B, N, F*S, T, 2)\n    \"\"\"\n    b, fs, t, ri = x.shape\n    if ri != 2:\n        raise ValueError(f\"expected final complex axis of size 2, got {ri}\")\n    f_full = fs // self.audio_channels\n    if fs % self.audio_channels != 0:\n        raise ValueError(\n            f\"expected frequency-channel axis divisible by audio channels ({self.audio_channels}), \"\n            f\"got {fs}\"\n        )\n\n    x_in = rearrange(\n        x,\n        \"b (f s) t ri -&gt; b (s ri) f t\",\n        s=self.audio_channels,\n        ri=2,\n    )\n    x_in = x_in[..., : self.cfg.dim_f, :]\n    mix = x_in = self.cac2cws(x_in)\n    first_conv_out = x_in = self.first_conv(x_in)\n    x_in = rearrange(x_in, \"b c f t -&gt; b c t f\")\n\n    encoder_outputs = []\n    for block in self.encoder_blocks:\n        x_in = block.tfc_tdf(x_in)  # type: ignore\n        encoder_outputs.append(x_in)\n        x_in = block.downscale(x_in)  # type: ignore\n\n    x_in = self.bottleneck_block(x_in)\n\n    for block in self.decoder_blocks:\n        x_in = block.upscale(x_in)  # type: ignore\n        x_in = torch.cat([x_in, encoder_outputs.pop()], 1)\n        x_in = block.tfc_tdf(x_in)  # type: ignore\n\n    x_in = rearrange(x_in, \"b c t f -&gt; b c f t\")\n    x_in = x_in * first_conv_out\n    x_in = self.final_conv(torch.cat([mix, x_in], 1))\n    x_in = self.cws2cac(x_in)\n\n    x_in = rearrange(\n        x_in,\n        \"b (n c) f t -&gt; b n c f t\",\n        n=self.num_target_instruments,\n    )\n\n    if f_full &gt; self.cfg.dim_f:\n        pad_size = f_full - self.cfg.dim_f\n        x_in = torch.nn.functional.pad(x_in, (0, 0, 0, pad_size))\n\n    x_in = rearrange(\n        x_in,\n        \"b n (s ri) f t -&gt; b n (f s) t ri\",\n        s=self.audio_channels,\n        ri=2,\n    )\n\n    return x_in\n</code></pre>"},{"location":"api/models/#splifft.models.mdx23c.MDX23C.__splifft_stem_selection_plan__","title":"__splifft_stem_selection_plan__  <code>classmethod</code>","text":"<pre><code>__splifft_stem_selection_plan__(\n    model_params: MDX23CParams,\n    output_stem_names: tuple[ModelOutputStemName, ...],\n) -&gt; StemSelectionPlan[MDX23CParams]\n</code></pre> <p>Slice <code>final_conv.2</code> per requested stem.</p> Source code in <code>src/splifft/models/mdx23c.py</code> <pre><code>@classmethod\ndef __splifft_stem_selection_plan__(\n    cls,\n    model_params: MDX23CParams,\n    output_stem_names: tuple[t.ModelOutputStemName, ...],\n) -&gt; StemSelectionPlan[MDX23CParams]:\n    \"\"\"Slice `final_conv.2` per requested stem.\"\"\"\n\n    full_stem_names = tuple(model_params.output_stem_names)\n    requested_stem_names = tuple(output_stem_names)\n    if requested_stem_names == full_stem_names:\n        return StemSelectionPlan(\n            model_params=model_params,\n            output_stem_names=full_stem_names,\n        )\n\n    selected_stem_indices = tuple(full_stem_names.index(name) for name in requested_stem_names)\n    dim_c = model_params.num_subbands * (2 if model_params.stereo else 1) * 2\n\n    def state_dict_transform(state_dict: dict[str, Tensor]) -&gt; dict[str, Tensor]:\n        key = \"final_conv.2.weight\"\n        if key not in state_dict:\n            return state_dict\n\n        transformed = dict(state_dict)\n        weight = transformed[key]\n        grouped = weight.reshape(len(full_stem_names), dim_c, *weight.shape[1:])\n        selected = grouped.index_select(\n            0,\n            torch.tensor(selected_stem_indices, device=weight.device),\n        )\n        transformed[key] = selected.reshape(\n            len(requested_stem_names) * dim_c, *weight.shape[1:]\n        )\n        return transformed\n\n    return StemSelectionPlan(\n        model_params=replace(model_params, output_stem_names=requested_stem_names),\n        output_stem_names=requested_stem_names,\n        state_dict_transform=state_dict_transform,\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this","title":"beat_this","text":"<p>Beat This! Beat Tracker.</p> <ul> <li>See: https://arxiv.org/abs/2407.21658</li> <li>Adapted from: https://github.com/CPJKU/beat_this.</li> <li>License: MIT</li> </ul> <p>Classes:</p> Name Description <code>BeatThisParams</code> <code>PartialFTTransformer</code> <p>Takes a (batch, channels, freqs, time) input, applies self-attention and</p> <code>SumHead</code> <code>Head</code> <code>BeatThis</code>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams","title":"BeatThisParams  <code>dataclass</code>","text":"<pre><code>BeatThisParams(\n    chunk_size: ChunkSize,\n    output_stem_names: tuple[ModelOutputStemName, ...],\n    spect_dim: Gt0[int] = 128,\n    transformer_dim: Gt0[int] = 512,\n    ff_mult: Gt0[int] = 4,\n    n_layers: Gt0[int] = 6,\n    head_dim: Gt0[int] = 32,\n    stem_dim: Gt0[int] = 32,\n    dropout_frontend: Dropout = 0.1,\n    dropout_transformer: Dropout = 0.2,\n    sum_head: bool = True,\n    partial_transformers: bool = True,\n    rotary_embed_dtype: TorchDtype | None = None,\n    transformer_residual_dtype: TorchDtype | None = None,\n    log_mel_hop_length: HopSize = 441,\n)\n</code></pre> <p>               Bases: <code>ModelParamsLike</code></p> <p>Attributes:</p> Name Type Description <code>chunk_size</code> <code>ChunkSize</code> <code>output_stem_names</code> <code>tuple[ModelOutputStemName, ...]</code> <code>spect_dim</code> <code>Gt0[int]</code> <code>transformer_dim</code> <code>Gt0[int]</code> <code>ff_mult</code> <code>Gt0[int]</code> <code>n_layers</code> <code>Gt0[int]</code> <code>head_dim</code> <code>Gt0[int]</code> <code>stem_dim</code> <code>Gt0[int]</code> <code>dropout_frontend</code> <code>Dropout</code> <code>dropout_transformer</code> <code>Dropout</code> <code>sum_head</code> <code>bool</code> <code>partial_transformers</code> <code>bool</code> <code>rotary_embed_dtype</code> <code>TorchDtype | None</code> <code>transformer_residual_dtype</code> <code>TorchDtype | None</code> <code>log_mel_hop_length</code> <code>HopSize</code> <p>The hop length of the log mel spectrogram.</p> <code>input_channels</code> <code>ModelInputChannels</code> <code>input_type</code> <code>ModelInputType</code> <code>output_type</code> <code>ModelOutputType</code> <code>inference_archetype</code> <code>InferenceArchetype</code>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams.chunk_size","title":"chunk_size  <code>instance-attribute</code>","text":"<pre><code>chunk_size: ChunkSize\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams.output_stem_names","title":"output_stem_names  <code>instance-attribute</code>","text":"<pre><code>output_stem_names: tuple[ModelOutputStemName, ...]\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams.spect_dim","title":"spect_dim  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>spect_dim: Gt0[int] = 128\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams.transformer_dim","title":"transformer_dim  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>transformer_dim: Gt0[int] = 512\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams.ff_mult","title":"ff_mult  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ff_mult: Gt0[int] = 4\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams.n_layers","title":"n_layers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_layers: Gt0[int] = 6\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams.head_dim","title":"head_dim  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>head_dim: Gt0[int] = 32\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams.stem_dim","title":"stem_dim  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stem_dim: Gt0[int] = 32\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams.dropout_frontend","title":"dropout_frontend  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dropout_frontend: Dropout = 0.1\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams.dropout_transformer","title":"dropout_transformer  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dropout_transformer: Dropout = 0.2\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams.sum_head","title":"sum_head  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sum_head: bool = True\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams.partial_transformers","title":"partial_transformers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>partial_transformers: bool = True\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams.rotary_embed_dtype","title":"rotary_embed_dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>rotary_embed_dtype: TorchDtype | None = None\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams.transformer_residual_dtype","title":"transformer_residual_dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>transformer_residual_dtype: TorchDtype | None = None\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams.log_mel_hop_length","title":"log_mel_hop_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_mel_hop_length: HopSize = 441\n</code></pre> <p>The hop length of the log mel spectrogram.</p> <p>Warning</p> <p>This must match the <code>hop_length</code> in the <code>LogMelConfig</code> to ensure the rotary embeddings are sized correctly for the sequence length.</p>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams.input_channels","title":"input_channels  <code>property</code>","text":"<pre><code>input_channels: ModelInputChannels\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams.input_type","title":"input_type  <code>property</code>","text":"<pre><code>input_type: ModelInputType\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams.output_type","title":"output_type  <code>property</code>","text":"<pre><code>output_type: ModelOutputType\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThisParams.inference_archetype","title":"inference_archetype  <code>property</code>","text":"<pre><code>inference_archetype: InferenceArchetype\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.PartialFTTransformer","title":"PartialFTTransformer","text":"<pre><code>PartialFTTransformer(\n    dim: int,\n    dim_head: int,\n    n_head: int,\n    rotary_embed_f: RotaryEmbedding,\n    rotary_embed_t: RotaryEmbedding,\n    dropout: float,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Takes a (batch, channels, freqs, time) input, applies self-attention and a feed-forward block once across frequencies and once across time.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>attnF</code> <code>ffF</code> <code>attnT</code> <code>ffT</code> Source code in <code>src/splifft/models/beat_this.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    dim_head: int,\n    n_head: int,\n    rotary_embed_f: RotaryEmbedding,\n    rotary_embed_t: RotaryEmbedding,\n    dropout: float,\n):\n    super().__init__()\n    self.attnF = Attention(\n        dim, heads=n_head, dim_head=dim_head, dropout=dropout, rotary_embed=rotary_embed_f\n    )\n    self.ffF = FeedForward(dim, dropout=dropout)\n    self.attnT = Attention(\n        dim, heads=n_head, dim_head=dim_head, dropout=dropout, rotary_embed=rotary_embed_t\n    )\n    self.ffT = FeedForward(dim, dropout=dropout)\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.PartialFTTransformer.attnF","title":"attnF  <code>instance-attribute</code>","text":"<pre><code>attnF = Attention(\n    dim,\n    heads=n_head,\n    dim_head=dim_head,\n    dropout=dropout,\n    rotary_embed=rotary_embed_f,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.PartialFTTransformer.ffF","title":"ffF  <code>instance-attribute</code>","text":"<pre><code>ffF = FeedForward(dim, dropout=dropout)\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.PartialFTTransformer.attnT","title":"attnT  <code>instance-attribute</code>","text":"<pre><code>attnT = Attention(\n    dim,\n    heads=n_head,\n    dim_head=dim_head,\n    dropout=dropout,\n    rotary_embed=rotary_embed_t,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.PartialFTTransformer.ffT","title":"ffT  <code>instance-attribute</code>","text":"<pre><code>ffT = FeedForward(dim, dropout=dropout)\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.PartialFTTransformer.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/beat_this.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    b = len(x)\n    x = rearrange(x, \"b c f t -&gt; (b t) f c\")\n    x = x + self.attnF(x)\n    x = x + self.ffF(x)\n    x = rearrange(x, \"(b t) f c -&gt; (b f) t c\", b=b)\n    x = x + self.attnT(x)\n    x = x + self.ffT(x)\n    x = rearrange(x, \"(b f) t c -&gt; b c f t\", b=b)\n    return x\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.SumHead","title":"SumHead","text":"<pre><code>SumHead(input_dim: int)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>beat_downbeat_lin</code> Source code in <code>src/splifft/models/beat_this.py</code> <pre><code>def __init__(self, input_dim: int):\n    super().__init__()\n    self.beat_downbeat_lin = nn.Linear(input_dim, 2)\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.SumHead.beat_downbeat_lin","title":"beat_downbeat_lin  <code>instance-attribute</code>","text":"<pre><code>beat_downbeat_lin = Linear(input_dim, 2)\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.SumHead.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/beat_this.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    beat_downbeat = self.beat_downbeat_lin(x)\n    beat, downbeat = rearrange(beat_downbeat, \"b t c -&gt; c b t\", c=2)\n\n    # aggregate beats and downbeats prediction\n    # autocast to float16 disabled to avoid numerical issues causing NaNs\n    device_type = beat.device.type\n    if device_type != \"mps\" and torch.amp.is_autocast_available(device_type):  # type: ignore\n        disable_autocast = torch.autocast(device_type, enabled=False)\n    else:\n        disable_autocast = contextlib.nullcontext()\n\n    with disable_autocast:\n        beat = beat.float() + downbeat.float()\n    return torch.stack([beat, downbeat], dim=0)  # (2, b, t)\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.Head","title":"Head","text":"<pre><code>Head(input_dim: int)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>beat_downbeat_lin</code> Source code in <code>src/splifft/models/beat_this.py</code> <pre><code>def __init__(self, input_dim: int):\n    super().__init__()\n    self.beat_downbeat_lin = nn.Linear(input_dim, 2)\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.Head.beat_downbeat_lin","title":"beat_downbeat_lin  <code>instance-attribute</code>","text":"<pre><code>beat_downbeat_lin = Linear(input_dim, 2)\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.Head.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/beat_this.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    beat_downbeat = self.beat_downbeat_lin(x)\n    beat, downbeat = rearrange(beat_downbeat, \"b t c -&gt; c b t\", c=2)\n    return torch.stack([beat, downbeat], dim=0)\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThis","title":"BeatThis","text":"<pre><code>BeatThis(cfg: BeatThisParams)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>:param x: Input spectrogram (B, T, F)</p> <p>Attributes:</p> Name Type Description <code>spect_dim</code> <code>frontend</code> <code>transformer_blocks</code> <code>task_heads</code> Source code in <code>src/splifft/models/beat_this.py</code> <pre><code>def __init__(self, cfg: BeatThisParams):\n    super().__init__()\n    self.spect_dim = cfg.spect_dim\n\n    max_frames = cfg.chunk_size // cfg.log_mel_hop_length\n    rotary_embed_t = RotaryEmbedding(\n        seq_len=max_frames,  # by default 1500 frames * 441 = 661500 samples\n        dim_head=cfg.head_dim,\n        dtype=cfg.rotary_embed_dtype,\n    )\n\n    # NOTE: removed rearrange from original impl to standardise input as (B, F, T)\n    stem = nn.Sequential(\n        OrderedDict(\n            bn1d=nn.BatchNorm1d(cfg.spect_dim),\n            add_channel=Rearrange(\"b f t -&gt; b 1 f t\"),\n            conv2d=nn.Conv2d(\n                in_channels=1,\n                out_channels=cfg.stem_dim,\n                kernel_size=(4, 3),\n                stride=(4, 1),\n                padding=(0, 1),\n                bias=False,\n            ),\n            bn2d=nn.BatchNorm2d(cfg.stem_dim),\n            activation=nn.GELU(),\n        )\n    )\n\n    spect_dim = cfg.spect_dim // 4\n    dim = cfg.stem_dim\n    frontend_blocks = []\n    for _ in range(3):\n        rotary_embed_f = RotaryEmbedding(\n            seq_len=spect_dim,\n            dim_head=cfg.head_dim,\n            dtype=cfg.rotary_embed_dtype,\n        )\n        frontend_blocks.append(\n            nn.Sequential(\n                OrderedDict(\n                    partial=(\n                        PartialFTTransformer(\n                            dim=dim,\n                            dim_head=cfg.head_dim,\n                            n_head=dim // cfg.head_dim,\n                            rotary_embed_f=rotary_embed_f,\n                            rotary_embed_t=rotary_embed_t,\n                            dropout=cfg.dropout_frontend,\n                        )\n                        if cfg.partial_transformers\n                        else nn.Identity()\n                    ),\n                    conv2d=nn.Conv2d(\n                        in_channels=dim,\n                        out_channels=dim * 2,\n                        kernel_size=(2, 3),\n                        stride=(2, 1),\n                        padding=(0, 1),\n                        bias=False,\n                    ),\n                    norm=nn.BatchNorm2d(dim * 2),\n                    activation=nn.GELU(),\n                )\n            )\n        )\n        dim *= 2\n        spect_dim //= 2\n\n    self.frontend = nn.Sequential(\n        OrderedDict(\n            stem=stem,\n            blocks=nn.Sequential(*frontend_blocks),\n            concat=Rearrange(\"b c f t -&gt; b t (c f)\"),\n            linear=nn.Linear(dim * spect_dim, cfg.transformer_dim),\n        )\n    )\n\n    n_heads = cfg.transformer_dim // cfg.head_dim\n    # TODO check if this is really equivalent\n    self.transformer_blocks = Transformer(\n        dim=cfg.transformer_dim,\n        depth=cfg.n_layers,\n        dim_head=cfg.head_dim,\n        heads=n_heads,\n        attn_dropout=cfg.dropout_transformer,\n        ff_dropout=cfg.dropout_transformer,\n        ff_mult=cfg.ff_mult,\n        norm_output=True,\n        rotary_embed=rotary_embed_t,\n        transformer_residual_dtype=cfg.transformer_residual_dtype,\n    )\n\n    if cfg.sum_head:\n        self.task_heads = SumHead(cfg.transformer_dim)\n    else:\n        self.task_heads = Head(cfg.transformer_dim)\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThis.spect_dim","title":"spect_dim  <code>instance-attribute</code>","text":"<pre><code>spect_dim = spect_dim\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThis.frontend","title":"frontend  <code>instance-attribute</code>","text":"<pre><code>frontend = Sequential(\n    OrderedDict(\n        stem=stem,\n        blocks=Sequential(*frontend_blocks),\n        concat=Rearrange(\"b c f t -&gt; b t (c f)\"),\n        linear=Linear(dim * spect_dim, transformer_dim),\n    )\n)\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThis.transformer_blocks","title":"transformer_blocks  <code>instance-attribute</code>","text":"<pre><code>transformer_blocks = Transformer(\n    dim=transformer_dim,\n    depth=n_layers,\n    dim_head=head_dim,\n    heads=n_heads,\n    attn_dropout=dropout_transformer,\n    ff_dropout=dropout_transformer,\n    ff_mult=ff_mult,\n    norm_output=True,\n    rotary_embed=rotary_embed_t,\n    transformer_residual_dtype=transformer_residual_dtype,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThis.task_heads","title":"task_heads  <code>instance-attribute</code>","text":"<pre><code>task_heads = SumHead(transformer_dim)\n</code></pre>"},{"location":"api/models/#splifft.models.beat_this.BeatThis.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input spectrogram (B, T, F)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Logits (2, B, T) -&gt; [Beats, Downbeats]</p> Source code in <code>src/splifft/models/beat_this.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    :param x: Input spectrogram (B, T, F)\n    :return: Logits (2, B, T) -&gt; [Beats, Downbeats]\n    \"\"\"\n    if x.ndim != 3:\n        raise ValueError(f\"expected 3D spectrogram input, got shape={tuple(x.shape)}\")\n    if x.shape[-1] != self.spect_dim:\n        raise ValueError(\n            f\"expected beat_this input shape `(B,T,{self.spect_dim})`, got {tuple(x.shape)}\"\n        )\n\n    x = x.transpose(1, 2)\n    x = self.frontend(x)\n    x = self.transformer_blocks(x)\n    x = self.task_heads(x)\n    return x\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer","title":"bs_roformer","text":"<p>Band-Split RoPE Transformer</p> <ul> <li>BS-RoFormer: https://arxiv.org/abs/2309.02612</li> <li>Mel-RoFormer: https://arxiv.org/abs/2409.04702</li> </ul> <p>This implementation merges the two versions found in <code>lucidrains</code>'s implementation However, there are several inconsistencies:</p> <ul> <li><code>MLP</code> was defined differently in each file, one that has <code>depth - 1</code> hidden layers and one that   has <code>depth</code> layers.</li> <li><code>BSRoformer</code> applies one final RMSNorm after the entire stack of transformer layers, while the   <code>MelBandRoformer</code> applies an RMSNorm at the end of each axial transformer block (time_transformer,   freq_transformer, etc.) and has no final normalization layer.</li> </ul> <p>Since fixing the three inconsistencies upstream is too big of a breaking change, we inherit them to maintain compatibility with community-trained models. See: https://web.archive.org/web/20260112010548/https://github.com/lucidrains/BS-RoFormer/issues/48.</p> <p>To avoid dependency bloat, we do not:</p> <ul> <li>depend on <code>rotary_embeddings_torch</code></li> <li>implement <code>hyper_connections</code></li> <li>implement learned value residual learning</li> </ul> <p>Classes:</p> Name Description <code>FixedBandsConfig</code> <code>MelBandsConfig</code> <code>BaselineMaskEstimatorConfig</code> <code>AxialRefinerLargeV2MaskEstimatorConfig</code> <p>unwa large-v2 head. Adds a small axial transformer refiner inside the mask head.</p> <code>HyperAceResidualV1MaskEstimatorConfig</code> <p>unwa HyperACE v1 residual head compatibility config.</p> <code>HyperAceResidualV2MaskEstimatorConfig</code> <p>UNWA HyperACE v2 residual head compatibility config.</p> <code>BSRoformerParams</code> <code>RMSNorm</code> <code>RMSNormWithEps</code> <code>RotaryEmbedding</code> <p>A performance-oriented version of RoPE.</p> <code>FeedForward</code> <code>Attention</code> <code>LinearAttention</code> <p>this flavor of linear attention proposed in https://arxiv.org/abs/2106.09681 by El-Nouby et al.</p> <code>Transformer</code> <code>BandSplit</code> <code>MaskEstimator</code> <code>AxialRefinerLargeV2MaskEstimator</code> <code>HyperAceResidualMaskEstimator</code> <code>BSRoformer</code> <p>Functions:</p> Name Description <code>l2norm</code> <code>rms_norm</code> <code>mlp</code> <p>Attributes:</p> Name Type Description <code>DEFAULT_FREQS_PER_BANDS</code> <code>MaskEstimatorConfig</code>"},{"location":"api/models/#splifft.models.bs_roformer.DEFAULT_FREQS_PER_BANDS","title":"DEFAULT_FREQS_PER_BANDS  <code>module-attribute</code>","text":"<pre><code>DEFAULT_FREQS_PER_BANDS = (\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    2,\n    4,\n    4,\n    4,\n    4,\n    4,\n    4,\n    4,\n    4,\n    4,\n    4,\n    4,\n    4,\n    12,\n    12,\n    12,\n    12,\n    12,\n    12,\n    12,\n    12,\n    24,\n    24,\n    24,\n    24,\n    24,\n    24,\n    24,\n    24,\n    48,\n    48,\n    48,\n    48,\n    48,\n    48,\n    48,\n    48,\n    128,\n    129,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.FixedBandsConfig","title":"FixedBandsConfig  <code>dataclass</code>","text":"<pre><code>FixedBandsConfig(\n    kind: Literal[\"fixed\"],\n    freqs_per_bands: tuple[Gt0[int], ...] = (\n        lambda: DEFAULT_FREQS_PER_BANDS\n    )(),\n)\n</code></pre> <p>Attributes:</p> Name Type Description <code>kind</code> <code>Literal['fixed']</code> <code>freqs_per_bands</code> <code>tuple[Gt0[int], ...]</code>"},{"location":"api/models/#splifft.models.bs_roformer.FixedBandsConfig.kind","title":"kind  <code>instance-attribute</code>","text":"<pre><code>kind: Literal['fixed']\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.FixedBandsConfig.freqs_per_bands","title":"freqs_per_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>freqs_per_bands: tuple[Gt0[int], ...] = field(\n    default_factory=lambda: DEFAULT_FREQS_PER_BANDS\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.MelBandsConfig","title":"MelBandsConfig  <code>dataclass</code>","text":"<pre><code>MelBandsConfig(\n    kind: Literal[\"mel\"],\n    stft_n_fft: Gt0[int] = 2048,\n    num_bands: Gt0[int] = 60,\n    sample_rate: Gt0[int] = 44100,\n)\n</code></pre> <p>Attributes:</p> Name Type Description <code>kind</code> <code>Literal['mel']</code> <code>stft_n_fft</code> <code>Gt0[int]</code> <code>num_bands</code> <code>Gt0[int]</code> <code>sample_rate</code> <code>Gt0[int]</code>"},{"location":"api/models/#splifft.models.bs_roformer.MelBandsConfig.kind","title":"kind  <code>instance-attribute</code>","text":"<pre><code>kind: Literal['mel']\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.MelBandsConfig.stft_n_fft","title":"stft_n_fft  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stft_n_fft: Gt0[int] = 2048\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.MelBandsConfig.num_bands","title":"num_bands  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_bands: Gt0[int] = 60\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.MelBandsConfig.sample_rate","title":"sample_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sample_rate: Gt0[int] = 44100\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BaselineMaskEstimatorConfig","title":"BaselineMaskEstimatorConfig  <code>dataclass</code>","text":"<pre><code>BaselineMaskEstimatorConfig(\n    kind: Literal[\"baseline\"] = \"baseline\",\n)\n</code></pre> <p>Attributes:</p> Name Type Description <code>kind</code> <code>Literal['baseline']</code>"},{"location":"api/models/#splifft.models.bs_roformer.BaselineMaskEstimatorConfig.kind","title":"kind  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>kind: Literal['baseline'] = 'baseline'\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.AxialRefinerLargeV2MaskEstimatorConfig","title":"AxialRefinerLargeV2MaskEstimatorConfig  <code>dataclass</code>","text":"<pre><code>AxialRefinerLargeV2MaskEstimatorConfig(\n    kind: Literal[\"axial_refiner_large_v2\"],\n    axial_refiner_depth: Gt0[int] = 4,\n)\n</code></pre> <p>unwa large-v2 head. Adds a small axial transformer refiner inside the mask head.</p> <p>Attributes:</p> Name Type Description <code>kind</code> <code>Literal['axial_refiner_large_v2']</code> <code>axial_refiner_depth</code> <code>Gt0[int]</code>"},{"location":"api/models/#splifft.models.bs_roformer.AxialRefinerLargeV2MaskEstimatorConfig.kind","title":"kind  <code>instance-attribute</code>","text":"<pre><code>kind: Literal['axial_refiner_large_v2']\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.AxialRefinerLargeV2MaskEstimatorConfig.axial_refiner_depth","title":"axial_refiner_depth  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>axial_refiner_depth: Gt0[int] = 4\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.HyperAceResidualV1MaskEstimatorConfig","title":"HyperAceResidualV1MaskEstimatorConfig  <code>dataclass</code>","text":"<pre><code>HyperAceResidualV1MaskEstimatorConfig(\n    kind: Literal[\"hyperace_residual_v1\"],\n    num_hyperedges: Gt0[int] | None = None,\n    num_heads: Gt0[int] = 8,\n)\n</code></pre> <p>unwa HyperACE v1 residual head compatibility config.</p> <p>Attributes:</p> Name Type Description <code>kind</code> <code>Literal['hyperace_residual_v1']</code> <code>num_hyperedges</code> <code>Gt0[int] | None</code> <code>num_heads</code> <code>Gt0[int]</code>"},{"location":"api/models/#splifft.models.bs_roformer.HyperAceResidualV1MaskEstimatorConfig.kind","title":"kind  <code>instance-attribute</code>","text":"<pre><code>kind: Literal['hyperace_residual_v1']\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.HyperAceResidualV1MaskEstimatorConfig.num_hyperedges","title":"num_hyperedges  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_hyperedges: Gt0[int] | None = None\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.HyperAceResidualV1MaskEstimatorConfig.num_heads","title":"num_heads  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_heads: Gt0[int] = 8\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.HyperAceResidualV2MaskEstimatorConfig","title":"HyperAceResidualV2MaskEstimatorConfig  <code>dataclass</code>","text":"<pre><code>HyperAceResidualV2MaskEstimatorConfig(\n    kind: Literal[\"hyperace_residual_v2\"],\n    num_hyperedges: Gt0[int] | None = None,\n    num_heads: Gt0[int] = 8,\n)\n</code></pre> <p>UNWA HyperACE v2 residual head compatibility config.</p> <p>Attributes:</p> Name Type Description <code>kind</code> <code>Literal['hyperace_residual_v2']</code> <code>num_hyperedges</code> <code>Gt0[int] | None</code> <code>num_heads</code> <code>Gt0[int]</code>"},{"location":"api/models/#splifft.models.bs_roformer.HyperAceResidualV2MaskEstimatorConfig.kind","title":"kind  <code>instance-attribute</code>","text":"<pre><code>kind: Literal['hyperace_residual_v2']\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.HyperAceResidualV2MaskEstimatorConfig.num_hyperedges","title":"num_hyperedges  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_hyperedges: Gt0[int] | None = None\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.HyperAceResidualV2MaskEstimatorConfig.num_heads","title":"num_heads  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_heads: Gt0[int] = 8\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.MaskEstimatorConfig","title":"MaskEstimatorConfig  <code>module-attribute</code>","text":"<pre><code>MaskEstimatorConfig = (\n    BaselineMaskEstimatorConfig\n    | AxialRefinerLargeV2MaskEstimatorConfig\n    | HyperAceResidualV1MaskEstimatorConfig\n    | HyperAceResidualV2MaskEstimatorConfig\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams","title":"BSRoformerParams  <code>dataclass</code>","text":"<pre><code>BSRoformerParams(\n    chunk_size: ChunkSize,\n    output_stem_names: tuple[ModelOutputStemName, ...],\n    dim: Gt0[int],\n    depth: Gt0[int],\n    band_config: FixedBandsConfig | MelBandsConfig,\n    stft_hop_length: HopSize = 512,\n    stereo: bool = True,\n    time_transformer_depth: Gt0[int] = 1,\n    freq_transformer_depth: Gt0[int] = 1,\n    linear_transformer_depth: Ge0[int] = 0,\n    dim_head: int = 64,\n    heads: Gt0[int] = 8,\n    attn_dropout: Dropout = 0.0,\n    ff_dropout: Dropout = 0.0,\n    ff_mult: Gt0[int] = 4,\n    flash_attn: bool = True,\n    norm_output: bool = False,\n    mask_estimator_depth: Gt0[int] = 2,\n    mlp_expansion_factor: Gt0[int] = 4,\n    mask_estimator: MaskEstimatorConfig = BaselineMaskEstimatorConfig(),\n    use_torch_checkpoint: bool = False,\n    sage_attention: bool = False,\n    use_shared_bias: bool = False,\n    skip_connection: bool = False,\n    rms_norm_eps: Ge0[float] | None = None,\n    rotary_embed_dtype: TorchDtype | None = None,\n    transformer_residual_dtype: TorchDtype | None = None,\n    debug: bool = False,\n)\n</code></pre> <p>               Bases: <code>ModelParamsLike</code></p> <p>Attributes:</p> Name Type Description <code>chunk_size</code> <code>ChunkSize</code> <code>output_stem_names</code> <code>tuple[ModelOutputStemName, ...]</code> <code>dim</code> <code>Gt0[int]</code> <code>depth</code> <code>Gt0[int]</code> <code>band_config</code> <code>FixedBandsConfig | MelBandsConfig</code> <code>stft_hop_length</code> <code>HopSize</code> <code>stereo</code> <code>bool</code> <code>time_transformer_depth</code> <code>Gt0[int]</code> <code>freq_transformer_depth</code> <code>Gt0[int]</code> <code>linear_transformer_depth</code> <code>Ge0[int]</code> <code>dim_head</code> <code>int</code> <code>heads</code> <code>Gt0[int]</code> <code>attn_dropout</code> <code>Dropout</code> <code>ff_dropout</code> <code>Dropout</code> <code>ff_mult</code> <code>Gt0[int]</code> <code>flash_attn</code> <code>bool</code> <code>norm_output</code> <code>bool</code> <p>Note that in <code>lucidrains</code>' implementation, this is set to</p> <code>mask_estimator_depth</code> <code>Gt0[int]</code> <p>The number of hidden layers of the MLP is <code>mask_estimator_depth - 1</code>, that is:</p> <code>mlp_expansion_factor</code> <code>Gt0[int]</code> <code>mask_estimator</code> <code>MaskEstimatorConfig</code> <code>use_torch_checkpoint</code> <code>bool</code> <code>sage_attention</code> <code>bool</code> <code>use_shared_bias</code> <code>bool</code> <code>skip_connection</code> <code>bool</code> <code>rms_norm_eps</code> <code>Ge0[float] | None</code> <code>rotary_embed_dtype</code> <code>TorchDtype | None</code> <code>transformer_residual_dtype</code> <code>TorchDtype | None</code> <code>debug</code> <code>bool</code> <p>Whether to check for nan/inf in model outputs. Keep it off for torch.compile.</p> <code>input_channels</code> <code>ModelInputChannels</code> <code>input_type</code> <code>ModelInputType</code> <code>output_type</code> <code>ModelOutputType</code> <code>inference_archetype</code> <code>InferenceArchetype</code>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.chunk_size","title":"chunk_size  <code>instance-attribute</code>","text":"<pre><code>chunk_size: ChunkSize\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.output_stem_names","title":"output_stem_names  <code>instance-attribute</code>","text":"<pre><code>output_stem_names: tuple[ModelOutputStemName, ...]\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.dim","title":"dim  <code>instance-attribute</code>","text":"<pre><code>dim: Gt0[int]\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.depth","title":"depth  <code>instance-attribute</code>","text":"<pre><code>depth: Gt0[int]\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.band_config","title":"band_config  <code>instance-attribute</code>","text":"<pre><code>band_config: FixedBandsConfig | MelBandsConfig\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.stft_hop_length","title":"stft_hop_length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stft_hop_length: HopSize = 512\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.stereo","title":"stereo  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stereo: bool = True\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.time_transformer_depth","title":"time_transformer_depth  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>time_transformer_depth: Gt0[int] = 1\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.freq_transformer_depth","title":"freq_transformer_depth  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>freq_transformer_depth: Gt0[int] = 1\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.linear_transformer_depth","title":"linear_transformer_depth  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>linear_transformer_depth: Ge0[int] = 0\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.dim_head","title":"dim_head  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dim_head: int = 64\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.heads","title":"heads  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>heads: Gt0[int] = 8\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.attn_dropout","title":"attn_dropout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>attn_dropout: Dropout = 0.0\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.ff_dropout","title":"ff_dropout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ff_dropout: Dropout = 0.0\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.ff_mult","title":"ff_mult  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ff_mult: Gt0[int] = 4\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.flash_attn","title":"flash_attn  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>flash_attn: bool = True\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.norm_output","title":"norm_output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>norm_output: bool = False\n</code></pre> <p>Note that in <code>lucidrains</code>' implementation, this is set to False for <code>bs_roformer</code> but True for <code>mel_roformer</code>!!</p>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.mask_estimator_depth","title":"mask_estimator_depth  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_estimator_depth: Gt0[int] = 2\n</code></pre> <p>The number of hidden layers of the MLP is <code>mask_estimator_depth - 1</code>, that is:</p> <ul> <li>depth = 1: (dim_in, dim_out)</li> <li>depth = 2: (dim_in, dim_hidden, dim_out)</li> </ul> <p>Note that in <code>lucidrains</code>' implementation of mel-band roformers, the number of hidden layers is incorrectly set as <code>mask_estimator_depth</code>. This includes popular models like kim-vocals and all models that use <code>zfturbo</code>'s music-source-separation training.</p> <p>If you are migrating a mel-band roformer's <code>zfturbo</code> configuration, increment the mask_estimator depth by 1.</p>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.mlp_expansion_factor","title":"mlp_expansion_factor  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mlp_expansion_factor: Gt0[int] = 4\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.mask_estimator","title":"mask_estimator  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_estimator: MaskEstimatorConfig = field(\n    default_factory=BaselineMaskEstimatorConfig\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.use_torch_checkpoint","title":"use_torch_checkpoint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_torch_checkpoint: bool = False\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.sage_attention","title":"sage_attention  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sage_attention: bool = False\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.use_shared_bias","title":"use_shared_bias  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_shared_bias: bool = False\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.skip_connection","title":"skip_connection  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>skip_connection: bool = False\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.rms_norm_eps","title":"rms_norm_eps  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>rms_norm_eps: Ge0[float] | None = None\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.rotary_embed_dtype","title":"rotary_embed_dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>rotary_embed_dtype: TorchDtype | None = None\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.transformer_residual_dtype","title":"transformer_residual_dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>transformer_residual_dtype: TorchDtype | None = None\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.debug","title":"debug  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>debug: bool = False\n</code></pre> <p>Whether to check for nan/inf in model outputs. Keep it off for torch.compile.</p>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.input_channels","title":"input_channels  <code>property</code>","text":"<pre><code>input_channels: ModelInputChannels\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.input_type","title":"input_type  <code>property</code>","text":"<pre><code>input_type: ModelInputType\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.output_type","title":"output_type  <code>property</code>","text":"<pre><code>output_type: ModelOutputType\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformerParams.inference_archetype","title":"inference_archetype  <code>property</code>","text":"<pre><code>inference_archetype: InferenceArchetype\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.l2norm","title":"l2norm","text":"<pre><code>l2norm(t: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def l2norm(t: Tensor) -&gt; Tensor:\n    return F.normalize(t, dim=-1, p=2)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RMSNorm","title":"RMSNorm","text":"<pre><code>RMSNorm(dim: int)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>scale</code> <code>gamma</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(self, dim: int):\n    super().__init__()\n    self.scale = dim**0.5\n    self.gamma = nn.Parameter(torch.ones(dim))\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RMSNorm.scale","title":"scale  <code>instance-attribute</code>","text":"<pre><code>scale = dim ** 0.5\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RMSNorm.gamma","title":"gamma  <code>instance-attribute</code>","text":"<pre><code>gamma = Parameter(ones(dim))\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RMSNorm.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    return F.normalize(x, dim=-1) * self.scale * self.gamma  # type: ignore\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RMSNormWithEps","title":"RMSNormWithEps","text":"<pre><code>RMSNormWithEps(\n    dim: int, eps: float = 5.960464477539063e-08\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>scale</code> <code>gamma</code> <code>eps</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(self, dim: int, eps: float = 5.960464477539063e-08):\n    super().__init__()\n    self.scale = dim**0.5\n    self.gamma = nn.Parameter(torch.ones(dim))\n    self.eps = eps\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RMSNormWithEps.scale","title":"scale  <code>instance-attribute</code>","text":"<pre><code>scale = dim ** 0.5\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RMSNormWithEps.gamma","title":"gamma  <code>instance-attribute</code>","text":"<pre><code>gamma = Parameter(ones(dim))\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RMSNormWithEps.eps","title":"eps  <code>instance-attribute</code>","text":"<pre><code>eps = eps\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RMSNormWithEps.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    l2_norm = torch.linalg.norm(x, dim=-1, keepdim=True)\n    denom = torch.maximum(l2_norm, torch.full_like(l2_norm, self.eps))\n    normalized_x = x / denom\n    return normalized_x * self.scale * self.gamma  # type: ignore\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.rms_norm","title":"rms_norm","text":"<pre><code>rms_norm(\n    dim: int, eps: float | None\n) -&gt; RMSNorm | RMSNormWithEps\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def rms_norm(dim: int, eps: float | None) -&gt; RMSNorm | RMSNormWithEps:\n    if eps is None:\n        return RMSNorm(dim)\n    return RMSNormWithEps(dim, eps)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RotaryEmbedding","title":"RotaryEmbedding","text":"<pre><code>RotaryEmbedding(\n    seq_len: int,\n    dim_head: int,\n    *,\n    dtype: dtype | None,\n    theta: int = 10000,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A performance-oriented version of RoPE.</p> <p>Unlike <code>lucidrains</code>' implementation which compute embeddings JIT during the forward pass and caches calls with the same or shorter sequence length, we simply compute them AOT as persistent buffers. To keep the computational graph clean, we do not support dynamic sequence lengths, learned frequencies or length extrapolation.</p> <p>Methods:</p> Name Description <code>rotate_half</code> <code>forward</code> <p>Attributes:</p> Name Type Description <code>cos_emb</code> <code>sin_emb</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(\n    self, seq_len: int, dim_head: int, *, dtype: torch.dtype | None, theta: int = 10000\n):\n    super().__init__()\n    # COMPAT: the original implementation does not generate the embeddings\n    # on the fly, but serialises them in fp16. there are some tiny\n    # differences:\n    # |                     |   from weights  |   generated    |\n    # | ------------------- | --------------- | -------------- |\n    # | cos_emb_time:971,22 | -0.99462890625  | -0.994140625   |\n    # | cos_emb_time:971,23 | -0.99462890625  | -0.994140625   |\n    # | sin_emb_time:727,12 | -0.457763671875 | -0.4580078125  |\n    # | sin_emb_time:727,13 | -0.457763671875 | -0.4580078125  |\n    # | sin_emb_time:825,4  | -0.8544921875   | -0.85400390625 |\n    # | sin_emb_time:825,5  | -0.8544921875   | -0.85400390625 |\n    freqs = 1.0 / (theta ** (torch.arange(0, dim_head, 2).float() / dim_head))\n    t = torch.arange(seq_len)\n    freqs = torch.einsum(\"i,j-&gt;ij\", t, freqs)  # (seq_len, dim / 2)\n    freqs = repeat(freqs, \"... d -&gt; ... (d r)\", r=2)  # (seq_len, dim)\n    self.cos_emb = freqs.cos().to(dtype)\n    self.sin_emb = freqs.sin().to(dtype)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RotaryEmbedding.cos_emb","title":"cos_emb  <code>instance-attribute</code>","text":"<pre><code>cos_emb = to(dtype)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RotaryEmbedding.sin_emb","title":"sin_emb  <code>instance-attribute</code>","text":"<pre><code>sin_emb = to(dtype)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RotaryEmbedding.rotate_half","title":"rotate_half","text":"<pre><code>rotate_half(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def rotate_half(self, x: Tensor) -&gt; Tensor:\n    x = rearrange(x, \"... (d r) -&gt; ... d r\", r=2)\n    x1, x2 = x.unbind(dim=-1)\n    x = torch.stack((-x2, x1), dim=-1)\n    return rearrange(x, \"... d r -&gt; ... (d r)\")\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.RotaryEmbedding.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    # x is (batch_eff, heads, seq_len_for_rotation, dim_head)\n    cos_b = self.cos_emb.unsqueeze(0).unsqueeze(0).to(x.device, x.dtype)\n    sin_b = self.sin_emb.unsqueeze(0).unsqueeze(0).to(x.device, x.dtype)\n\n    term1 = x * cos_b\n    term2 = self.rotate_half(x) * sin_b\n\n    # NOTE: original impl performed addition between two f32s but it comes with 30% slowdown\n    # we eliminate it so the addition is performed between two f16s (according to __init__).\n    return term1 + term2\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.FeedForward","title":"FeedForward","text":"<pre><code>FeedForward(\n    dim: int,\n    mult: int = 4,\n    dropout: float = 0.0,\n    rms_norm_eps: float | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>net</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(\n    self, dim: int, mult: int = 4, dropout: float = 0.0, rms_norm_eps: float | None = None\n):\n    super().__init__()\n    dim_inner = int(dim * mult)\n    # NOTE: in the paper: RMSNorm -&gt; FC -&gt; Tanh -&gt; FC -&gt; GLU\n    self.net = nn.Sequential(\n        rms_norm(dim, eps=rms_norm_eps),\n        nn.Linear(dim, dim_inner),\n        nn.GELU(),\n        nn.Dropout(dropout),\n        nn.Linear(dim_inner, dim),\n        nn.Dropout(dropout),\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.FeedForward.net","title":"net  <code>instance-attribute</code>","text":"<pre><code>net = Sequential(\n    rms_norm(dim, eps=rms_norm_eps),\n    Linear(dim, dim_inner),\n    GELU(),\n    Dropout(dropout),\n    Linear(dim_inner, dim),\n    Dropout(dropout),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.FeedForward.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    return cast(Tensor, self.net(x))\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention","title":"Attention","text":"<pre><code>Attention(\n    dim: int,\n    heads: int = 8,\n    dim_head: int = 64,\n    dropout: float = 0.0,\n    shared_qkv_bias: Parameter | None = None,\n    shared_out_bias: Parameter | None = None,\n    rotary_embed: RotaryEmbedding | None = None,\n    flash: bool = True,\n    sage_attention: bool = False,\n    rms_norm_eps: float | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>heads</code> <code>scale</code> <code>rotary_embed</code> <code>attend</code> <code>norm</code> <code>to_qkv</code> <code>to_gates</code> <code>to_out</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    heads: int = 8,\n    dim_head: int = 64,\n    dropout: float = 0.0,\n    shared_qkv_bias: nn.Parameter | None = None,\n    shared_out_bias: nn.Parameter | None = None,\n    rotary_embed: RotaryEmbedding | None = None,\n    flash: bool = True,\n    sage_attention: bool = False,\n    rms_norm_eps: float | None = None,\n):\n    super().__init__()\n    self.heads = heads\n    self.scale = dim_head**-0.5\n    dim_inner = heads * dim_head\n\n    self.rotary_embed = rotary_embed\n\n    if sage_attention:\n        from .utils.attend_sage import AttendSage\n\n        self.attend = AttendSage(flash=flash, dropout=dropout)\n    else:\n        from .utils.attend import Attend\n\n        self.attend = Attend(flash=flash, dropout=dropout)  # type: ignore\n\n    self.norm = rms_norm(dim, eps=rms_norm_eps)\n    self.to_qkv = nn.Linear(dim, dim_inner * 3, bias=(shared_qkv_bias is not None))\n    if shared_qkv_bias is not None:\n        self.to_qkv.bias = shared_qkv_bias\n\n    self.to_gates = nn.Linear(dim, heads)\n\n    self.to_out = nn.Sequential(\n        nn.Linear(dim_inner, dim, bias=(shared_out_bias is not None)),\n        nn.Dropout(dropout),\n    )\n    if shared_out_bias is not None:\n        self.to_out[0].bias = shared_out_bias\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention.heads","title":"heads  <code>instance-attribute</code>","text":"<pre><code>heads = heads\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention.scale","title":"scale  <code>instance-attribute</code>","text":"<pre><code>scale = dim_head ** -0.5\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention.rotary_embed","title":"rotary_embed  <code>instance-attribute</code>","text":"<pre><code>rotary_embed = rotary_embed\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention.attend","title":"attend  <code>instance-attribute</code>","text":"<pre><code>attend = AttendSage(flash=flash, dropout=dropout)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = rms_norm(dim, eps=rms_norm_eps)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention.to_qkv","title":"to_qkv  <code>instance-attribute</code>","text":"<pre><code>to_qkv = Linear(\n    dim, dim_inner * 3, bias=shared_qkv_bias is not None\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention.to_gates","title":"to_gates  <code>instance-attribute</code>","text":"<pre><code>to_gates = Linear(dim, heads)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention.to_out","title":"to_out  <code>instance-attribute</code>","text":"<pre><code>to_out = Sequential(\n    Linear(\n        dim_inner, dim, bias=shared_out_bias is not None\n    ),\n    Dropout(dropout),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Attention.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    x = self.norm(x)\n\n    qkv = self.to_qkv(x)\n    q, k, v = rearrange(qkv, \"b n (qkv h d) -&gt; qkv b h n d\", qkv=3, h=self.heads)\n\n    if self.rotary_embed is not None:\n        q = self.rotary_embed(q)\n        k = self.rotary_embed(k)\n\n    out = self.attend(q, k, v)\n\n    gates = self.to_gates(x)\n    gate_act = gates.sigmoid()\n\n    out = out * rearrange(gate_act, \"b n h -&gt; b h n 1\")\n\n    out = rearrange(out, \"b h n d -&gt; b n (h d)\")\n    out = self.to_out(out)\n    return cast(Tensor, out)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.LinearAttention","title":"LinearAttention","text":"<pre><code>LinearAttention(\n    *,\n    dim: int,\n    dim_head: int = 32,\n    heads: int = 8,\n    scale: int = 8,\n    flash: bool = False,\n    dropout: float = 0.0,\n    sage_attention: bool = False,\n    rms_norm_eps: float | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>this flavor of linear attention proposed in https://arxiv.org/abs/2106.09681 by El-Nouby et al.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>norm</code> <code>to_qkv</code> <code>temperature</code> <code>attend</code> <code>to_out</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(\n    self,\n    *,\n    dim: int,\n    dim_head: int = 32,\n    heads: int = 8,\n    scale: int = 8,\n    flash: bool = False,\n    dropout: float = 0.0,\n    sage_attention: bool = False,\n    rms_norm_eps: float | None = None,\n):\n    super().__init__()\n    dim_inner = dim_head * heads\n    self.norm = rms_norm(dim, eps=rms_norm_eps)\n\n    self.to_qkv = nn.Sequential(\n        nn.Linear(dim, dim_inner * 3, bias=False),\n        Rearrange(\"b n (qkv h d) -&gt; qkv b h d n\", qkv=3, h=heads),\n    )\n\n    self.temperature = nn.Parameter(torch.ones(heads, 1, 1))\n\n    if sage_attention:\n        from .utils.attend_sage import AttendSage\n\n        self.attend = AttendSage(scale=scale, dropout=dropout, flash=flash)\n    else:\n        from .utils.attend import Attend\n\n        self.attend = Attend(scale=scale, dropout=dropout, flash=flash)  # type: ignore\n\n    self.to_out = nn.Sequential(\n        Rearrange(\"b h d n -&gt; b n (h d)\"), nn.Linear(dim_inner, dim, bias=False)\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.LinearAttention.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = rms_norm(dim, eps=rms_norm_eps)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.LinearAttention.to_qkv","title":"to_qkv  <code>instance-attribute</code>","text":"<pre><code>to_qkv = Sequential(\n    Linear(dim, dim_inner * 3, bias=False),\n    Rearrange(\n        \"b n (qkv h d) -&gt; qkv b h d n\", qkv=3, h=heads\n    ),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.LinearAttention.temperature","title":"temperature  <code>instance-attribute</code>","text":"<pre><code>temperature = Parameter(ones(heads, 1, 1))\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.LinearAttention.attend","title":"attend  <code>instance-attribute</code>","text":"<pre><code>attend = AttendSage(\n    scale=scale, dropout=dropout, flash=flash\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.LinearAttention.to_out","title":"to_out  <code>instance-attribute</code>","text":"<pre><code>to_out = Sequential(\n    Rearrange(\"b h d n -&gt; b n (h d)\"),\n    Linear(dim_inner, dim, bias=False),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.LinearAttention.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    x = self.norm(x)\n\n    q, k, v = self.to_qkv(x)\n\n    q, k = map(l2norm, (q, k))\n    q = q * self.temperature.exp()\n\n    out = self.attend(q, k, v)\n\n    return cast(Tensor, self.to_out(out))\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Transformer","title":"Transformer","text":"<pre><code>Transformer(\n    *,\n    dim: int,\n    depth: int,\n    dim_head: int = 64,\n    heads: int = 8,\n    attn_dropout: float = 0.0,\n    ff_dropout: float = 0.0,\n    ff_mult: int = 4,\n    norm_output: bool = True,\n    rotary_embed: RotaryEmbedding | None = None,\n    flash_attn: bool = True,\n    linear_attn: bool = False,\n    sage_attention: bool = False,\n    shared_qkv_bias: Parameter | None = None,\n    shared_out_bias: Parameter | None = None,\n    rms_norm_eps: float | None = None,\n    transformer_residual_dtype: dtype | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>layers</code> <code>transformer_residual_dtype</code> <code>norm</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(\n    self,\n    *,\n    dim: int,\n    depth: int,\n    dim_head: int = 64,\n    heads: int = 8,\n    attn_dropout: float = 0.0,\n    ff_dropout: float = 0.0,\n    ff_mult: int = 4,\n    norm_output: bool = True,\n    rotary_embed: RotaryEmbedding | None = None,\n    flash_attn: bool = True,\n    linear_attn: bool = False,\n    sage_attention: bool = False,\n    shared_qkv_bias: nn.Parameter | None = None,\n    shared_out_bias: nn.Parameter | None = None,\n    rms_norm_eps: float | None = None,\n    transformer_residual_dtype: torch.dtype | None = None,  # COMPAT: float32, see 265\n):\n    super().__init__()\n    self.layers = ModuleList([])\n\n    for _ in range(depth):\n        attn: LinearAttention | Attention\n        if linear_attn:\n            attn = LinearAttention(\n                dim=dim,\n                dim_head=dim_head,\n                heads=heads,\n                dropout=attn_dropout,\n                flash=flash_attn,\n                sage_attention=sage_attention,\n                rms_norm_eps=rms_norm_eps,\n            )\n        else:\n            attn = Attention(\n                dim=dim,\n                dim_head=dim_head,\n                heads=heads,\n                dropout=attn_dropout,\n                shared_qkv_bias=shared_qkv_bias,\n                shared_out_bias=shared_out_bias,\n                rotary_embed=rotary_embed,\n                flash=flash_attn,\n                sage_attention=sage_attention,\n                rms_norm_eps=rms_norm_eps,\n            )\n\n        ff = FeedForward(dim=dim, mult=ff_mult, dropout=ff_dropout, rms_norm_eps=rms_norm_eps)\n        self.layers.append(ModuleList([attn, ff]))\n    self.transformer_residual_dtype = transformer_residual_dtype\n\n    self.norm = rms_norm(dim, eps=rms_norm_eps) if norm_output else nn.Identity()\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Transformer.layers","title":"layers  <code>instance-attribute</code>","text":"<pre><code>layers = ModuleList([])\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Transformer.transformer_residual_dtype","title":"transformer_residual_dtype  <code>instance-attribute</code>","text":"<pre><code>transformer_residual_dtype = transformer_residual_dtype\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Transformer.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = (\n    rms_norm(dim, eps=rms_norm_eps)\n    if norm_output\n    else Identity()\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.Transformer.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    for layer in self.layers:\n        block = cast(ModuleList, layer)\n        attn = block[0]\n        ff = block[1]\n        attn_out = attn(x)\n        if self.transformer_residual_dtype is not None:\n            x = (\n                attn_out.to(self.transformer_residual_dtype)\n                + x.to(self.transformer_residual_dtype)\n            ).to(x.dtype)\n        else:\n            x = attn_out + x\n\n        ff_out = ff(x)\n        if self.transformer_residual_dtype is not None:\n            x = (\n                ff_out.to(self.transformer_residual_dtype)\n                + x.to(self.transformer_residual_dtype)\n            ).to(x.dtype)\n        else:\n            x = ff_out + x\n    return cast(Tensor, self.norm(x))\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BandSplit","title":"BandSplit","text":"<pre><code>BandSplit(\n    dim: int,\n    dim_inputs: tuple[int, ...],\n    rms_norm_eps: float | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>dim_inputs</code> <code>to_features</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(self, dim: int, dim_inputs: tuple[int, ...], rms_norm_eps: float | None = None):\n    super().__init__()\n    self.dim_inputs = dim_inputs\n    self.to_features = ModuleList([])\n\n    for dim_in in dim_inputs:\n        net = nn.Sequential(rms_norm(dim_in, rms_norm_eps), nn.Linear(dim_in, dim))\n        self.to_features.append(net)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BandSplit.dim_inputs","title":"dim_inputs  <code>instance-attribute</code>","text":"<pre><code>dim_inputs = dim_inputs\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BandSplit.to_features","title":"to_features  <code>instance-attribute</code>","text":"<pre><code>to_features = ModuleList([])\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BandSplit.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    x_split = torch.split(x, list(self.dim_inputs), dim=-1)\n    outs = []\n    for split_input, to_feature_net in zip(x_split, self.to_features):\n        split_output = to_feature_net(split_input)\n        outs.append(split_output)\n    return torch.stack(outs, dim=-2)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.mlp","title":"mlp","text":"<pre><code>mlp(\n    dim_in: int,\n    dim_out: int,\n    dim_hidden: int | None = None,\n    depth: int = 1,\n    activation: type[Module] = Tanh,\n) -&gt; Sequential\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def mlp(\n    dim_in: int,\n    dim_out: int,\n    dim_hidden: int | None = None,\n    depth: int = 1,\n    activation: type[Module] = nn.Tanh,\n) -&gt; nn.Sequential:\n    dim_hidden_ = dim_hidden or dim_in\n\n    net: list[Module] = []\n    # NOTE: in lucidrain's impl, `bs_roformer` has `depth - 1` but `mel_roformer` has `depth`\n    num_hidden_layers = depth - 1\n    dims = (dim_in, *((dim_hidden_,) * num_hidden_layers), dim_out)\n\n    for ind, (layer_dim_in, layer_dim_out) in enumerate(zip(dims[:-1], dims[1:])):\n        is_last = ind == (len(dims) - 2)\n\n        net.append(nn.Linear(layer_dim_in, layer_dim_out))\n\n        if is_last:\n            continue\n\n        net.append(activation())\n\n    return nn.Sequential(*net)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.MaskEstimator","title":"MaskEstimator","text":"<pre><code>MaskEstimator(\n    dim: int,\n    dim_inputs: tuple[int, ...],\n    depth: int,\n    mlp_expansion_factor: int,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>dim_inputs</code> <code>to_freqs</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    dim_inputs: tuple[int, ...],\n    depth: int,\n    mlp_expansion_factor: int,\n):\n    super().__init__()\n    self.dim_inputs = dim_inputs\n    self.to_freqs = _build_band_to_freq_mlps(\n        dim=dim,\n        dim_inputs=dim_inputs,\n        depth=depth,\n        mlp_expansion_factor=mlp_expansion_factor,\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.MaskEstimator.dim_inputs","title":"dim_inputs  <code>instance-attribute</code>","text":"<pre><code>dim_inputs = dim_inputs\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.MaskEstimator.to_freqs","title":"to_freqs  <code>instance-attribute</code>","text":"<pre><code>to_freqs = _build_band_to_freq_mlps(\n    dim=dim,\n    dim_inputs=dim_inputs,\n    depth=depth,\n    mlp_expansion_factor=mlp_expansion_factor,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.MaskEstimator.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    x_unbound = x.unbind(dim=-2)\n\n    outs = []\n\n    for band_features, mlp_net in zip(x_unbound, self.to_freqs):\n        freq_out = mlp_net(band_features)\n        outs.append(freq_out)\n\n    return torch.cat(outs, dim=-1)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.AxialRefinerLargeV2MaskEstimator","title":"AxialRefinerLargeV2MaskEstimator","text":"<pre><code>AxialRefinerLargeV2MaskEstimator(\n    dim: int,\n    dim_inputs: tuple[int, ...],\n    mlp_depth: int,\n    mlp_expansion_factor: int,\n    axial_refiner_depth: int,\n    t_frames: int,\n    num_bands: int,\n    rotary_embed_dtype: dtype | None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>dim_inputs</code> <code>to_freqs</code> <code>layers</code> <code>norm</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    dim_inputs: tuple[int, ...],\n    mlp_depth: int,\n    mlp_expansion_factor: int,\n    axial_refiner_depth: int,\n    t_frames: int,\n    num_bands: int,\n    rotary_embed_dtype: torch.dtype | None,\n):\n    super().__init__()\n    self.dim_inputs = dim_inputs\n    self.to_freqs = _build_band_to_freq_mlps(\n        dim=dim,\n        dim_inputs=dim_inputs,\n        depth=mlp_depth,\n        mlp_expansion_factor=mlp_expansion_factor,\n    )\n\n    self.layers = ModuleList([])\n\n    heads = 8\n    dim_head = 64\n\n    time_rotary_embed = RotaryEmbedding(\n        seq_len=t_frames,\n        dim_head=dim_head,\n        dtype=rotary_embed_dtype,\n    )\n    freq_rotary_embed = RotaryEmbedding(\n        seq_len=num_bands,\n        dim_head=dim_head,\n        dtype=rotary_embed_dtype,\n    )\n\n    for _ in range(axial_refiner_depth):\n        self.layers.append(\n            nn.ModuleList(\n                [\n                    Transformer(\n                        dim=dim,\n                        depth=1,\n                        heads=heads,\n                        dim_head=dim_head,\n                        attn_dropout=0.0,\n                        ff_dropout=0.0,\n                        flash_attn=True,\n                        norm_output=False,\n                        rotary_embed=time_rotary_embed,\n                        sage_attention=False,\n                    ),\n                    Transformer(\n                        dim=dim,\n                        depth=1,\n                        heads=heads,\n                        dim_head=dim_head,\n                        attn_dropout=0.0,\n                        ff_dropout=0.0,\n                        flash_attn=True,\n                        norm_output=False,\n                        rotary_embed=freq_rotary_embed,\n                        sage_attention=False,\n                    ),\n                ]\n            )\n        )\n\n    self.norm = RMSNorm(dim)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.AxialRefinerLargeV2MaskEstimator.dim_inputs","title":"dim_inputs  <code>instance-attribute</code>","text":"<pre><code>dim_inputs = dim_inputs\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.AxialRefinerLargeV2MaskEstimator.to_freqs","title":"to_freqs  <code>instance-attribute</code>","text":"<pre><code>to_freqs = _build_band_to_freq_mlps(\n    dim=dim,\n    dim_inputs=dim_inputs,\n    depth=mlp_depth,\n    mlp_expansion_factor=mlp_expansion_factor,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.AxialRefinerLargeV2MaskEstimator.layers","title":"layers  <code>instance-attribute</code>","text":"<pre><code>layers = ModuleList([])\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.AxialRefinerLargeV2MaskEstimator.norm","title":"norm  <code>instance-attribute</code>","text":"<pre><code>norm = RMSNorm(dim)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.AxialRefinerLargeV2MaskEstimator.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    for transformer_block in self.layers:\n        block = cast(ModuleList, transformer_block)\n        time_transformer, freq_transformer = block\n\n        x = rearrange(x, \"b t f d -&gt; b f t d\")\n        x, ps = pack([x], \"* t d\")\n\n        x = time_transformer(x)\n\n        (x,) = unpack(x, ps, \"* t d\")\n        x = rearrange(x, \"b f t d -&gt; b t f d\")\n        x, ps = pack([x], \"* f d\")\n\n        x = freq_transformer(x)\n\n        (x,) = unpack(x, ps, \"* f d\")\n\n    x = self.norm(x)\n\n    x_unbound = x.unbind(dim=-2)\n\n    outs = []\n\n    for band_features, mlp_net in zip(x_unbound, self.to_freqs):\n        freq_out = mlp_net(band_features)\n        outs.append(freq_out)\n\n    return torch.cat(outs, dim=-1)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.HyperAceResidualMaskEstimator","title":"HyperAceResidualMaskEstimator","text":"<pre><code>HyperAceResidualMaskEstimator(\n    dim: int,\n    dim_inputs: tuple[int, ...],\n    depth: int,\n    mlp_expansion_factor: int,\n    segm: Module,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>dim_inputs</code> <code>to_freqs</code> <code>segm</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    dim_inputs: tuple[int, ...],\n    depth: int,\n    mlp_expansion_factor: int,\n    segm: nn.Module,\n):\n    super().__init__()\n    self.dim_inputs = dim_inputs\n    self.to_freqs = _build_band_to_freq_mlps(\n        dim=dim,\n        dim_inputs=dim_inputs,\n        depth=depth,\n        mlp_expansion_factor=mlp_expansion_factor,\n    )\n\n    self.segm = segm\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.HyperAceResidualMaskEstimator.dim_inputs","title":"dim_inputs  <code>instance-attribute</code>","text":"<pre><code>dim_inputs = dim_inputs\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.HyperAceResidualMaskEstimator.to_freqs","title":"to_freqs  <code>instance-attribute</code>","text":"<pre><code>to_freqs = _build_band_to_freq_mlps(\n    dim=dim,\n    dim_inputs=dim_inputs,\n    depth=depth,\n    mlp_expansion_factor=mlp_expansion_factor,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.HyperAceResidualMaskEstimator.segm","title":"segm  <code>instance-attribute</code>","text":"<pre><code>segm = segm\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.HyperAceResidualMaskEstimator.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    y = rearrange(x, \"b t f c -&gt; b c t f\")\n    y = self.segm(y)\n    y = rearrange(y, \"b c t f -&gt; b t (f c)\")\n\n    x_unbound = x.unbind(dim=-2)\n    outs = []\n    for band_features, mlp_net in zip(x_unbound, self.to_freqs):\n        freq_out = mlp_net(band_features)\n        outs.append(freq_out)\n\n    return cast(Tensor, torch.cat(outs, dim=-1) + y)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer","title":"BSRoformer","text":"<pre><code>BSRoformer(cfg: BSRoformerParams)\n</code></pre> <p>               Bases: <code>Module</code>, <code>SupportsStemSelection[BSRoformerParams]</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>:param stft_repr: input spectrogram. shape (b, f*s, t, c)</p> <code>__splifft_stem_selection_plan__</code> <p>Remap <code>mask_estimators.{i}.*</code> state-dict entries to a compact</p> <p>Attributes:</p> Name Type Description <code>stereo</code> <code>audio_channels</code> <code>num_stems</code> <code>use_torch_checkpoint</code> <code>skip_connection</code> <code>layers</code> <code>shared_qkv_bias</code> <code>Parameter | None</code> <code>shared_out_bias</code> <code>Parameter | None</code> <code>is_mel</code> <code>final_norm</code> <code>band_split</code> <code>mask_estimators</code> <code>debug</code> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def __init__(self, cfg: BSRoformerParams):\n    super().__init__()\n    self.stereo = cfg.stereo\n    self.audio_channels = 2 if cfg.stereo else 1\n    self.num_stems = len(cfg.output_stem_names)\n    self.use_torch_checkpoint = cfg.use_torch_checkpoint\n    self.skip_connection = cfg.skip_connection\n\n    self.layers = ModuleList([])\n\n    self.shared_qkv_bias: nn.Parameter | None = None\n    self.shared_out_bias: nn.Parameter | None = None\n    if cfg.use_shared_bias:\n        dim_inner = cfg.heads * cfg.dim_head\n        self.shared_qkv_bias = nn.Parameter(torch.ones(dim_inner * 3))\n        self.shared_out_bias = nn.Parameter(torch.ones(cfg.dim))\n\n    transformer = partial(\n        Transformer,\n        dim=cfg.dim,\n        heads=cfg.heads,\n        dim_head=cfg.dim_head,\n        attn_dropout=cfg.attn_dropout,\n        ff_dropout=cfg.ff_dropout,\n        ff_mult=cfg.ff_mult,\n        flash_attn=cfg.flash_attn,\n        norm_output=cfg.norm_output,\n        sage_attention=cfg.sage_attention,\n        shared_qkv_bias=self.shared_qkv_bias,\n        shared_out_bias=self.shared_out_bias,\n        rms_norm_eps=cfg.rms_norm_eps,\n        transformer_residual_dtype=cfg.transformer_residual_dtype,\n    )\n\n    t_frames = cfg.chunk_size // cfg.stft_hop_length + 1  # e.g. 588800 // 512 + 1 = 1151\n    time_rotary_embed = RotaryEmbedding(\n        seq_len=t_frames, dim_head=cfg.dim_head, dtype=cfg.rotary_embed_dtype\n    )\n\n    if is_mel := isinstance(cfg.band_config, MelBandsConfig):\n        from torchaudio.functional import melscale_fbanks\n\n        mel_cfg = cfg.band_config\n        num_bands = mel_cfg.num_bands\n        freqs = mel_cfg.stft_n_fft // 2 + 1\n        mel_filter_bank = melscale_fbanks(\n            n_freqs=freqs,\n            f_min=0.0,\n            f_max=float(mel_cfg.sample_rate / 2),\n            n_mels=num_bands,\n            sample_rate=mel_cfg.sample_rate,\n            norm=\"slaney\",\n            mel_scale=\"slaney\",\n        ).T\n        # TODO: adopt https://github.com/lucidrains/BS-RoFormer/issues/47\n        mel_filter_bank[0, 0] = 1.0\n        mel_filter_bank[-1, -1] = 1.0\n\n        freqs_per_band_mask = mel_filter_bank &gt; 0\n        assert freqs_per_band_mask.any(dim=0).all(), (\n            \"all frequencies must be covered by at least one band\"\n        )\n\n        repeated_freq_indices = repeat(torch.arange(freqs), \"f -&gt; b f\", b=num_bands)\n        freq_indices = repeated_freq_indices[freqs_per_band_mask]\n        if self.stereo:\n            freq_indices = repeat(freq_indices, \"f -&gt; f s\", s=2)\n            freq_indices = freq_indices * 2 + torch.arange(2)\n            freq_indices = rearrange(freq_indices, \"f s -&gt; (f s)\")\n        self.register_buffer(\"freq_indices\", freq_indices, persistent=False)\n        self.register_buffer(\"freqs_per_band_mask\", freqs_per_band_mask, persistent=False)\n\n        num_freqs_per_band = reduce(freqs_per_band_mask, \"b f -&gt; b\", \"sum\")\n        num_bands_per_freq = reduce(freqs_per_band_mask, \"b f -&gt; f\", \"sum\")\n\n        self.register_buffer(\"num_freqs_per_band\", num_freqs_per_band, persistent=False)\n        self.register_buffer(\"num_bands_per_freq\", num_bands_per_freq, persistent=False)\n\n    elif isinstance(cfg.band_config, FixedBandsConfig):\n        num_freqs_per_band = torch.tensor(cfg.band_config.freqs_per_bands)\n        num_bands = len(cfg.band_config.freqs_per_bands)\n    else:\n        raise TypeError(f\"unknown band config: {cfg.band_config}\")\n    self.is_mel = is_mel\n\n    freq_rotary_embed = RotaryEmbedding(\n        seq_len=num_bands, dim_head=cfg.dim_head, dtype=cfg.rotary_embed_dtype\n    )\n\n    for _ in range(cfg.depth):\n        tran_modules = []\n        if cfg.linear_transformer_depth &gt; 0:\n            tran_modules.append(\n                transformer(depth=cfg.linear_transformer_depth, linear_attn=True)\n            )\n        tran_modules.append(\n            transformer(depth=cfg.time_transformer_depth, rotary_embed=time_rotary_embed)\n        )\n        tran_modules.append(\n            transformer(depth=cfg.freq_transformer_depth, rotary_embed=freq_rotary_embed)\n        )\n        self.layers.append(nn.ModuleList(tran_modules))\n\n    self.final_norm = (\n        rms_norm(cfg.dim, eps=cfg.rms_norm_eps) if not self.is_mel else nn.Identity()\n    )\n\n    freqs_per_bands_with_complex = tuple(\n        2 * f * self.audio_channels for f in num_freqs_per_band.tolist()\n    )\n\n    self.band_split = BandSplit(\n        dim=cfg.dim,\n        dim_inputs=freqs_per_bands_with_complex,\n        rms_norm_eps=cfg.rms_norm_eps,\n    )\n\n    self.mask_estimators = nn.ModuleList([])\n\n    def build_hyperace(config: MaskEstimatorConfig) -&gt; nn.Module:\n        if isinstance(config, HyperAceResidualV1MaskEstimatorConfig):\n            from .utils.hyperace import SegmModelHyperAceV1\n\n            return SegmModelHyperAceV1(\n                in_bands=len(freqs_per_bands_with_complex),\n                in_dim=cfg.dim,\n                out_bins=sum(freqs_per_bands_with_complex) // 4,\n                num_hyperedges=config.num_hyperedges or 16,\n                num_heads=config.num_heads,\n            )\n        if isinstance(config, HyperAceResidualV2MaskEstimatorConfig):\n            from .utils.hyperace import SegmModelHyperAceV2\n\n            return SegmModelHyperAceV2(\n                in_bands=len(freqs_per_bands_with_complex),\n                in_dim=cfg.dim,\n                out_bins=sum(freqs_per_bands_with_complex) // 4,\n                num_hyperedges=config.num_hyperedges or 32,\n                num_heads=config.num_heads,\n            )\n        raise TypeError(f\"mask estimator is not hyperace-based: {config}\")\n\n    def build_mask_estimator(config: MaskEstimatorConfig) -&gt; nn.Module:\n        if isinstance(config, BaselineMaskEstimatorConfig):\n            return MaskEstimator(\n                dim=cfg.dim,\n                dim_inputs=freqs_per_bands_with_complex,\n                depth=cfg.mask_estimator_depth,\n                mlp_expansion_factor=cfg.mlp_expansion_factor,\n            )\n\n        if isinstance(config, AxialRefinerLargeV2MaskEstimatorConfig):\n            return AxialRefinerLargeV2MaskEstimator(\n                dim=cfg.dim,\n                dim_inputs=freqs_per_bands_with_complex,\n                mlp_depth=cfg.mask_estimator_depth,\n                mlp_expansion_factor=cfg.mlp_expansion_factor,\n                axial_refiner_depth=config.axial_refiner_depth,\n                t_frames=t_frames,\n                num_bands=num_bands,\n                rotary_embed_dtype=cfg.rotary_embed_dtype,\n            )\n\n        if isinstance(\n            config,\n            HyperAceResidualV1MaskEstimatorConfig | HyperAceResidualV2MaskEstimatorConfig,\n        ):\n            return HyperAceResidualMaskEstimator(\n                dim=cfg.dim,\n                dim_inputs=freqs_per_bands_with_complex,\n                depth=cfg.mask_estimator_depth,\n                mlp_expansion_factor=cfg.mlp_expansion_factor,\n                segm=build_hyperace(config),\n            )\n\n        raise TypeError(f\"unknown mask_estimator config: {config}\")\n\n    for _ in range(len(cfg.output_stem_names)):\n        self.mask_estimators.append(build_mask_estimator(cfg.mask_estimator))\n\n    self.debug = cfg.debug\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.stereo","title":"stereo  <code>instance-attribute</code>","text":"<pre><code>stereo = stereo\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.audio_channels","title":"audio_channels  <code>instance-attribute</code>","text":"<pre><code>audio_channels = 2 if stereo else 1\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.num_stems","title":"num_stems  <code>instance-attribute</code>","text":"<pre><code>num_stems = len(output_stem_names)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.use_torch_checkpoint","title":"use_torch_checkpoint  <code>instance-attribute</code>","text":"<pre><code>use_torch_checkpoint = use_torch_checkpoint\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.skip_connection","title":"skip_connection  <code>instance-attribute</code>","text":"<pre><code>skip_connection = skip_connection\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.layers","title":"layers  <code>instance-attribute</code>","text":"<pre><code>layers = ModuleList([])\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.shared_qkv_bias","title":"shared_qkv_bias  <code>instance-attribute</code>","text":"<pre><code>shared_qkv_bias: Parameter | None = None\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.shared_out_bias","title":"shared_out_bias  <code>instance-attribute</code>","text":"<pre><code>shared_out_bias: Parameter | None = None\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.is_mel","title":"is_mel  <code>instance-attribute</code>","text":"<pre><code>is_mel = is_mel\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.final_norm","title":"final_norm  <code>instance-attribute</code>","text":"<pre><code>final_norm = (\n    rms_norm(dim, eps=rms_norm_eps)\n    if not is_mel\n    else Identity()\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.band_split","title":"band_split  <code>instance-attribute</code>","text":"<pre><code>band_split = BandSplit(\n    dim=dim,\n    dim_inputs=freqs_per_bands_with_complex,\n    rms_norm_eps=rms_norm_eps,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.mask_estimators","title":"mask_estimators  <code>instance-attribute</code>","text":"<pre><code>mask_estimators = ModuleList([])\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.debug","title":"debug  <code>instance-attribute</code>","text":"<pre><code>debug = debug\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.forward","title":"forward","text":"<pre><code>forward(stft_repr: Tensor) -&gt; Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>stft_repr</code> <code>Tensor</code> <p>input spectrogram. shape (b, f*s, t, c)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>estimated mask. shape (b, n, f*s, t, c)</p> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>def forward(self, stft_repr: Tensor) -&gt; Tensor:\n    \"\"\"\n    :param stft_repr: input spectrogram. shape (b, f*s, t, c)\n    :return: estimated mask. shape (b, n, f*s, t, c)\n    \"\"\"\n    batch, _, t_frames, _ = stft_repr.shape\n    device = stft_repr.device\n    if self.is_mel:\n        batch_arange = torch.arange(batch, device=device)[..., None]\n        x = stft_repr[batch_arange, cast(Tensor, self.freq_indices)]\n        x = rearrange(x, \"b f t c -&gt; b t (f c)\")\n    else:\n        x = rearrange(stft_repr, \"b f t c -&gt; b t (f c)\")\n\n    if self.debug and (torch.isnan(x).any() or torch.isinf(x).any()):\n        raise RuntimeError(\n            f\"nan/inf in x after rearrange: {x.isnan().sum()} nans, {x.isinf().sum()} infs\"\n        )\n\n    if self.use_torch_checkpoint:\n        x = cast(Tensor, checkpoint(self.band_split, x, use_reentrant=False))\n    else:\n        x = cast(Tensor, self.band_split(x))\n\n    if self.debug and (torch.isnan(x).any() or torch.isinf(x).any()):\n        raise RuntimeError(\n            f\"nan/inf in x after band_split: {x.isnan().sum()} nans, {x.isinf().sum()} infs\"\n        )\n\n    # axial / hierarchical attention\n\n    store: list[Tensor | None] = [None] * len(self.layers)\n    for i, transformer_block in enumerate(self.layers):\n        block = cast(ModuleList, transformer_block)\n        if len(block) == 3:\n            linear_transformer, time_transformer, freq_transformer = block\n\n            x, ft_ps = pack([x], \"b * d\")\n            if self.use_torch_checkpoint:\n                x = checkpoint(linear_transformer, x, use_reentrant=False)\n            else:\n                x = linear_transformer(x)\n            (x,) = unpack(x, ft_ps, \"b * d\")\n        else:\n            time_transformer, freq_transformer = block\n\n        if self.skip_connection:\n            for j in range(i):\n                if store[j] is not None:\n                    assert x is not None\n                    x = x + cast(Tensor, store[j])\n\n        x = rearrange(x, \"b t f d -&gt; b f t d\")\n        x, ps = pack([x], \"* t d\")\n\n        if self.use_torch_checkpoint:\n            x = checkpoint(time_transformer, x, use_reentrant=False)\n        else:\n            x = time_transformer(x)\n\n        (x,) = unpack(x, ps, \"* t d\")\n        x = rearrange(x, \"b f t d -&gt; b t f d\")\n        x, ps = pack([x], \"* f d\")\n\n        if self.use_torch_checkpoint:\n            x = checkpoint(freq_transformer, x, use_reentrant=False)\n        else:\n            x = freq_transformer(x)\n\n        (x,) = unpack(x, ps, \"* f d\")\n\n        if self.skip_connection:\n            store[i] = x\n\n    x = self.final_norm(x)\n\n    if self.use_torch_checkpoint:\n        mask = torch.stack(\n            [\n                cast(Tensor, checkpoint(fn, x, use_reentrant=False))\n                for fn in self.mask_estimators\n            ],\n            dim=1,\n        )\n    else:\n        mask = torch.stack([fn(x) for fn in self.mask_estimators], dim=1)\n    mask = rearrange(mask, \"b n t (f c) -&gt; b n f t c\", c=2)\n\n    if not self.is_mel:\n        return mask\n\n    stft_repr = rearrange(stft_repr, \"b f t c -&gt; b 1 f t c\")\n    # stft_repr may be fp16 but complex32 support is experimental so we upcast it early\n    stft_repr_complex = torch.view_as_complex(stft_repr.to(torch.float32))\n\n    masks_per_band_complex = torch.view_as_complex(mask)\n    masks_per_band_complex = masks_per_band_complex.type(stft_repr_complex.dtype)\n\n    scatter_indices = repeat(\n        cast(Tensor, self.freq_indices),\n        \"f -&gt; b n f t\",\n        b=batch,\n        n=self.num_stems,\n        t=stft_repr_complex.shape[-1],\n    )\n    stft_repr_expanded_stems = repeat(stft_repr_complex, \"b 1 ... -&gt; b n ...\", n=self.num_stems)\n\n    masks_summed = torch.zeros_like(stft_repr_expanded_stems).scatter_add_(\n        2, scatter_indices, masks_per_band_complex\n    )\n\n    denom = cast(Tensor, repeat(self.num_bands_per_freq, \"f -&gt; (f r) 1\", r=self.audio_channels))\n    masks_averaged = masks_summed / denom.clamp(min=1e-8)\n\n    return torch.view_as_real(masks_averaged).to(stft_repr.dtype)\n</code></pre>"},{"location":"api/models/#splifft.models.bs_roformer.BSRoformer.__splifft_stem_selection_plan__","title":"__splifft_stem_selection_plan__  <code>classmethod</code>","text":"<pre><code>__splifft_stem_selection_plan__(\n    model_params: BSRoformerParams,\n    output_stem_names: tuple[ModelOutputStemName, ...],\n) -&gt; StemSelectionPlan[BSRoformerParams]\n</code></pre> <p>Remap <code>mask_estimators.{i}.*</code> state-dict entries to a compact <code>[0..k)</code> index range so unrelated per-stem heads are never instantiated or loaded.</p> Source code in <code>src/splifft/models/bs_roformer.py</code> <pre><code>@classmethod\ndef __splifft_stem_selection_plan__(\n    cls,\n    model_params: BSRoformerParams,\n    output_stem_names: tuple[t.ModelOutputStemName, ...],\n) -&gt; StemSelectionPlan[BSRoformerParams]:\n    \"\"\"Remap `mask_estimators.{i}.*` state-dict entries to a compact\n    `[0..k)` index range so unrelated per-stem heads are never instantiated\n    or loaded.\n    \"\"\"\n\n    full_stem_names = tuple(model_params.output_stem_names)\n    requested_stem_names = tuple(output_stem_names)\n    if requested_stem_names == full_stem_names:\n        return StemSelectionPlan(\n            model_params=model_params,\n            output_stem_names=full_stem_names,\n        )\n\n    selected_stem_indices = tuple(full_stem_names.index(name) for name in requested_stem_names)\n    key_re = re.compile(r\"^mask_estimators\\.(\\d+)\\.(.+)$\")\n    index_remap = {src: dst for dst, src in enumerate(selected_stem_indices)}\n\n    def state_dict_transform(state_dict: dict[str, Tensor]) -&gt; dict[str, Tensor]:\n        remapped: dict[str, Tensor] = {}\n        for key, value in state_dict.items():\n            if (m := key_re.match(key)) is None:\n                remapped[key] = value\n                continue\n            if (src_idx := int(m.group(1))) not in index_remap:\n                continue\n            suffix = m.group(2)\n            remapped[f\"mask_estimators.{index_remap[src_idx]}.{suffix}\"] = value\n        return remapped\n\n    return StemSelectionPlan(\n        model_params=replace(model_params, output_stem_names=requested_stem_names),\n        output_stem_names=requested_stem_names,\n        state_dict_transform=state_dict_transform,\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch","title":"basic_pitch","text":"<p>ICASSP 2022 Basic Pitch. Raw multi-stream outputs only, no symbolic decoding.</p> <p>See: https://github.com/spotify/basic-pitch, https://arxiv.org/abs/2203.09893</p> <p>Classes:</p> Name Description <code>BasicPitchParams</code> <code>HarmonicStacking</code> <code>BasicPitch</code>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitchParams","title":"BasicPitchParams  <code>dataclass</code>","text":"<pre><code>BasicPitchParams(\n    chunk_size: ChunkSize,\n    output_stem_names: tuple[ModelOutputStemName, ...],\n    n_semitones: Gt0[int] = 88,\n    contour_bins_per_semitone: Gt0[int] = 3,\n    cqt_bins_per_semitone: Gt0[int] = 3,\n    cqt_n_bins: Gt0[int] = 372,\n    stack_harmonics: tuple[Gt0[float], ...] = (\n        0.5,\n        1.0,\n        2.0,\n        3.0,\n        4.0,\n        5.0,\n        6.0,\n        7.0,\n    ),\n)\n</code></pre> <p>               Bases: <code>ModelParamsLike</code></p> <p>Attributes:</p> Name Type Description <code>chunk_size</code> <code>ChunkSize</code> <code>output_stem_names</code> <code>tuple[ModelOutputStemName, ...]</code> <code>n_semitones</code> <code>Gt0[int]</code> <code>contour_bins_per_semitone</code> <code>Gt0[int]</code> <code>cqt_bins_per_semitone</code> <code>Gt0[int]</code> <code>cqt_n_bins</code> <code>Gt0[int]</code> <code>stack_harmonics</code> <code>tuple[Gt0[float], ...]</code> <code>input_channels</code> <code>ModelInputChannels</code> <code>input_type</code> <code>ModelInputType</code> <code>output_type</code> <code>ModelOutputType</code> <code>inference_archetype</code> <code>InferenceArchetype</code>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitchParams.chunk_size","title":"chunk_size  <code>instance-attribute</code>","text":"<pre><code>chunk_size: ChunkSize\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitchParams.output_stem_names","title":"output_stem_names  <code>instance-attribute</code>","text":"<pre><code>output_stem_names: tuple[ModelOutputStemName, ...]\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitchParams.n_semitones","title":"n_semitones  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_semitones: Gt0[int] = 88\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitchParams.contour_bins_per_semitone","title":"contour_bins_per_semitone  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>contour_bins_per_semitone: Gt0[int] = 3\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitchParams.cqt_bins_per_semitone","title":"cqt_bins_per_semitone  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cqt_bins_per_semitone: Gt0[int] = 3\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitchParams.cqt_n_bins","title":"cqt_n_bins  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cqt_n_bins: Gt0[int] = 372\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitchParams.stack_harmonics","title":"stack_harmonics  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stack_harmonics: tuple[Gt0[float], ...] = (\n    0.5,\n    1.0,\n    2.0,\n    3.0,\n    4.0,\n    5.0,\n    6.0,\n    7.0,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitchParams.input_channels","title":"input_channels  <code>property</code>","text":"<pre><code>input_channels: ModelInputChannels\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitchParams.input_type","title":"input_type  <code>property</code>","text":"<pre><code>input_type: ModelInputType\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitchParams.output_type","title":"output_type  <code>property</code>","text":"<pre><code>output_type: ModelOutputType\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitchParams.inference_archetype","title":"inference_archetype  <code>property</code>","text":"<pre><code>inference_archetype: InferenceArchetype\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.HarmonicStacking","title":"HarmonicStacking","text":"<pre><code>HarmonicStacking(\n    *,\n    bins_per_semitone: int,\n    harmonics: tuple[float, ...],\n    n_output_freqs: int,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>:param x: (B, T, F)</p> <p>Attributes:</p> Name Type Description <code>n_output_freqs</code> <code>shifts</code> Source code in <code>src/splifft/models/basic_pitch.py</code> <pre><code>def __init__(\n    self,\n    *,\n    bins_per_semitone: int,\n    harmonics: tuple[float, ...],\n    n_output_freqs: int,\n):\n    super().__init__()\n    self.n_output_freqs = n_output_freqs\n    self.shifts = [int(round(12.0 * bins_per_semitone * math.log2(h))) for h in harmonics]\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.HarmonicStacking.n_output_freqs","title":"n_output_freqs  <code>instance-attribute</code>","text":"<pre><code>n_output_freqs = n_output_freqs\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.HarmonicStacking.shifts","title":"shifts  <code>instance-attribute</code>","text":"<pre><code>shifts = [\n    (int(round(12.0 * bins_per_semitone * log2(h))))\n    for h in harmonics\n]\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.HarmonicStacking.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>(B, T, F)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>(B, H, T, F_out)</p> Source code in <code>src/splifft/models/basic_pitch.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    :param x: (B, T, F)\n    :return: (B, H, T, F_out)\n    \"\"\"\n    stacked: list[torch.Tensor] = []\n    for shift in self.shifts:\n        if shift == 0:\n            shifted = x\n        elif shift &gt; 0:\n            shifted = F.pad(x[:, :, shift:], (0, shift))\n        else:\n            shifted = F.pad(x[:, :, :shift], (-shift, 0))\n        stacked.append(shifted[:, :, : self.n_output_freqs])\n    return torch.stack(stacked, dim=1)\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitch","title":"BasicPitch","text":"<pre><code>BasicPitch(cfg: BasicPitchParams)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>cfg</code> <code>n_contour_bins</code> <code>hs</code> <code>conv_contour</code> <code>conv_note</code> <code>conv_onset_pre</code> <code>conv_onset_post</code> Source code in <code>src/splifft/models/basic_pitch.py</code> <pre><code>def __init__(self, cfg: BasicPitchParams):\n    super().__init__()\n    self.cfg = cfg\n    self.n_contour_bins = cfg.n_semitones * cfg.contour_bins_per_semitone\n\n    self.hs = HarmonicStacking(\n        bins_per_semitone=cfg.cqt_bins_per_semitone,\n        harmonics=cfg.stack_harmonics,\n        n_output_freqs=self.n_contour_bins,\n    )\n\n    num_in_channels = len(cfg.stack_harmonics)\n    self.conv_contour = nn.Sequential(\n        nn.Conv2d(num_in_channels, 8, kernel_size=(3, 39), padding=\"same\"),\n        nn.BatchNorm2d(8, eps=0.001),\n        nn.ReLU(),\n        nn.Conv2d(8, 1, kernel_size=5, padding=\"same\"),\n        nn.Sigmoid(),\n    )\n    self.conv_note = nn.Sequential(\n        nn.Conv2d(1, 32, kernel_size=7, stride=(1, 3)),\n        nn.ReLU(),\n        nn.Conv2d(32, 1, kernel_size=(7, 3), padding=\"same\"),\n        nn.Sigmoid(),\n    )\n    self.conv_onset_pre = nn.Sequential(\n        nn.Conv2d(num_in_channels, 32, kernel_size=5, stride=(1, 3)),\n        nn.BatchNorm2d(32, eps=0.001),\n        nn.ReLU(),\n    )\n    self.conv_onset_post = nn.Sequential(\n        nn.Conv2d(33, 1, kernel_size=3, stride=1, padding=\"same\"),\n        nn.Sigmoid(),\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitch.cfg","title":"cfg  <code>instance-attribute</code>","text":"<pre><code>cfg = cfg\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitch.n_contour_bins","title":"n_contour_bins  <code>instance-attribute</code>","text":"<pre><code>n_contour_bins = n_semitones * contour_bins_per_semitone\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitch.hs","title":"hs  <code>instance-attribute</code>","text":"<pre><code>hs = HarmonicStacking(\n    bins_per_semitone=cqt_bins_per_semitone,\n    harmonics=stack_harmonics,\n    n_output_freqs=n_contour_bins,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitch.conv_contour","title":"conv_contour  <code>instance-attribute</code>","text":"<pre><code>conv_contour = Sequential(\n    Conv2d(\n        num_in_channels,\n        8,\n        kernel_size=(3, 39),\n        padding=\"same\",\n    ),\n    BatchNorm2d(8, eps=0.001),\n    ReLU(),\n    Conv2d(8, 1, kernel_size=5, padding=\"same\"),\n    Sigmoid(),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitch.conv_note","title":"conv_note  <code>instance-attribute</code>","text":"<pre><code>conv_note = Sequential(\n    Conv2d(1, 32, kernel_size=7, stride=(1, 3)),\n    ReLU(),\n    Conv2d(32, 1, kernel_size=(7, 3), padding=\"same\"),\n    Sigmoid(),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitch.conv_onset_pre","title":"conv_onset_pre  <code>instance-attribute</code>","text":"<pre><code>conv_onset_pre = Sequential(\n    Conv2d(\n        num_in_channels, 32, kernel_size=5, stride=(1, 3)\n    ),\n    BatchNorm2d(32, eps=0.001),\n    ReLU(),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitch.conv_onset_post","title":"conv_onset_post  <code>instance-attribute</code>","text":"<pre><code>conv_onset_post = Sequential(\n    Conv2d(33, 1, kernel_size=3, stride=1, padding=\"same\"),\n    Sigmoid(),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.basic_pitch.BasicPitch.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; dict[str, Tensor]\n</code></pre> Source code in <code>src/splifft/models/basic_pitch.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; dict[str, torch.Tensor]:\n    if x.ndim != 3:\n        raise ValueError(f\"expected `(B,T,F)` input, got {tuple(x.shape)}\")\n    if x.shape[-1] != self.cfg.cqt_n_bins:\n        raise ValueError(f\"expected feature dim {self.cfg.cqt_n_bins}, got {x.shape[-1]}\")\n\n    cqt = self.hs(x)\n\n    contour = self.conv_contour(cqt)\n\n    contour_for_note = F.pad(contour, (2, 2, 3, 3))\n    note = self.conv_note(contour_for_note)\n\n    cqt_for_onset = F.pad(cqt, (1, 1, 2, 2))\n    onset_pre = self.conv_onset_pre(cqt_for_onset)\n    onset_in = torch.cat((note, onset_pre), dim=1)\n    onset = self.conv_onset_post(onset_in)\n\n    contour_out = contour.squeeze(1)\n    note_out = note.squeeze(1)\n    onset_out = onset.squeeze(1)\n\n    return {\n        \"onset\": onset_out,\n        \"note\": note_out,\n        \"contour\": contour_out,\n    }\n</code></pre>"},{"location":"api/models/#splifft.models.utils","title":"utils","text":"<p>Modules:</p> Name Description <code>attend</code> <code>attend_sage</code> <code>hyperace</code> <p>HyperACE segmentation backbones for BS-RoFormer mask heads.</p> <code>stft</code> <code>tfc_tdf</code> <p>Time-Frequency Convolutions and Time-Distributed Fully-connected (TFC-TDF)</p> <p>Functions:</p> Name Description <code>parse_version</code> <code>log_once</code>"},{"location":"api/models/#splifft.models.utils.parse_version","title":"parse_version","text":"<pre><code>parse_version(v_str: str) -&gt; tuple[int, ...]\n</code></pre> Source code in <code>src/splifft/models/utils/__init__.py</code> <pre><code>def parse_version(v_str: str) -&gt; tuple[int, ...]:\n    # e.g \"2.1.0+cu118\" -&gt; (2, 1, 0)\n    return tuple(map(int, v_str.split(\"+\")[0].split(\".\")))\n</code></pre>"},{"location":"api/models/#splifft.models.utils.log_once","title":"log_once  <code>cached</code>","text":"<pre><code>log_once(\n    logger: Logger, msg: object, *, level: int = DEBUG\n) -&gt; None\n</code></pre> Source code in <code>src/splifft/models/utils/__init__.py</code> <pre><code>@lru_cache(10)\ndef log_once(logger: Logger, msg: object, *, level: int = logging.DEBUG) -&gt; None:\n    logger.log(level, msg)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend","title":"attend","text":"<p>Classes:</p> Name Description <code>Attend</code> <p>Attributes:</p> Name Type Description <code>logger</code>"},{"location":"api/models/#splifft.models.utils.attend.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend.Attend","title":"Attend","text":"<pre><code>Attend(\n    dropout: float = 0.0,\n    flash: bool = False,\n    scale: float | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>flash_attn</code> <code>forward</code> <p>einstein notation</p> <p>Attributes:</p> Name Type Description <code>scale</code> <code>dropout</code> <code>attn_dropout</code> <code>flash</code> <code>cpu_backends</code> <code>cuda_backends</code> <code>list[_SDPBackend] | None</code> Source code in <code>src/splifft/models/utils/attend.py</code> <pre><code>def __init__(\n    self, dropout: float = 0.0, flash: bool = False, scale: float | None = None\n) -&gt; None:\n    super().__init__()\n    self.scale = scale\n    self.dropout = dropout\n    self.attn_dropout = nn.Dropout(dropout)\n\n    self.flash = flash\n    assert not (flash and parse_version(torch.__version__) &lt; (2, 0, 0)), (\n        \"expected pytorch &gt;= 2.0.0 to use flash attention\"\n    )\n\n    # determine efficient attention configs for cuda and cpu\n    self.cpu_backends = [\n        SDPBackend.FLASH_ATTENTION,\n        SDPBackend.EFFICIENT_ATTENTION,\n        SDPBackend.MATH,\n    ]\n    self.cuda_backends: list[_SDPBackend] | None = None\n\n    if not torch.cuda.is_available() or not flash:\n        return\n\n    device_properties = torch.cuda.get_device_properties(torch.device(\"cuda\"))\n    device_version = parse_version(f\"{device_properties.major}.{device_properties.minor}\")\n\n    if device_version &gt;= (8, 0):\n        if os.name == \"nt\":\n            cuda_backends = [SDPBackend.EFFICIENT_ATTENTION, SDPBackend.MATH]\n            log_once(logger, f\"windows detected, using {cuda_backends=}\")\n        else:\n            cuda_backends = [SDPBackend.FLASH_ATTENTION, SDPBackend.MATH]\n            log_once(logger, f\"gpu compute capability &gt;= 8.0, using {cuda_backends=}\")\n    else:\n        cuda_backends = [SDPBackend.EFFICIENT_ATTENTION, SDPBackend.MATH]\n        log_once(logger, f\"gpu compute capability &lt; 8.0, using {cuda_backends=}\")\n\n    self.cuda_backends = cuda_backends\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend.Attend.scale","title":"scale  <code>instance-attribute</code>","text":"<pre><code>scale = scale\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend.Attend.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout = dropout\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend.Attend.attn_dropout","title":"attn_dropout  <code>instance-attribute</code>","text":"<pre><code>attn_dropout = Dropout(dropout)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend.Attend.flash","title":"flash  <code>instance-attribute</code>","text":"<pre><code>flash = flash\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend.Attend.cpu_backends","title":"cpu_backends  <code>instance-attribute</code>","text":"<pre><code>cpu_backends = [FLASH_ATTENTION, EFFICIENT_ATTENTION, MATH]\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend.Attend.cuda_backends","title":"cuda_backends  <code>instance-attribute</code>","text":"<pre><code>cuda_backends: list[_SDPBackend] | None = cuda_backends\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend.Attend.flash_attn","title":"flash_attn","text":"<pre><code>flash_attn(q: Tensor, k: Tensor, v: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/utils/attend.py</code> <pre><code>def flash_attn(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tensor:\n    _, _heads, _q_len, _, _k_len, is_cuda, _device = (\n        *q.shape,\n        k.shape[-2],\n        q.is_cuda,\n        q.device,\n    )  # type: ignore\n\n    if self.scale is not None:\n        default_scale = q.shape[-1] ** -0.5\n        q = q * (self.scale / default_scale)\n\n    backends = self.cuda_backends if is_cuda else self.cpu_backends\n    # pytorch 2.0 flash attn: q, k, v, mask, dropout, softmax_scale\n    with sdpa_kernel(backends=backends):  # type: ignore\n        out = F.scaled_dot_product_attention(\n            q, k, v, dropout_p=self.dropout if self.training else 0.0\n        )\n\n    return out\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend.Attend.forward","title":"forward","text":"<pre><code>forward(q: Tensor, k: Tensor, v: Tensor) -&gt; Tensor\n</code></pre> <p>einstein notation</p> <ul> <li>b: batch</li> <li>h: heads</li> <li>n, i, j: sequence length (base sequence length, source, target)</li> <li>d: feature dimension</li> </ul> Source code in <code>src/splifft/models/utils/attend.py</code> <pre><code>def forward(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tensor:\n    \"\"\"\n    einstein notation\n\n    - b: batch\n    - h: heads\n    - n, i, j: sequence length (base sequence length, source, target)\n    - d: feature dimension\n    \"\"\"\n    _q_len, _k_len, _device = q.shape[-2], k.shape[-2], q.device\n\n    scale = self.scale or q.shape[-1] ** -0.5\n\n    if self.flash:\n        return self.flash_attn(q, k, v)\n\n    # similarity\n    sim = einsum(\"b h i d, b h j d -&gt; b h i j\", q, k) * scale\n\n    # attention\n    attn = sim.softmax(dim=-1)\n    attn = self.attn_dropout(attn)\n\n    # aggregate values\n    out = einsum(\"b h i j, b h j d -&gt; b h i d\", attn, v)\n\n    return out\n</code></pre>"},{"location":"api/models/#splifft.models.utils.tfc_tdf","title":"tfc_tdf","text":"<p>Time-Frequency Convolutions and Time-Distributed Fully-connected (TFC-TDF)</p> <p>See: https://arxiv.org/pdf/2306.09382</p> <p>Classes:</p> Name Description <code>TfcTdfBlock</code> <code>TfcTdf</code> <p>Functions:</p> Name Description <code>instance_norm_factory</code> <code>silu_factory</code> <p>Attributes:</p> Name Type Description <code>NormFactory</code> <code>ActFactory</code>"},{"location":"api/models/#splifft.models.utils.tfc_tdf.NormFactory","title":"NormFactory  <code>module-attribute</code>","text":"<pre><code>NormFactory = Callable[[int], Module]\n</code></pre>"},{"location":"api/models/#splifft.models.utils.tfc_tdf.ActFactory","title":"ActFactory  <code>module-attribute</code>","text":"<pre><code>ActFactory = Callable[[], Module]\n</code></pre>"},{"location":"api/models/#splifft.models.utils.tfc_tdf.instance_norm_factory","title":"instance_norm_factory","text":"<pre><code>instance_norm_factory(channels: int) -&gt; Module\n</code></pre> Source code in <code>src/splifft/models/utils/tfc_tdf.py</code> <pre><code>def instance_norm_factory(channels: int) -&gt; nn.Module:\n    return nn.InstanceNorm2d(channels, affine=True, eps=1e-8)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.tfc_tdf.silu_factory","title":"silu_factory","text":"<pre><code>silu_factory() -&gt; Module\n</code></pre> Source code in <code>src/splifft/models/utils/tfc_tdf.py</code> <pre><code>def silu_factory() -&gt; nn.Module:\n    return nn.SiLU()\n</code></pre>"},{"location":"api/models/#splifft.models.utils.tfc_tdf.TfcTdfBlock","title":"TfcTdfBlock","text":"<pre><code>TfcTdfBlock(\n    in_channels: int,\n    out_channels: int,\n    f_bins: int,\n    bottleneck_factor: int,\n    *,\n    norm_factory: NormFactory,\n    act_factory: ActFactory,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>tfc1</code> <code>tdf</code> <code>tfc2</code> <code>shortcut</code> Source code in <code>src/splifft/models/utils/tfc_tdf.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    f_bins: int,\n    bottleneck_factor: int,\n    *,\n    norm_factory: NormFactory,\n    act_factory: ActFactory,\n):\n    super().__init__()\n\n    self.tfc1 = nn.Sequential(\n        norm_factory(in_channels),\n        act_factory(),\n        nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n    )\n    self.tdf = nn.Sequential(\n        norm_factory(out_channels),\n        act_factory(),\n        nn.Linear(f_bins, f_bins // bottleneck_factor, bias=False),\n        norm_factory(out_channels),\n        act_factory(),\n        nn.Linear(f_bins // bottleneck_factor, f_bins, bias=False),\n    )\n    self.tfc2 = nn.Sequential(\n        norm_factory(out_channels),\n        act_factory(),\n        nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n    )\n    self.shortcut = nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.tfc_tdf.TfcTdfBlock.tfc1","title":"tfc1  <code>instance-attribute</code>","text":"<pre><code>tfc1 = Sequential(\n    norm_factory(in_channels),\n    act_factory(),\n    Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.tfc_tdf.TfcTdfBlock.tdf","title":"tdf  <code>instance-attribute</code>","text":"<pre><code>tdf = Sequential(\n    norm_factory(out_channels),\n    act_factory(),\n    Linear(f_bins, f_bins // bottleneck_factor, bias=False),\n    norm_factory(out_channels),\n    act_factory(),\n    Linear(f_bins // bottleneck_factor, f_bins, bias=False),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.tfc_tdf.TfcTdfBlock.tfc2","title":"tfc2  <code>instance-attribute</code>","text":"<pre><code>tfc2 = Sequential(\n    norm_factory(out_channels),\n    act_factory(),\n    Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.tfc_tdf.TfcTdfBlock.shortcut","title":"shortcut  <code>instance-attribute</code>","text":"<pre><code>shortcut = Conv2d(\n    in_channels, out_channels, 1, 1, 0, bias=False\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.tfc_tdf.TfcTdfBlock.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/utils/tfc_tdf.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    s = self.shortcut(x)\n    x = self.tfc1(x)\n    x = x + self.tdf(x)\n    x = self.tfc2(x)\n    x = x + s\n    return x\n</code></pre>"},{"location":"api/models/#splifft.models.utils.tfc_tdf.TfcTdf","title":"TfcTdf","text":"<pre><code>TfcTdf(\n    in_channels: int,\n    out_channels: int,\n    num_blocks: int,\n    f_bins: int,\n    bottleneck_factor: int,\n    *,\n    norm_factory: NormFactory,\n    act_factory: ActFactory,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>blocks</code> Source code in <code>src/splifft/models/utils/tfc_tdf.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    num_blocks: int,\n    f_bins: int,\n    bottleneck_factor: int,\n    *,\n    norm_factory: NormFactory,\n    act_factory: ActFactory,\n):\n    super().__init__()\n\n    self.blocks = nn.ModuleList(\n        [\n            TfcTdfBlock(\n                in_channels=in_channels if i == 0 else out_channels,\n                out_channels=out_channels,\n                f_bins=f_bins,\n                bottleneck_factor=bottleneck_factor,\n                norm_factory=norm_factory,\n                act_factory=act_factory,\n            )\n            for i in range(num_blocks)\n        ]\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.utils.tfc_tdf.TfcTdf.blocks","title":"blocks  <code>instance-attribute</code>","text":"<pre><code>blocks = ModuleList(\n    [\n        (\n            TfcTdfBlock(\n                in_channels=in_channels\n                if i == 0\n                else out_channels,\n                out_channels=out_channels,\n                f_bins=f_bins,\n                bottleneck_factor=bottleneck_factor,\n                norm_factory=norm_factory,\n                act_factory=act_factory,\n            )\n        )\n        for i in (range(num_blocks))\n    ]\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.tfc_tdf.TfcTdf.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/utils/tfc_tdf.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    for block in self.blocks:\n        x = block(x)\n    return x\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace","title":"hyperace","text":"<p>HyperACE segmentation backbones for BS-RoFormer mask heads.</p> <p>These modules are compatibility shims for unwa variants trained in msst (<code>hyperace_v1</code>, <code>hyperace_v2</code>, and <code>large_inst_v2</code> head behavior). They are kept separate from the core transformer stack because they are used only by a small subset of checkpoints.</p> <p>See: https://huggingface.co/pcunwa/BS-Roformer-HyperACE and https://arxiv.org/abs/2506.17733</p> <p>Classes:</p> Name Description <code>Conv</code> <code>DSConv</code> <code>DS_Bottleneck</code> <code>DS_C3k</code> <code>DS_C3k2</code> <code>AdaptiveHyperedgeGeneration</code> <code>HypergraphConvolution</code> <code>AdaptiveHypergraphComputation</code> <code>C3AH</code> <code>HyperACE</code> <code>GatedFusion</code> <code>BackboneHyperAceV1</code> <code>BackboneHyperAceV2</code> <code>DecoderHyperAce</code> <code>FreqPixelShuffleV1</code> <code>ProgressiveUpsampleHeadV1</code> <code>FreqPixelShuffleV2</code> <code>ProgressiveUpsampleHeadV2</code> <code>SegmModelHyperAceV1</code> <code>SegmModelHyperAceV2</code> <p>Functions:</p> Name Description <code>autopad</code> <code>build_hyperace_tfc_tdf</code>"},{"location":"api/models/#splifft.models.utils.hyperace.autopad","title":"autopad","text":"<pre><code>autopad(\n    k: int | tuple[int, int],\n    p: int | tuple[int, int] | None = None,\n) -&gt; int | tuple[int, int]\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def autopad(\n    k: int | tuple[int, int],\n    p: int | tuple[int, int] | None = None,\n) -&gt; int | tuple[int, int]:\n    if p is None:\n        p = k // 2 if isinstance(k, int) else (k[0] // 2, k[1] // 2)\n    return p\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.build_hyperace_tfc_tdf","title":"build_hyperace_tfc_tdf","text":"<pre><code>build_hyperace_tfc_tdf(\n    in_c: int, c: int, l: int, f: int, bn: int = 4\n) -&gt; TfcTdf\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def build_hyperace_tfc_tdf(\n    in_c: int,\n    c: int,\n    l: int,\n    f: int,\n    bn: int = 4,\n) -&gt; TfcTdf:\n    return TfcTdf(\n        in_channels=in_c,\n        out_channels=c,\n        num_blocks=l,\n        f_bins=f,\n        bottleneck_factor=bn,\n        norm_factory=instance_norm_factory,\n        act_factory=silu_factory,\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.Conv","title":"Conv","text":"<pre><code>Conv(\n    c1: int,\n    c2: int,\n    k: int | tuple[int, int] = 1,\n    s: int | tuple[int, int] = 1,\n    p: int | tuple[int, int] | None = None,\n    g: int = 1,\n    act: bool = True,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>conv</code> <code>bn</code> <code>act</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(\n    self,\n    c1: int,\n    c2: int,\n    k: int | tuple[int, int] = 1,\n    s: int | tuple[int, int] = 1,\n    p: int | tuple[int, int] | None = None,\n    g: int = 1,\n    act: bool = True,\n):\n    super().__init__()\n    self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n    self.bn = nn.InstanceNorm2d(c2, affine=True, eps=1e-8)\n    self.act = nn.SiLU() if act else nn.Identity()\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.Conv.conv","title":"conv  <code>instance-attribute</code>","text":"<pre><code>conv = Conv2d(\n    c1, c2, k, s, autopad(k, p), groups=g, bias=False\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.Conv.bn","title":"bn  <code>instance-attribute</code>","text":"<pre><code>bn = InstanceNorm2d(c2, affine=True, eps=1e-08)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.Conv.act","title":"act  <code>instance-attribute</code>","text":"<pre><code>act = SiLU() if act else Identity()\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.Conv.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Any\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Any:\n    return self.act(self.bn(self.conv(x)))\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DSConv","title":"DSConv","text":"<pre><code>DSConv(\n    c1: int,\n    c2: int,\n    k: int | tuple[int, int] = 3,\n    s: int | tuple[int, int] = 1,\n    p: int | tuple[int, int] | None = None,\n    act: bool = True,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>dwconv</code> <code>pwconv</code> <code>bn</code> <code>act</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(\n    self,\n    c1: int,\n    c2: int,\n    k: int | tuple[int, int] = 3,\n    s: int | tuple[int, int] = 1,\n    p: int | tuple[int, int] | None = None,\n    act: bool = True,\n):\n    super().__init__()\n    self.dwconv = nn.Conv2d(c1, c1, k, s, autopad(k, p), groups=c1, bias=False)\n    self.pwconv = nn.Conv2d(c1, c2, 1, 1, 0, bias=False)\n    self.bn = nn.InstanceNorm2d(c2, affine=True, eps=1e-8)\n    self.act = nn.SiLU() if act else nn.Identity()\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DSConv.dwconv","title":"dwconv  <code>instance-attribute</code>","text":"<pre><code>dwconv = Conv2d(\n    c1, c1, k, s, autopad(k, p), groups=c1, bias=False\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DSConv.pwconv","title":"pwconv  <code>instance-attribute</code>","text":"<pre><code>pwconv = Conv2d(c1, c2, 1, 1, 0, bias=False)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DSConv.bn","title":"bn  <code>instance-attribute</code>","text":"<pre><code>bn = InstanceNorm2d(c2, affine=True, eps=1e-08)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DSConv.act","title":"act  <code>instance-attribute</code>","text":"<pre><code>act = SiLU() if act else Identity()\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DSConv.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Any\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Any:\n    return self.act(self.bn(self.pwconv(self.dwconv(x))))\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DS_Bottleneck","title":"DS_Bottleneck","text":"<pre><code>DS_Bottleneck(\n    c1: int, c2: int, k: int = 3, shortcut: bool = True\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>dsconv1</code> <code>dsconv2</code> <code>shortcut</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(self, c1: int, c2: int, k: int = 3, shortcut: bool = True):\n    super().__init__()\n    c_ = c1\n    self.dsconv1 = DSConv(c1, c_, k=3, s=1)\n    self.dsconv2 = DSConv(c_, c2, k=k, s=1)\n    self.shortcut = shortcut and c1 == c2\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DS_Bottleneck.dsconv1","title":"dsconv1  <code>instance-attribute</code>","text":"<pre><code>dsconv1 = DSConv(c1, c_, k=3, s=1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DS_Bottleneck.dsconv2","title":"dsconv2  <code>instance-attribute</code>","text":"<pre><code>dsconv2 = DSConv(c_, c2, k=k, s=1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DS_Bottleneck.shortcut","title":"shortcut  <code>instance-attribute</code>","text":"<pre><code>shortcut = shortcut and c1 == c2\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DS_Bottleneck.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Any\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Any:\n    if self.shortcut:\n        return x + self.dsconv2(self.dsconv1(x))\n    return self.dsconv2(self.dsconv1(x))\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DS_C3k","title":"DS_C3k","text":"<pre><code>DS_C3k(\n    c1: int, c2: int, n: int = 1, k: int = 3, e: float = 0.5\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>cv1</code> <code>cv2</code> <code>cv3</code> <code>m</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(self, c1: int, c2: int, n: int = 1, k: int = 3, e: float = 0.5):\n    super().__init__()\n    c_ = int(c2 * e)\n    self.cv1 = Conv(c1, c_, 1, 1)\n    self.cv2 = Conv(c1, c_, 1, 1)\n    self.cv3 = Conv(2 * c_, c2, 1, 1)\n    self.m = nn.Sequential(*[DS_Bottleneck(c_, c_, k=k, shortcut=True) for _ in range(n)])\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DS_C3k.cv1","title":"cv1  <code>instance-attribute</code>","text":"<pre><code>cv1 = Conv(c1, c_, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DS_C3k.cv2","title":"cv2  <code>instance-attribute</code>","text":"<pre><code>cv2 = Conv(c1, c_, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DS_C3k.cv3","title":"cv3  <code>instance-attribute</code>","text":"<pre><code>cv3 = Conv(2 * c_, c2, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DS_C3k.m","title":"m  <code>instance-attribute</code>","text":"<pre><code>m = Sequential(\n    *[\n        (DS_Bottleneck(c_, c_, k=k, shortcut=True))\n        for _ in (range(n))\n    ]\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DS_C3k.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Any\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Any:\n    return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DS_C3k2","title":"DS_C3k2","text":"<pre><code>DS_C3k2(\n    c1: int, c2: int, n: int = 1, k: int = 3, e: float = 0.5\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>cv1</code> <code>m</code> <code>cv2</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(self, c1: int, c2: int, n: int = 1, k: int = 3, e: float = 0.5):\n    super().__init__()\n    c_ = int(c2 * e)\n    self.cv1 = Conv(c1, c_, 1, 1)\n    self.m = DS_C3k(c_, c_, n=n, k=k, e=1.0)\n    self.cv2 = Conv(c_, c2, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DS_C3k2.cv1","title":"cv1  <code>instance-attribute</code>","text":"<pre><code>cv1 = Conv(c1, c_, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DS_C3k2.m","title":"m  <code>instance-attribute</code>","text":"<pre><code>m = DS_C3k(c_, c_, n=n, k=k, e=1.0)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DS_C3k2.cv2","title":"cv2  <code>instance-attribute</code>","text":"<pre><code>cv2 = Conv(c_, c2, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DS_C3k2.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Any\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Any:\n    x_ = self.cv1(x)\n    x_ = self.m(x_)\n    return self.cv2(x_)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.AdaptiveHyperedgeGeneration","title":"AdaptiveHyperedgeGeneration","text":"<pre><code>AdaptiveHyperedgeGeneration(\n    in_channels: int,\n    num_hyperedges: int,\n    num_heads: int = 8,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>num_hyperedges</code> <code>num_heads</code> <code>head_dim</code> <code>global_proto</code> <code>context_mapper</code> <code>query_proj</code> <code>scale</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(self, in_channels: int, num_hyperedges: int, num_heads: int = 8):\n    super().__init__()\n    self.num_hyperedges = num_hyperedges\n    self.num_heads = num_heads\n    self.head_dim = in_channels // num_heads\n\n    self.global_proto = nn.Parameter(torch.randn(num_hyperedges, in_channels))\n\n    self.context_mapper = nn.Linear(2 * in_channels, num_hyperedges * in_channels, bias=False)\n\n    self.query_proj = nn.Linear(in_channels, in_channels, bias=False)\n\n    self.scale = self.head_dim**-0.5\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.AdaptiveHyperedgeGeneration.num_hyperedges","title":"num_hyperedges  <code>instance-attribute</code>","text":"<pre><code>num_hyperedges = num_hyperedges\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.AdaptiveHyperedgeGeneration.num_heads","title":"num_heads  <code>instance-attribute</code>","text":"<pre><code>num_heads = num_heads\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.AdaptiveHyperedgeGeneration.head_dim","title":"head_dim  <code>instance-attribute</code>","text":"<pre><code>head_dim = in_channels // num_heads\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.AdaptiveHyperedgeGeneration.global_proto","title":"global_proto  <code>instance-attribute</code>","text":"<pre><code>global_proto = Parameter(randn(num_hyperedges, in_channels))\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.AdaptiveHyperedgeGeneration.context_mapper","title":"context_mapper  <code>instance-attribute</code>","text":"<pre><code>context_mapper = Linear(\n    2 * in_channels,\n    num_hyperedges * in_channels,\n    bias=False,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.AdaptiveHyperedgeGeneration.query_proj","title":"query_proj  <code>instance-attribute</code>","text":"<pre><code>query_proj = Linear(in_channels, in_channels, bias=False)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.AdaptiveHyperedgeGeneration.scale","title":"scale  <code>instance-attribute</code>","text":"<pre><code>scale = head_dim ** -0.5\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.AdaptiveHyperedgeGeneration.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Tensor\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Tensor:\n    b, n, c = x.shape\n\n    f_avg = F.adaptive_avg_pool1d(x.permute(0, 2, 1), 1).squeeze(-1)\n    f_max = F.adaptive_max_pool1d(x.permute(0, 2, 1), 1).squeeze(-1)\n    f_ctx = torch.cat((f_avg, f_max), dim=1)\n\n    delta_p = self.context_mapper(f_ctx).view(b, self.num_hyperedges, c)\n    p = self.global_proto.unsqueeze(0) + delta_p\n\n    z = self.query_proj(x)\n\n    z = z.view(b, n, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n\n    p = p.view(b, self.num_hyperedges, self.num_heads, self.head_dim).permute(0, 2, 3, 1)\n\n    sim = (z @ p) * self.scale\n\n    s_bar = sim.mean(dim=1)\n\n    a = F.softmax(s_bar.permute(0, 2, 1), dim=-1)\n\n    return a\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.HypergraphConvolution","title":"HypergraphConvolution","text":"<pre><code>HypergraphConvolution(in_channels: int, out_channels: int)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>W_e</code> <code>W_v</code> <code>act</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(self, in_channels: int, out_channels: int):\n    super().__init__()\n    self.W_e = nn.Linear(in_channels, in_channels, bias=False)\n    self.W_v = nn.Linear(in_channels, out_channels, bias=False)\n    self.act = nn.SiLU()\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.HypergraphConvolution.W_e","title":"W_e  <code>instance-attribute</code>","text":"<pre><code>W_e = Linear(in_channels, in_channels, bias=False)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.HypergraphConvolution.W_v","title":"W_v  <code>instance-attribute</code>","text":"<pre><code>W_v = Linear(in_channels, out_channels, bias=False)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.HypergraphConvolution.act","title":"act  <code>instance-attribute</code>","text":"<pre><code>act = SiLU()\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.HypergraphConvolution.forward","title":"forward","text":"<pre><code>forward(x: Tensor, a: Tensor) -&gt; Any\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, x: Tensor, a: Tensor) -&gt; Any:\n    f_m = torch.bmm(a, x)\n    f_m = self.act(self.W_e(f_m))\n\n    x_out = torch.bmm(a.transpose(1, 2), f_m)\n    x_out = self.act(self.W_v(x_out))\n\n    return x + x_out\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.AdaptiveHypergraphComputation","title":"AdaptiveHypergraphComputation","text":"<pre><code>AdaptiveHypergraphComputation(\n    in_channels: int,\n    out_channels: int,\n    num_hyperedges: int = 8,\n    num_heads: int = 8,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>adaptive_hyperedge_gen</code> <code>hypergraph_conv</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    num_hyperedges: int = 8,\n    num_heads: int = 8,\n):\n    super().__init__()\n    self.adaptive_hyperedge_gen = AdaptiveHyperedgeGeneration(\n        in_channels, num_hyperedges, num_heads\n    )\n    self.hypergraph_conv = HypergraphConvolution(in_channels, out_channels)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.AdaptiveHypergraphComputation.adaptive_hyperedge_gen","title":"adaptive_hyperedge_gen  <code>instance-attribute</code>","text":"<pre><code>adaptive_hyperedge_gen = AdaptiveHyperedgeGeneration(\n    in_channels, num_hyperedges, num_heads\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.AdaptiveHypergraphComputation.hypergraph_conv","title":"hypergraph_conv  <code>instance-attribute</code>","text":"<pre><code>hypergraph_conv = HypergraphConvolution(\n    in_channels, out_channels\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.AdaptiveHypergraphComputation.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Any\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Any:\n    b, _, h, w = x.shape\n    x_flat = x.flatten(2).permute(0, 2, 1)\n\n    a = self.adaptive_hyperedge_gen(x_flat)\n\n    x_out_flat = self.hypergraph_conv(x_flat, a)\n\n    x_out = x_out_flat.permute(0, 2, 1).view(b, -1, h, w)\n    return x_out\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.C3AH","title":"C3AH","text":"<pre><code>C3AH(\n    c1: int,\n    c2: int,\n    num_hyperedges: int = 8,\n    num_heads: int = 8,\n    e: float = 0.5,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>cv1</code> <code>cv2</code> <code>ahc</code> <code>cv3</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(\n    self,\n    c1: int,\n    c2: int,\n    num_hyperedges: int = 8,\n    num_heads: int = 8,\n    e: float = 0.5,\n):\n    super().__init__()\n    c_ = int(c1 * e)\n    self.cv1 = Conv(c1, c_, 1, 1)\n    self.cv2 = Conv(c1, c_, 1, 1)\n    self.ahc = AdaptiveHypergraphComputation(c_, c_, num_hyperedges, num_heads)\n    self.cv3 = Conv(2 * c_, c2, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.C3AH.cv1","title":"cv1  <code>instance-attribute</code>","text":"<pre><code>cv1 = Conv(c1, c_, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.C3AH.cv2","title":"cv2  <code>instance-attribute</code>","text":"<pre><code>cv2 = Conv(c1, c_, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.C3AH.ahc","title":"ahc  <code>instance-attribute</code>","text":"<pre><code>ahc = AdaptiveHypergraphComputation(\n    c_, c_, num_hyperedges, num_heads\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.C3AH.cv3","title":"cv3  <code>instance-attribute</code>","text":"<pre><code>cv3 = Conv(2 * c_, c2, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.C3AH.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Any\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Any:\n    x_lateral = self.cv1(x)\n    x_ahc = self.ahc(self.cv2(x))\n    return self.cv3(torch.cat((x_ahc, x_lateral), dim=1))\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.HyperACE","title":"HyperACE","text":"<pre><code>HyperACE(\n    in_channels: list[int],\n    out_channels: int,\n    num_hyperedges: int = 8,\n    num_heads: int = 8,\n    k: int = 2,\n    l: int = 1,\n    c_h: float = 0.5,\n    c_l: float = 0.25,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>fuse_conv</code> <code>c_h</code> <code>c_l</code> <code>c_s</code> <code>high_order_branch</code> <code>high_order_fuse</code> <code>low_order_branch</code> <code>final_fuse</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(\n    self,\n    in_channels: list[int],\n    out_channels: int,\n    num_hyperedges: int = 8,\n    num_heads: int = 8,\n    k: int = 2,\n    l: int = 1,\n    c_h: float = 0.5,\n    c_l: float = 0.25,\n):\n    super().__init__()\n\n    c2, c3, c4, c5 = in_channels\n    c_mid = c4\n\n    self.fuse_conv = Conv(c2 + c3 + c4 + c5, c_mid, 1, 1)\n\n    self.c_h = int(c_mid * c_h)\n    self.c_l = int(c_mid * c_l)\n    self.c_s = c_mid - self.c_h - self.c_l\n    assert self.c_s &gt; 0, \"Channel split error\"\n\n    self.high_order_branch = nn.ModuleList(\n        [C3AH(self.c_h, self.c_h, num_hyperedges, num_heads, e=1.0) for _ in range(k)]\n    )\n    self.high_order_fuse = Conv(self.c_h * k, self.c_h, 1, 1)\n\n    self.low_order_branch = nn.Sequential(\n        *[DS_C3k(self.c_l, self.c_l, n=1, k=3, e=1.0) for _ in range(l)]\n    )\n\n    self.final_fuse = Conv(self.c_h + self.c_l + self.c_s, out_channels, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.HyperACE.fuse_conv","title":"fuse_conv  <code>instance-attribute</code>","text":"<pre><code>fuse_conv = Conv(c2 + c3 + c4 + c5, c_mid, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.HyperACE.c_h","title":"c_h  <code>instance-attribute</code>","text":"<pre><code>c_h = int(c_mid * c_h)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.HyperACE.c_l","title":"c_l  <code>instance-attribute</code>","text":"<pre><code>c_l = int(c_mid * c_l)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.HyperACE.c_s","title":"c_s  <code>instance-attribute</code>","text":"<pre><code>c_s = c_mid - c_h - c_l\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.HyperACE.high_order_branch","title":"high_order_branch  <code>instance-attribute</code>","text":"<pre><code>high_order_branch = ModuleList(\n    [\n        (C3AH(c_h, c_h, num_hyperedges, num_heads, e=1.0))\n        for _ in (range(k))\n    ]\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.HyperACE.high_order_fuse","title":"high_order_fuse  <code>instance-attribute</code>","text":"<pre><code>high_order_fuse = Conv(c_h * k, c_h, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.HyperACE.low_order_branch","title":"low_order_branch  <code>instance-attribute</code>","text":"<pre><code>low_order_branch = Sequential(\n    *[\n        (DS_C3k(c_l, c_l, n=1, k=3, e=1.0))\n        for _ in (range(l))\n    ]\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.HyperACE.final_fuse","title":"final_fuse  <code>instance-attribute</code>","text":"<pre><code>final_fuse = Conv(c_h + c_l + c_s, out_channels, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.HyperACE.forward","title":"forward","text":"<pre><code>forward(x: list[Tensor]) -&gt; Any\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, x: list[Tensor]) -&gt; Any:\n    b2, b3, b4, b5 = x\n\n    _, _, h4, w4 = b4.shape\n\n    b2_resized = F.interpolate(b2, size=(h4, w4), mode=\"bilinear\", align_corners=False)\n    b3_resized = F.interpolate(b3, size=(h4, w4), mode=\"bilinear\", align_corners=False)\n    b5_resized = F.interpolate(b5, size=(h4, w4), mode=\"bilinear\", align_corners=False)\n\n    x_b = self.fuse_conv(torch.cat((b2_resized, b3_resized, b4, b5_resized), dim=1))\n\n    x_h, x_l, x_s = torch.split(x_b, [self.c_h, self.c_l, self.c_s], dim=1)\n\n    x_h_outs = [m(x_h) for m in self.high_order_branch]\n    x_h_fused = self.high_order_fuse(torch.cat(x_h_outs, dim=1))\n\n    x_l_out = self.low_order_branch(x_l)\n\n    y = self.final_fuse(torch.cat((x_h_fused, x_l_out, x_s), dim=1))\n\n    return y\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.GatedFusion","title":"GatedFusion","text":"<pre><code>GatedFusion(in_channels: int)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>gamma</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(self, in_channels: int):\n    super().__init__()\n    self.gamma = nn.Parameter(torch.zeros(1, in_channels, 1, 1))\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.GatedFusion.gamma","title":"gamma  <code>instance-attribute</code>","text":"<pre><code>gamma = Parameter(zeros(1, in_channels, 1, 1))\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.GatedFusion.forward","title":"forward","text":"<pre><code>forward(f_in: Tensor, h: Tensor) -&gt; Any\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, f_in: Tensor, h: Tensor) -&gt; Any:\n    if f_in.shape[1] != h.shape[1]:\n        raise ValueError(f\"Channel mismatch: f_in={f_in.shape}, h={h.shape}\")\n    return f_in + self.gamma * h\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.BackboneHyperAceV1","title":"BackboneHyperAceV1","text":"<pre><code>BackboneHyperAceV1(\n    in_channels: int = 256,\n    base_channels: int = 64,\n    base_depth: int = 3,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>stem</code> <code>p2</code> <code>p3</code> <code>p4</code> <code>p5</code> <code>out_channels</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(self, in_channels: int = 256, base_channels: int = 64, base_depth: int = 3):\n    super().__init__()\n    c2 = base_channels\n    c3 = 256\n    c4 = 384\n    c5 = 512\n    c6 = 768\n\n    self.stem = DSConv(in_channels, c2, k=3, s=(2, 1), p=1)\n\n    self.p2 = nn.Sequential(\n        DSConv(c2, c3, k=3, s=(2, 1), p=1),\n        DS_C3k2(c3, c3, n=base_depth),\n    )\n\n    self.p3 = nn.Sequential(\n        DSConv(c3, c4, k=3, s=(2, 1), p=1),\n        DS_C3k2(c4, c4, n=base_depth * 2),\n    )\n\n    self.p4 = nn.Sequential(\n        DSConv(c4, c5, k=3, s=(2, 1), p=1),\n        DS_C3k2(c5, c5, n=base_depth * 2),\n    )\n\n    self.p5 = nn.Sequential(\n        DSConv(c5, c6, k=3, s=(2, 1), p=1),\n        DS_C3k2(c6, c6, n=base_depth),\n    )\n\n    self.out_channels = [c3, c4, c5, c6]\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.BackboneHyperAceV1.stem","title":"stem  <code>instance-attribute</code>","text":"<pre><code>stem = DSConv(in_channels, c2, k=3, s=(2, 1), p=1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.BackboneHyperAceV1.p2","title":"p2  <code>instance-attribute</code>","text":"<pre><code>p2 = Sequential(\n    DSConv(c2, c3, k=3, s=(2, 1), p=1),\n    DS_C3k2(c3, c3, n=base_depth),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.BackboneHyperAceV1.p3","title":"p3  <code>instance-attribute</code>","text":"<pre><code>p3 = Sequential(\n    DSConv(c3, c4, k=3, s=(2, 1), p=1),\n    DS_C3k2(c4, c4, n=base_depth * 2),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.BackboneHyperAceV1.p4","title":"p4  <code>instance-attribute</code>","text":"<pre><code>p4 = Sequential(\n    DSConv(c4, c5, k=3, s=(2, 1), p=1),\n    DS_C3k2(c5, c5, n=base_depth * 2),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.BackboneHyperAceV1.p5","title":"p5  <code>instance-attribute</code>","text":"<pre><code>p5 = Sequential(\n    DSConv(c5, c6, k=3, s=(2, 1), p=1),\n    DS_C3k2(c6, c6, n=base_depth),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.BackboneHyperAceV1.out_channels","title":"out_channels  <code>instance-attribute</code>","text":"<pre><code>out_channels = [c3, c4, c5, c6]\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.BackboneHyperAceV1.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; list[Tensor]\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, x: Tensor) -&gt; list[Tensor]:\n    x = self.stem(x)\n    x2 = self.p2(x)\n    x3 = self.p3(x2)\n    x4 = self.p4(x3)\n    x5 = self.p5(x4)\n    return [x2, x3, x4, x5]\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.BackboneHyperAceV2","title":"BackboneHyperAceV2","text":"<pre><code>BackboneHyperAceV2(\n    in_channels: int = 256,\n    base_channels: int = 64,\n    base_depth: int = 3,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>stem</code> <code>p2</code> <code>p3</code> <code>p4</code> <code>p5</code> <code>out_channels</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(self, in_channels: int = 256, base_channels: int = 64, base_depth: int = 3):\n    super().__init__()\n    c2 = base_channels\n    c3 = 256\n    c4 = 384\n    c5 = 512\n    c6 = 768\n\n    self.stem = DSConv(in_channels, c2, k=3, s=(2, 1), p=1)\n\n    self.p2 = nn.Sequential(\n        DSConv(c2, c3, k=3, s=(2, 1), p=1),\n        DS_C3k2(c3, c3, n=base_depth),\n    )\n\n    self.p3 = nn.Sequential(\n        DSConv(c3, c4, k=3, s=(2, 1), p=1),\n        DS_C3k2(c4, c4, n=base_depth * 2),\n    )\n\n    self.p4 = nn.Sequential(\n        DSConv(c4, c5, k=3, s=2, p=1),\n        DS_C3k2(c5, c5, n=base_depth * 2),\n    )\n\n    self.p5 = nn.Sequential(\n        DSConv(c5, c6, k=3, s=2, p=1),\n        DS_C3k2(c6, c6, n=base_depth),\n    )\n\n    self.out_channels = [c3, c4, c5, c6]\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.BackboneHyperAceV2.stem","title":"stem  <code>instance-attribute</code>","text":"<pre><code>stem = DSConv(in_channels, c2, k=3, s=(2, 1), p=1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.BackboneHyperAceV2.p2","title":"p2  <code>instance-attribute</code>","text":"<pre><code>p2 = Sequential(\n    DSConv(c2, c3, k=3, s=(2, 1), p=1),\n    DS_C3k2(c3, c3, n=base_depth),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.BackboneHyperAceV2.p3","title":"p3  <code>instance-attribute</code>","text":"<pre><code>p3 = Sequential(\n    DSConv(c3, c4, k=3, s=(2, 1), p=1),\n    DS_C3k2(c4, c4, n=base_depth * 2),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.BackboneHyperAceV2.p4","title":"p4  <code>instance-attribute</code>","text":"<pre><code>p4 = Sequential(\n    DSConv(c4, c5, k=3, s=2, p=1),\n    DS_C3k2(c5, c5, n=base_depth * 2),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.BackboneHyperAceV2.p5","title":"p5  <code>instance-attribute</code>","text":"<pre><code>p5 = Sequential(\n    DSConv(c5, c6, k=3, s=2, p=1),\n    DS_C3k2(c6, c6, n=base_depth),\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.BackboneHyperAceV2.out_channels","title":"out_channels  <code>instance-attribute</code>","text":"<pre><code>out_channels = [c3, c4, c5, c6]\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.BackboneHyperAceV2.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; list[Tensor]\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, x: Tensor) -&gt; list[Tensor]:\n    x = self.stem(x)\n    x2 = self.p2(x)\n    x3 = self.p3(x2)\n    x4 = self.p4(x3)\n    x5 = self.p5(x4)\n    return [x2, x3, x4, x5]\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DecoderHyperAce","title":"DecoderHyperAce","text":"<pre><code>DecoderHyperAce(\n    encoder_channels: list[int],\n    hyperace_out_c: int,\n    decoder_channels: list[int],\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>h_to_d5</code> <code>h_to_d4</code> <code>h_to_d3</code> <code>h_to_d2</code> <code>fusion_d5</code> <code>fusion_d4</code> <code>fusion_d3</code> <code>fusion_d2</code> <code>skip_p5</code> <code>skip_p4</code> <code>skip_p3</code> <code>skip_p2</code> <code>up_d5</code> <code>up_d4</code> <code>up_d3</code> <code>final_d2</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(\n    self, encoder_channels: list[int], hyperace_out_c: int, decoder_channels: list[int]\n):\n    super().__init__()\n    c_p2, c_p3, c_p4, c_p5 = encoder_channels\n    c_d2, c_d3, c_d4, c_d5 = decoder_channels\n\n    self.h_to_d5 = Conv(hyperace_out_c, c_d5, 1, 1)\n    self.h_to_d4 = Conv(hyperace_out_c, c_d4, 1, 1)\n    self.h_to_d3 = Conv(hyperace_out_c, c_d3, 1, 1)\n    self.h_to_d2 = Conv(hyperace_out_c, c_d2, 1, 1)\n\n    self.fusion_d5 = GatedFusion(c_d5)\n    self.fusion_d4 = GatedFusion(c_d4)\n    self.fusion_d3 = GatedFusion(c_d3)\n    self.fusion_d2 = GatedFusion(c_d2)\n\n    self.skip_p5 = Conv(c_p5, c_d5, 1, 1)\n    self.skip_p4 = Conv(c_p4, c_d4, 1, 1)\n    self.skip_p3 = Conv(c_p3, c_d3, 1, 1)\n    self.skip_p2 = Conv(c_p2, c_d2, 1, 1)\n\n    self.up_d5 = DS_C3k2(c_d5, c_d4, n=1)\n    self.up_d4 = DS_C3k2(c_d4, c_d3, n=1)\n    self.up_d3 = DS_C3k2(c_d3, c_d2, n=1)\n\n    self.final_d2 = DS_C3k2(c_d2, c_d2, n=1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DecoderHyperAce.h_to_d5","title":"h_to_d5  <code>instance-attribute</code>","text":"<pre><code>h_to_d5 = Conv(hyperace_out_c, c_d5, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DecoderHyperAce.h_to_d4","title":"h_to_d4  <code>instance-attribute</code>","text":"<pre><code>h_to_d4 = Conv(hyperace_out_c, c_d4, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DecoderHyperAce.h_to_d3","title":"h_to_d3  <code>instance-attribute</code>","text":"<pre><code>h_to_d3 = Conv(hyperace_out_c, c_d3, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DecoderHyperAce.h_to_d2","title":"h_to_d2  <code>instance-attribute</code>","text":"<pre><code>h_to_d2 = Conv(hyperace_out_c, c_d2, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DecoderHyperAce.fusion_d5","title":"fusion_d5  <code>instance-attribute</code>","text":"<pre><code>fusion_d5 = GatedFusion(c_d5)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DecoderHyperAce.fusion_d4","title":"fusion_d4  <code>instance-attribute</code>","text":"<pre><code>fusion_d4 = GatedFusion(c_d4)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DecoderHyperAce.fusion_d3","title":"fusion_d3  <code>instance-attribute</code>","text":"<pre><code>fusion_d3 = GatedFusion(c_d3)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DecoderHyperAce.fusion_d2","title":"fusion_d2  <code>instance-attribute</code>","text":"<pre><code>fusion_d2 = GatedFusion(c_d2)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DecoderHyperAce.skip_p5","title":"skip_p5  <code>instance-attribute</code>","text":"<pre><code>skip_p5 = Conv(c_p5, c_d5, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DecoderHyperAce.skip_p4","title":"skip_p4  <code>instance-attribute</code>","text":"<pre><code>skip_p4 = Conv(c_p4, c_d4, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DecoderHyperAce.skip_p3","title":"skip_p3  <code>instance-attribute</code>","text":"<pre><code>skip_p3 = Conv(c_p3, c_d3, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DecoderHyperAce.skip_p2","title":"skip_p2  <code>instance-attribute</code>","text":"<pre><code>skip_p2 = Conv(c_p2, c_d2, 1, 1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DecoderHyperAce.up_d5","title":"up_d5  <code>instance-attribute</code>","text":"<pre><code>up_d5 = DS_C3k2(c_d5, c_d4, n=1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DecoderHyperAce.up_d4","title":"up_d4  <code>instance-attribute</code>","text":"<pre><code>up_d4 = DS_C3k2(c_d4, c_d3, n=1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DecoderHyperAce.up_d3","title":"up_d3  <code>instance-attribute</code>","text":"<pre><code>up_d3 = DS_C3k2(c_d3, c_d2, n=1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DecoderHyperAce.final_d2","title":"final_d2  <code>instance-attribute</code>","text":"<pre><code>final_d2 = DS_C3k2(c_d2, c_d2, n=1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.DecoderHyperAce.forward","title":"forward","text":"<pre><code>forward(enc_feats: list[Tensor], h_ace: Tensor) -&gt; Any\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, enc_feats: list[Tensor], h_ace: Tensor) -&gt; Any:\n    p2, p3, p4, p5 = enc_feats\n\n    d5 = self.skip_p5(p5)\n    h_d5 = self.h_to_d5(F.interpolate(h_ace, size=d5.shape[2:], mode=\"bilinear\"))\n    d5 = self.fusion_d5(d5, h_d5)\n\n    d5_up = F.interpolate(d5, size=p4.shape[2:], mode=\"bilinear\")\n    d4_skip = self.skip_p4(p4)\n    d4 = self.up_d5(d5_up) + d4_skip\n\n    h_d4 = self.h_to_d4(F.interpolate(h_ace, size=d4.shape[2:], mode=\"bilinear\"))\n    d4 = self.fusion_d4(d4, h_d4)\n\n    d4_up = F.interpolate(d4, size=p3.shape[2:], mode=\"bilinear\")\n    d3_skip = self.skip_p3(p3)\n    d3 = self.up_d4(d4_up) + d3_skip\n\n    h_d3 = self.h_to_d3(F.interpolate(h_ace, size=d3.shape[2:], mode=\"bilinear\"))\n    d3 = self.fusion_d3(d3, h_d3)\n\n    d3_up = F.interpolate(d3, size=p2.shape[2:], mode=\"bilinear\")\n    d2_skip = self.skip_p2(p2)\n    d2 = self.up_d3(d3_up) + d2_skip\n\n    h_d2 = self.h_to_d2(F.interpolate(h_ace, size=d2.shape[2:], mode=\"bilinear\"))\n    d2 = self.fusion_d2(d2, h_d2)\n\n    d2_final = self.final_d2(d2)\n\n    return d2_final\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.FreqPixelShuffleV1","title":"FreqPixelShuffleV1","text":"<pre><code>FreqPixelShuffleV1(\n    in_channels: int, out_channels: int, scale: int = 2\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>scale</code> <code>conv</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(self, in_channels: int, out_channels: int, scale: int = 2):\n    super().__init__()\n    self.scale = scale\n    self.conv = DSConv(in_channels, out_channels * scale, k=3, s=1, p=1)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.FreqPixelShuffleV1.scale","title":"scale  <code>instance-attribute</code>","text":"<pre><code>scale = scale\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.FreqPixelShuffleV1.conv","title":"conv  <code>instance-attribute</code>","text":"<pre><code>conv = DSConv(\n    in_channels, out_channels * scale, k=3, s=1, p=1\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.FreqPixelShuffleV1.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Any\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Any:\n    x = self.conv(x)\n    b, c_r, h, w = x.shape\n    out_c = c_r // self.scale\n\n    x = x.view(b, out_c, self.scale, h, w)\n\n    x = x.permute(0, 1, 3, 4, 2).contiguous()\n    x = x.view(b, out_c, h, w * self.scale)\n\n    return x\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.ProgressiveUpsampleHeadV1","title":"ProgressiveUpsampleHeadV1","text":"<pre><code>ProgressiveUpsampleHeadV1(\n    in_channels: int,\n    out_channels: int,\n    target_bins: int = 1025,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>target_bins</code> <code>block1</code> <code>block2</code> <code>block3</code> <code>block4</code> <code>final_conv</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(self, in_channels: int, out_channels: int, target_bins: int = 1025):\n    super().__init__()\n    self.target_bins = target_bins\n\n    c = in_channels\n\n    self.block1 = FreqPixelShuffleV1(c, c, scale=2)\n    self.block2 = FreqPixelShuffleV1(c, c // 2, scale=2)\n    self.block3 = FreqPixelShuffleV1(c // 2, c // 2, scale=2)\n    self.block4 = FreqPixelShuffleV1(c // 2, c // 4, scale=2)\n\n    self.final_conv = nn.Conv2d(c // 4, out_channels, kernel_size=1, bias=False)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.ProgressiveUpsampleHeadV1.target_bins","title":"target_bins  <code>instance-attribute</code>","text":"<pre><code>target_bins = target_bins\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.ProgressiveUpsampleHeadV1.block1","title":"block1  <code>instance-attribute</code>","text":"<pre><code>block1 = FreqPixelShuffleV1(c, c, scale=2)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.ProgressiveUpsampleHeadV1.block2","title":"block2  <code>instance-attribute</code>","text":"<pre><code>block2 = FreqPixelShuffleV1(c, c // 2, scale=2)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.ProgressiveUpsampleHeadV1.block3","title":"block3  <code>instance-attribute</code>","text":"<pre><code>block3 = FreqPixelShuffleV1(c // 2, c // 2, scale=2)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.ProgressiveUpsampleHeadV1.block4","title":"block4  <code>instance-attribute</code>","text":"<pre><code>block4 = FreqPixelShuffleV1(c // 2, c // 4, scale=2)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.ProgressiveUpsampleHeadV1.final_conv","title":"final_conv  <code>instance-attribute</code>","text":"<pre><code>final_conv = Conv2d(\n    c // 4, out_channels, kernel_size=1, bias=False\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.ProgressiveUpsampleHeadV1.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Any\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Any:\n    x = self.block1(x)\n    x = self.block2(x)\n    x = self.block3(x)\n    x = self.block4(x)\n\n    if x.shape[-1] != self.target_bins:\n        x = F.interpolate(\n            x,\n            size=(x.shape[2], self.target_bins),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n\n    x = self.final_conv(x)\n    return x\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.FreqPixelShuffleV2","title":"FreqPixelShuffleV2","text":"<pre><code>FreqPixelShuffleV2(\n    in_channels: int, out_channels: int, scale: int, f: int\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>scale</code> <code>conv</code> <code>out_conv</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(self, in_channels: int, out_channels: int, scale: int, f: int):\n    super().__init__()\n    self.scale = scale\n    self.conv = DSConv(in_channels, out_channels * scale)\n    self.out_conv = build_hyperace_tfc_tdf(out_channels, out_channels, 2, f)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.FreqPixelShuffleV2.scale","title":"scale  <code>instance-attribute</code>","text":"<pre><code>scale = scale\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.FreqPixelShuffleV2.conv","title":"conv  <code>instance-attribute</code>","text":"<pre><code>conv = DSConv(in_channels, out_channels * scale)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.FreqPixelShuffleV2.out_conv","title":"out_conv  <code>instance-attribute</code>","text":"<pre><code>out_conv = build_hyperace_tfc_tdf(\n    out_channels, out_channels, 2, f\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.FreqPixelShuffleV2.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Any\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Any:\n    x = self.conv(x)\n    b, c_r, h, w = x.shape\n    out_c = c_r // self.scale\n\n    x = x.view(b, out_c, self.scale, h, w)\n\n    x = x.permute(0, 1, 3, 4, 2).contiguous()\n    x = x.view(b, out_c, h, w * self.scale)\n\n    return self.out_conv(x)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.ProgressiveUpsampleHeadV2","title":"ProgressiveUpsampleHeadV2","text":"<pre><code>ProgressiveUpsampleHeadV2(\n    in_channels: int,\n    out_channels: int,\n    target_bins: int = 1025,\n    in_bands: int = 62,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>target_bins</code> <code>block1</code> <code>block2</code> <code>block3</code> <code>block4</code> <code>final_conv</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(\n    self, in_channels: int, out_channels: int, target_bins: int = 1025, in_bands: int = 62\n):\n    super().__init__()\n    self.target_bins = target_bins\n\n    c = in_channels\n\n    self.block1 = FreqPixelShuffleV2(c, c // 2, scale=2, f=in_bands * 2)\n    self.block2 = FreqPixelShuffleV2(c // 2, c // 4, scale=2, f=in_bands * 4)\n    self.block3 = FreqPixelShuffleV2(c // 4, c // 8, scale=2, f=in_bands * 8)\n    self.block4 = FreqPixelShuffleV2(c // 8, c // 16, scale=2, f=in_bands * 16)\n\n    self.final_conv = nn.Conv2d(\n        c // 16,\n        out_channels,\n        kernel_size=3,\n        stride=1,\n        padding=\"same\",\n        bias=False,\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.ProgressiveUpsampleHeadV2.target_bins","title":"target_bins  <code>instance-attribute</code>","text":"<pre><code>target_bins = target_bins\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.ProgressiveUpsampleHeadV2.block1","title":"block1  <code>instance-attribute</code>","text":"<pre><code>block1 = FreqPixelShuffleV2(\n    c, c // 2, scale=2, f=in_bands * 2\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.ProgressiveUpsampleHeadV2.block2","title":"block2  <code>instance-attribute</code>","text":"<pre><code>block2 = FreqPixelShuffleV2(\n    c // 2, c // 4, scale=2, f=in_bands * 4\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.ProgressiveUpsampleHeadV2.block3","title":"block3  <code>instance-attribute</code>","text":"<pre><code>block3 = FreqPixelShuffleV2(\n    c // 4, c // 8, scale=2, f=in_bands * 8\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.ProgressiveUpsampleHeadV2.block4","title":"block4  <code>instance-attribute</code>","text":"<pre><code>block4 = FreqPixelShuffleV2(\n    c // 8, c // 16, scale=2, f=in_bands * 16\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.ProgressiveUpsampleHeadV2.final_conv","title":"final_conv  <code>instance-attribute</code>","text":"<pre><code>final_conv = Conv2d(\n    c // 16,\n    out_channels,\n    kernel_size=3,\n    stride=1,\n    padding=\"same\",\n    bias=False,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.ProgressiveUpsampleHeadV2.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Any\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Any:\n    x = self.block1(x)\n    x = self.block2(x)\n    x = self.block3(x)\n    x = self.block4(x)\n\n    if x.shape[-1] != self.target_bins:\n        x = F.interpolate(\n            x,\n            size=(x.shape[2], self.target_bins),\n            mode=\"bilinear\",\n            align_corners=False,\n        )\n\n    x = self.final_conv(x)\n    return x\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.SegmModelHyperAceV1","title":"SegmModelHyperAceV1","text":"<pre><code>SegmModelHyperAceV1(\n    in_bands: int = 62,\n    in_dim: int = 256,\n    out_bins: int = 1025,\n    out_channels: int = 4,\n    base_channels: int = 64,\n    base_depth: int = 2,\n    num_hyperedges: int = 16,\n    num_heads: int = 8,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>backbone</code> <code>hyperace</code> <code>decoder</code> <code>upsample_head</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(\n    self,\n    in_bands: int = 62,\n    in_dim: int = 256,\n    out_bins: int = 1025,\n    out_channels: int = 4,\n    base_channels: int = 64,\n    base_depth: int = 2,\n    num_hyperedges: int = 16,\n    num_heads: int = 8,\n):\n    super().__init__()\n\n    self.backbone = BackboneHyperAceV1(\n        in_channels=in_dim,\n        base_channels=base_channels,\n        base_depth=base_depth,\n    )\n    enc_channels = self.backbone.out_channels\n    c2, c3, c4, c5 = enc_channels\n\n    hyperace_in_channels = enc_channels\n    hyperace_out_channels = c4\n    self.hyperace = HyperACE(\n        hyperace_in_channels,\n        hyperace_out_channels,\n        num_hyperedges,\n        num_heads,\n        k=3,\n        l=2,\n    )\n\n    decoder_channels = [c2, c3, c4, c5]\n    self.decoder = DecoderHyperAce(enc_channels, hyperace_out_channels, decoder_channels)\n\n    self.upsample_head = ProgressiveUpsampleHeadV1(\n        in_channels=decoder_channels[0],\n        out_channels=out_channels,\n        target_bins=out_bins,\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.SegmModelHyperAceV1.backbone","title":"backbone  <code>instance-attribute</code>","text":"<pre><code>backbone = BackboneHyperAceV1(\n    in_channels=in_dim,\n    base_channels=base_channels,\n    base_depth=base_depth,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.SegmModelHyperAceV1.hyperace","title":"hyperace  <code>instance-attribute</code>","text":"<pre><code>hyperace = HyperACE(\n    hyperace_in_channels,\n    hyperace_out_channels,\n    num_hyperedges,\n    num_heads,\n    k=3,\n    l=2,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.SegmModelHyperAceV1.decoder","title":"decoder  <code>instance-attribute</code>","text":"<pre><code>decoder = DecoderHyperAce(\n    enc_channels, hyperace_out_channels, decoder_channels\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.SegmModelHyperAceV1.upsample_head","title":"upsample_head  <code>instance-attribute</code>","text":"<pre><code>upsample_head = ProgressiveUpsampleHeadV1(\n    in_channels=decoder_channels[0],\n    out_channels=out_channels,\n    target_bins=out_bins,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.SegmModelHyperAceV1.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Any\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Any:\n    h, _ = x.shape[2:]\n\n    enc_feats = self.backbone(x)\n\n    h_ace_feats = self.hyperace(enc_feats)\n\n    dec_feat = self.decoder(enc_feats, h_ace_feats)\n\n    feat_time_restored = F.interpolate(\n        dec_feat,\n        size=(h, dec_feat.shape[-1]),\n        mode=\"bilinear\",\n        align_corners=False,\n    )\n\n    out = self.upsample_head(feat_time_restored)\n\n    return out\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.SegmModelHyperAceV2","title":"SegmModelHyperAceV2","text":"<pre><code>SegmModelHyperAceV2(\n    in_bands: int = 62,\n    in_dim: int = 256,\n    out_bins: int = 1025,\n    out_channels: int = 4,\n    base_channels: int = 64,\n    base_depth: int = 2,\n    num_hyperedges: int = 32,\n    num_heads: int = 8,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>backbone</code> <code>hyperace</code> <code>decoder</code> <code>upsample_head</code> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def __init__(\n    self,\n    in_bands: int = 62,\n    in_dim: int = 256,\n    out_bins: int = 1025,\n    out_channels: int = 4,\n    base_channels: int = 64,\n    base_depth: int = 2,\n    num_hyperedges: int = 32,\n    num_heads: int = 8,\n):\n    super().__init__()\n\n    self.backbone = BackboneHyperAceV2(\n        in_channels=in_dim,\n        base_channels=base_channels,\n        base_depth=base_depth,\n    )\n    enc_channels = self.backbone.out_channels\n    c2, c3, c4, c5 = enc_channels\n\n    hyperace_in_channels = enc_channels\n    hyperace_out_channels = c4\n    self.hyperace = HyperACE(\n        hyperace_in_channels,\n        hyperace_out_channels,\n        num_hyperedges,\n        num_heads,\n        k=2,\n        l=1,\n    )\n\n    decoder_channels = [c2, c3, c4, c5]\n    self.decoder = DecoderHyperAce(enc_channels, hyperace_out_channels, decoder_channels)\n\n    self.upsample_head = ProgressiveUpsampleHeadV2(\n        in_channels=decoder_channels[0],\n        out_channels=out_channels,\n        target_bins=out_bins,\n        in_bands=in_bands,\n    )\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.SegmModelHyperAceV2.backbone","title":"backbone  <code>instance-attribute</code>","text":"<pre><code>backbone = BackboneHyperAceV2(\n    in_channels=in_dim,\n    base_channels=base_channels,\n    base_depth=base_depth,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.SegmModelHyperAceV2.hyperace","title":"hyperace  <code>instance-attribute</code>","text":"<pre><code>hyperace = HyperACE(\n    hyperace_in_channels,\n    hyperace_out_channels,\n    num_hyperedges,\n    num_heads,\n    k=2,\n    l=1,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.SegmModelHyperAceV2.decoder","title":"decoder  <code>instance-attribute</code>","text":"<pre><code>decoder = DecoderHyperAce(\n    enc_channels, hyperace_out_channels, decoder_channels\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.SegmModelHyperAceV2.upsample_head","title":"upsample_head  <code>instance-attribute</code>","text":"<pre><code>upsample_head = ProgressiveUpsampleHeadV2(\n    in_channels=decoder_channels[0],\n    out_channels=out_channels,\n    target_bins=out_bins,\n    in_bands=in_bands,\n)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.hyperace.SegmModelHyperAceV2.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; Any\n</code></pre> Source code in <code>src/splifft/models/utils/hyperace.py</code> <pre><code>def forward(self, x: Tensor) -&gt; Any:\n    h, _ = x.shape[2:]\n\n    enc_feats = self.backbone(x)\n\n    h_ace_feats = self.hyperace(enc_feats)\n\n    dec_feat = self.decoder(enc_feats, h_ace_feats)\n\n    feat_time_restored = F.interpolate(\n        dec_feat,\n        size=(h, dec_feat.shape[-1]),\n        mode=\"bilinear\",\n        align_corners=False,\n    )\n\n    out = self.upsample_head(feat_time_restored)\n\n    return out\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft","title":"stft","text":"<p>Classes:</p> Name Description <code>Stft</code> <p>A custom STFT implementation using 1D convolutions to ensure compatibility with CoreML.</p> <code>IStft</code> <p>A simple wrapper around torch.istft with a hacky workaround for MPS.</p>"},{"location":"api/models/#splifft.models.utils.stft.Stft","title":"Stft","text":"<pre><code>Stft(\n    n_fft: int,\n    hop_length: int,\n    win_length: int,\n    window_fn: Callable[[int], Tensor],\n    conv_dtype: dtype | None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A custom STFT implementation using 1D convolutions to ensure compatibility with CoreML.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>n_fft</code> <code>hop_length</code> <code>win_length</code> <code>conv_dtype</code> Source code in <code>src/splifft/models/utils/stft.py</code> <pre><code>def __init__(\n    self,\n    n_fft: int,\n    hop_length: int,\n    win_length: int,\n    window_fn: Callable[[int], Tensor],\n    conv_dtype: torch.dtype | None,\n):\n    super().__init__()\n    self.n_fft = n_fft\n    self.hop_length = hop_length\n    self.win_length = win_length\n    self.conv_dtype = conv_dtype\n\n    window = window_fn(self.win_length)\n\n    dft_mat = torch.fft.fft(torch.eye(self.n_fft, device=window.device))\n    dft_mat_T = dft_mat.T\n\n    real_kernels = dft_mat_T.real[\n        : self.win_length, : (self.n_fft // 2 + 1)\n    ] * window.unsqueeze(-1)\n    imag_kernels = dft_mat_T.imag[\n        : self.win_length, : (self.n_fft // 2 + 1)\n    ] * window.unsqueeze(-1)\n\n    # (out_channels, in_channels, kernel_size)\n    self.register_buffer(\"real_conv_weight\", real_kernels.T.unsqueeze(1).to(self.conv_dtype))\n    self.register_buffer(\"imag_conv_weight\", imag_kernels.T.unsqueeze(1).to(self.conv_dtype))\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.Stft.n_fft","title":"n_fft  <code>instance-attribute</code>","text":"<pre><code>n_fft = n_fft\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.Stft.hop_length","title":"hop_length  <code>instance-attribute</code>","text":"<pre><code>hop_length = hop_length\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.Stft.win_length","title":"win_length  <code>instance-attribute</code>","text":"<pre><code>win_length = win_length\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.Stft.conv_dtype","title":"conv_dtype  <code>instance-attribute</code>","text":"<pre><code>conv_dtype = conv_dtype\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.Stft.forward","title":"forward","text":"<pre><code>forward(x: Tensor) -&gt; ComplexSpectrogram\n</code></pre> Source code in <code>src/splifft/models/utils/stft.py</code> <pre><code>def forward(self, x: Tensor) -&gt; t.ComplexSpectrogram:\n    b, s, t = x.shape\n    x = x.reshape(b * s, 1, t).to(self.conv_dtype)\n\n    padding = self.n_fft // 2\n    x = F.pad(x, (padding, padding), \"reflect\")\n\n    real_part = F.conv1d(x, self.real_conv_weight, stride=self.hop_length)  # type: ignore\n    imag_part = F.conv1d(x, self.imag_conv_weight, stride=self.hop_length)  # type: ignore\n    spec = torch.stack((real_part, imag_part), dim=-1)  # (b*s, f, t_frames, c=2)\n\n    _bs, f, t_frames, c = spec.shape\n    spec = spec.view(b, s, f, t_frames, c)\n\n    return spec  # type: ignore\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.IStft","title":"IStft","text":"<pre><code>IStft(\n    n_fft: int,\n    hop_length: int,\n    win_length: int,\n    window_fn: Callable[[int], Tensor] = hann_window,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A simple wrapper around torch.istft with a hacky workaround for MPS.</p> <p>TODO: implement a proper workaround.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Attributes:</p> Name Type Description <code>n_fft</code> <code>hop_length</code> <code>win_length</code> <code>window</code> Source code in <code>src/splifft/models/utils/stft.py</code> <pre><code>def __init__(\n    self,\n    n_fft: int,\n    hop_length: int,\n    win_length: int,\n    window_fn: Callable[[int], Tensor] = torch.hann_window,\n):\n    super().__init__()\n    self.n_fft = n_fft\n    self.hop_length = hop_length\n    self.win_length = win_length\n    self.window = window_fn(self.win_length)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.IStft.n_fft","title":"n_fft  <code>instance-attribute</code>","text":"<pre><code>n_fft = n_fft\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.IStft.hop_length","title":"hop_length  <code>instance-attribute</code>","text":"<pre><code>hop_length = hop_length\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.IStft.win_length","title":"win_length  <code>instance-attribute</code>","text":"<pre><code>win_length = win_length\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.IStft.window","title":"window  <code>instance-attribute</code>","text":"<pre><code>window = window_fn(win_length)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.stft.IStft.forward","title":"forward","text":"<pre><code>forward(\n    spec: ComplexSpectrogram, length: int | None = None\n) -&gt; RawAudioTensor | NormalizedAudioTensor\n</code></pre> Source code in <code>src/splifft/models/utils/stft.py</code> <pre><code>def forward(\n    self, spec: t.ComplexSpectrogram, length: int | None = None\n) -&gt; t.RawAudioTensor | t.NormalizedAudioTensor:\n    device = spec.device\n    is_mps = device.type == \"mps\"\n    window = self.window.to(device)\n    # see https://github.com/lucidrains/BS-RoFormer/issues/47\n    # this would introduce a breaking change.\n    # spec = spec.index_fill(1, torch.tensor(0, device=spec.device), 0.)  # type: ignore\n    spec_complex = torch.view_as_complex(spec)\n\n    try:\n        audio = torch.istft(\n            spec_complex,\n            n_fft=self.n_fft,\n            hop_length=self.hop_length,\n            win_length=self.win_length,\n            window=window,\n            return_complex=False,\n            length=length,\n        )\n    except RuntimeError:\n        audio = torch.istft(\n            spec_complex.cpu() if is_mps else spec_complex,\n            n_fft=self.n_fft,\n            hop_length=self.hop_length,\n            win_length=self.win_length,\n            window=window.cpu() if is_mps else window,\n            return_complex=False,\n            length=length,\n        ).to(device)\n\n    return audio  # type: ignore\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend_sage","title":"attend_sage","text":"<p>Classes:</p> Name Description <code>AttendSage</code> <p>Attributes:</p> Name Type Description <code>logger</code>"},{"location":"api/models/#splifft.models.utils.attend_sage.logger","title":"logger  <code>module-attribute</code>","text":"<pre><code>logger = getLogger(__name__)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend_sage.AttendSage","title":"AttendSage","text":"<pre><code>AttendSage(\n    dropout: float = 0.0,\n    flash: bool = False,\n    scale: float | None = None,\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Parameters:</p> Name Type Description Default <code>flash</code> <code>bool</code> <p>if True, attempts to use SageAttention or PyTorch SDPA.</p> <code>False</code> <p>Methods:</p> Name Description <code>forward</code> <p>einstein notation</p> <p>Attributes:</p> Name Type Description <code>scale</code> <code>dropout</code> <code>use_sage</code> <code>use_pytorch_sdpa</code> <code>attn_dropout</code> Source code in <code>src/splifft/models/utils/attend_sage.py</code> <pre><code>def __init__(\n    self,\n    dropout: float = 0.0,\n    flash: bool = False,\n    scale: float | None = None,\n):\n    \"\"\"\n    :param flash: if True, attempts to use SageAttention or PyTorch SDPA.\n    \"\"\"\n    super().__init__()\n    self.scale = scale  # for einsum path\n    self.dropout = dropout  # for einsum/SDPA path\n\n    self.use_sage = flash and _has_sage_attention\n    self.use_pytorch_sdpa = False\n    self._sdpa_checked = False\n\n    if flash and not self.use_sage:\n        if not self._sdpa_checked:\n            if parse_version(torch.__version__) &gt;= (2, 0, 0):\n                self.use_pytorch_sdpa = True\n                log_once(\n                    logger,\n                    \"Using PyTorch SDPA backend (FlashAttention-2, Memory-Efficient, or Math).\",\n                )\n            else:\n                log_once(\n                    logger,\n                    \"Flash attention requested but Pytorch &lt; 2.0 and SageAttention not found. Falling back to einsum.\",\n                )\n            self._sdpa_checked = True\n\n    # dropout layer for manual einsum implementation ONLY\n    # SDPA and SageAttention handle dropout differently\n    # (or not at all in Sage's base API)\n    self.attn_dropout = nn.Dropout(dropout)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend_sage.AttendSage.scale","title":"scale  <code>instance-attribute</code>","text":"<pre><code>scale = scale\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend_sage.AttendSage.dropout","title":"dropout  <code>instance-attribute</code>","text":"<pre><code>dropout = dropout\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend_sage.AttendSage.use_sage","title":"use_sage  <code>instance-attribute</code>","text":"<pre><code>use_sage = flash and _has_sage_attention\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend_sage.AttendSage.use_pytorch_sdpa","title":"use_pytorch_sdpa  <code>instance-attribute</code>","text":"<pre><code>use_pytorch_sdpa = False\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend_sage.AttendSage.attn_dropout","title":"attn_dropout  <code>instance-attribute</code>","text":"<pre><code>attn_dropout = Dropout(dropout)\n</code></pre>"},{"location":"api/models/#splifft.models.utils.attend_sage.AttendSage.forward","title":"forward","text":"<pre><code>forward(q: Tensor, k: Tensor, v: Tensor) -&gt; Tensor\n</code></pre> <p>einstein notation</p> <ul> <li>b: batch</li> <li>h: heads</li> <li>n, i, j: sequence length (base sequence length, source, target)</li> <li>d: feature dimension</li> </ul> <p>Input tensors q, k, v expected in shape: (batch, heads, seq_len, dim_head) -&gt; HND layout</p> Source code in <code>src/splifft/models/utils/attend_sage.py</code> <pre><code>def forward(self, q: Tensor, k: Tensor, v: Tensor) -&gt; Tensor:\n    \"\"\"\n    einstein notation\n\n    - b: batch\n    - h: heads\n    - n, i, j: sequence length (base sequence length, source, target)\n    - d: feature dimension\n\n    Input tensors q, k, v expected in shape: (batch, heads, seq_len, dim_head) -&gt; HND layout\n    \"\"\"\n    _q_len, _k_len, _device = q.shape[-2], k.shape[-2], q.device\n\n    # priority 1: SageAttention\n    if self.use_sage:\n        # assumes q, k, v are FP16/BF16 (handled by autocast upstream)\n        # assumes scale is handled internally by sageattn\n        # assumes dropout is NOT handled by sageattn kernel\n        # is_causal=False based on how Attend is called in mel_band_roformer\n        out = sageattn(q, k, v, tensor_layout=\"HND\", is_causal=False)  # type: ignore\n        return out  # type: ignore\n        try:\n            out = sageattn(q, k, v, tensor_layout=\"HND\", is_causal=False)\n            return out\n        except Exception as e:\n            logger.error(f\"SageAttention failed with error: {e}. Falling back.\")\n            self.use_sage = False\n            if not self._sdpa_checked:\n                if parse_version(torch.__version__) &gt;= (2, 0, 0):\n                    self.use_pytorch_sdpa = True\n                    log_once(logger, \"falling back to PyTorch SDPA\")\n                else:\n                    log_once(logger, \"falling back to einsum.\")\n\n                self._sdpa_checked = True\n\n    # priority 2: PyTorch SDPA\n    if self.use_pytorch_sdpa:\n        # it handles scaling and dropout internally.\n        try:\n            with sdpa_kernel(\n                [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION, SDPBackend.MATH]\n            ):\n                out = F.scaled_dot_product_attention(\n                    q,\n                    k,\n                    v,\n                    attn_mask=None,  # assuming no explicit mask needed here\n                    dropout_p=self.dropout if self.training else 0.0,\n                    is_causal=False,  # assuming not needed based on usage context\n                )\n            return out\n        except Exception as e:\n            log_once(\n                logger,\n                f\"pytorch SDPA failed with error: {e}. falling back to einsum.\",\n                level=logging.ERROR,\n            )\n            self.use_pytorch_sdpa = False\n\n    scale = self.scale or q.shape[-1] ** -0.5\n\n    # similarity\n    sim = einsum(\"b h i d, b h j d -&gt; b h i j\", q, k) * scale\n\n    # attention\n    attn = sim.softmax(dim=-1)\n    attn = self.attn_dropout(attn)  # ONLY in einsum path\n\n    # aggregate values\n    out = einsum(\"b h i j, b h j d -&gt; b h i d\", attn, v)\n\n    return out\n</code></pre>"},{"location":"api/training/","title":"Training","text":""},{"location":"api/training/#splifft.training","title":"training","text":"<p>High level orchestrator for model training</p> <p>Warning</p> <p>This module is incomplete. they only contain annotations for future use.</p> <p>Attributes:</p> Name Type Description <code>Epoch</code> <code>TypeAlias</code> <p>The number of times the model has seen the entire training dataset.</p> <code>TrainingBatchSize</code> <code>TypeAlias</code> <p>Number of training examples (audio chunks) processed before a weight update.</p> <code>GradientAccumulationSteps</code> <code>TypeAlias</code> <p>Number of batches to process before performing a weight update.</p> <code>OptimizerName</code> <code>TypeAlias</code> <p>Algorithm used to update the model weights to minimize the loss function.</p> <code>LearningRateSchedulerName</code> <code>TypeAlias</code> <p>Algorithm used to adjust the learning rate during training.</p> <code>UseAutomaticMixedPrecision</code> <code>TypeAlias</code> <p>Whether to use automatic mixed precision (AMP) during training.</p> <code>UseLoRA</code> <code>TypeAlias</code> <p>Whether to use Low-Rank Adaptation for efficient fine-tuning.</p>"},{"location":"api/training/#splifft.training.Epoch","title":"Epoch  <code>module-attribute</code>","text":"<pre><code>Epoch: TypeAlias = int\n</code></pre> <p>The number of times the model has seen the entire training dataset.</p>"},{"location":"api/training/#splifft.training.TrainingBatchSize","title":"TrainingBatchSize  <code>module-attribute</code>","text":"<pre><code>TrainingBatchSize: TypeAlias = Annotated[int, Gt(0)]\n</code></pre> <p>Number of training examples (audio chunks) processed before a weight update.</p> <p>Larger batches may offer more stable gradients but require more memory.</p>"},{"location":"api/training/#splifft.training.GradientAccumulationSteps","title":"GradientAccumulationSteps  <code>module-attribute</code>","text":"<pre><code>GradientAccumulationSteps: TypeAlias = Annotated[int, Gt(0)]\n</code></pre> <p>Number of batches to process before performing a weight update.</p> <p>This simulates a larger batch size without increasing memory, e.g., a batch size of 4 with 8 accumulation steps has an effective batch size of 32.</p>"},{"location":"api/training/#splifft.training.OptimizerName","title":"OptimizerName  <code>module-attribute</code>","text":"<pre><code>OptimizerName: TypeAlias = str\n</code></pre> <p>Algorithm used to update the model weights to minimize the loss function.</p>"},{"location":"api/training/#splifft.training.LearningRateSchedulerName","title":"LearningRateSchedulerName  <code>module-attribute</code>","text":"<pre><code>LearningRateSchedulerName: TypeAlias = str\n</code></pre> <p>Algorithm used to adjust the learning rate during training.</p> <p>e.g. <code>ReduceLROnPlateau</code> can reduce the learning rate when a metric stops improving.</p>"},{"location":"api/training/#splifft.training.UseAutomaticMixedPrecision","title":"UseAutomaticMixedPrecision  <code>module-attribute</code>","text":"<pre><code>UseAutomaticMixedPrecision: TypeAlias = bool\n</code></pre> <p>Whether to use automatic mixed precision (AMP) during training.</p>"},{"location":"api/training/#splifft.training.UseLoRA","title":"UseLoRA  <code>module-attribute</code>","text":"<pre><code>UseLoRA: TypeAlias = bool\n</code></pre> <p>Whether to use Low-Rank Adaptation for efficient fine-tuning.</p> <p>This freezes pre-trained weights and injects smaller, trainable low-rank matrices, dramatically reducing the number of trainable parameters.</p>"},{"location":"api/types/","title":"Types","text":""},{"location":"api/types/#splifft.types","title":"types","text":"<p>Types for documentation and data validation (for use in pydantic).</p> <p>They provide semantic meaning only and we additionally use <code>NewType</code> for strong semantic distinction to avoid mixing up different kinds of tensors.</p> <p>Note that no code implementations shall be placed here.</p> <p>Attributes:</p> Name Type Description <code>StrPath</code> <code>TypeAlias</code> <code>BytesPath</code> <code>TypeAlias</code> <code>Gt0</code> <code>TypeAlias</code> <code>Ge0</code> <code>TypeAlias</code> <code>ModelType</code> <code>TypeAlias</code> <p>The type of the model, e.g. <code>bs_roformer</code>, <code>demucs</code></p> <code>ModelInputChannels</code> <code>TypeAlias</code> <p>Required channel layout for model input audio.</p> <code>ModelInputType</code> <code>TypeAlias</code> <code>ModelOutputType</code> <code>TypeAlias</code> <code>ChunkSize</code> <code>TypeAlias</code> <p>The length of an audio segment, in samples, processed by the model at one time.</p> <code>HopSize</code> <code>TypeAlias</code> <p>The step size, in samples, between the start of consecutive chunks.</p> <code>Dropout</code> <code>TypeAlias</code> <code>ModelOutputStemName</code> <code>TypeAlias</code> <p>The output stem name, e.g. <code>vocals</code>, <code>drums</code>, <code>bass</code>, etc.</p> <code>Samples</code> <code>TypeAlias</code> <p>Number of samples in the audio signal.</p> <code>SampleRate</code> <code>TypeAlias</code> <p>The number of samples of audio recorded per second (hertz).</p> <code>Channels</code> <code>TypeAlias</code> <p>Number of audio streams.</p> <code>FileFormat</code> <code>TypeAlias</code> <code>BitRate</code> <code>TypeAlias</code> <p>Number of bits of information in each sample.</p> <code>RawAudioTensor</code> <p>Time domain tensor of audio samples.</p> <code>NormalizedAudioTensor</code> <p>A mixture tensor that has been normalized using on-the-fly statistics.</p> <code>ComplexSpectrogram</code> <p>A complex-valued representation of audio's frequency content over time via the STFT.</p> <code>LogMelSpectrogram</code> <p>A real-valued log-mel spectrogram.</p> <code>HybridModelInput</code> <code>TypeAlias</code> <p>Input for hybrid models that require both spectrogram and waveform.</p> <code>WindowShape</code> <code>TypeAlias</code> <p>The shape of the window function applied to each chunk before computing the STFT.</p> <code>FftSize</code> <code>TypeAlias</code> <p>The number of frequency bins in the STFT, controlling the frequency resolution.</p> <code>Bands</code> <code>TypeAlias</code> <p>Groups of adjacent frequency bins in the spectrogram.</p> <code>BatchSize</code> <code>TypeAlias</code> <p>The number of chunks processed simultaneously by the GPU.</p> <code>PaddingMode</code> <code>TypeAlias</code> <p>The method used to pad the audio before chunking, crucial for handling the edges of the audio signal.</p> <code>ChunkDuration</code> <code>TypeAlias</code> <p>The length of an audio segment, in seconds, processed by the model at one time.</p> <code>InferenceArchetype</code> <code>TypeAlias</code> <p>Inference pipeline archetype used to route runtime execution.</p> <code>OverlapRatio</code> <code>TypeAlias</code> <p>The fraction of a chunk that overlaps with the next one.</p> <code>TrimMargin</code> <code>TypeAlias</code> <p>Number of frames to trim from the edges of each chunk when aggregating logits.</p> <code>OverlapMode</code> <code>TypeAlias</code> <p>How overlapping logits chunks are resolved during aggregation.</p> <code>Padding</code> <code>TypeAlias</code> <p>Samples to add to the beginning and end of each chunk.</p> <code>PaddedChunkedAudioTensor</code> <p>A batch of audio chunks from a padded source.</p> <code>NumModelStems</code> <code>TypeAlias</code> <p>The number of stems the model outputs. This should be the length of [splifft.models.ModelParamsLike.output_stem_names].</p> <code>SeparatedSpectrogramTensor</code> <p>A batch of separated spectrograms.</p> <code>SeparatedChunkedTensor</code> <p>A batch of separated audio chunks from the model.</p> <code>WindowTensor</code> <p>A 1D tensor representing a window function.</p> <code>RawSeparatedTensor</code> <p>The final, stitched, raw-domain separated audio.</p> <code>LogitsTensor</code> <p>A time-series tensor representing activation probabilities or logits.</p> <code>PreprocessFn</code> <code>TypeAlias</code> <code>PostprocessFn</code> <code>TypeAlias</code> <code>Identifier</code> <code>TypeAlias</code> <p><code>{{architecture}}-{{first_author}}-{{unique_name_short}}</code>, use underscore if it has spaces</p> <code>Instrument</code> <code>TypeAlias</code> <code>Metric</code> <code>TypeAlias</code> <code>Sdr</code> <code>TypeAlias</code> <p>Signal-to-Distortion Ratio (decibels). Higher is better.</p> <code>SiSdr</code> <code>TypeAlias</code> <p>Scale-Invariant SDR (SI-SDR) is invariant to scaling errors (decibels). Higher is better.</p> <code>L1Norm</code> <code>TypeAlias</code> <p>L1 norm (mean absolute error) between two signals (dimensionless). Lower is better.</p> <code>DbDifferenceMel</code> <code>TypeAlias</code> <p>Difference in the dB-scaled mel spectrogram.</p> <code>Bleedless</code> <code>TypeAlias</code> <p>A metric to quantify the amount of \"bleeding\" from other sources. Higher is better.</p> <code>Fullness</code> <code>TypeAlias</code> <p>A metric to quantify how much of the original source is missing. Higher is better.</p>"},{"location":"api/types/#splifft.types.StrPath","title":"StrPath  <code>module-attribute</code>","text":"<pre><code>StrPath: TypeAlias = str | PathLike[str]\n</code></pre>"},{"location":"api/types/#splifft.types.BytesPath","title":"BytesPath  <code>module-attribute</code>","text":"<pre><code>BytesPath: TypeAlias = bytes | PathLike[bytes]\n</code></pre>"},{"location":"api/types/#splifft.types.Gt0","title":"Gt0  <code>module-attribute</code>","text":"<pre><code>Gt0: TypeAlias = Annotated[_T, Gt(0)]\n</code></pre>"},{"location":"api/types/#splifft.types.Ge0","title":"Ge0  <code>module-attribute</code>","text":"<pre><code>Ge0: TypeAlias = Annotated[_T, Ge(0)]\n</code></pre>"},{"location":"api/types/#splifft.types.ModelType","title":"ModelType  <code>module-attribute</code>","text":"<pre><code>ModelType: TypeAlias = str\n</code></pre> <p>The type of the model, e.g. <code>bs_roformer</code>, <code>demucs</code></p>"},{"location":"api/types/#splifft.types.ModelInputChannels","title":"ModelInputChannels  <code>module-attribute</code>","text":"<pre><code>ModelInputChannels: TypeAlias = Literal['mono', 'stereo']\n</code></pre> <p>Required channel layout for model input audio.</p> <ul> <li><code>mono</code>: model expects a single channel</li> <li><code>stereo</code>: model expects two channels</li> </ul>"},{"location":"api/types/#splifft.types.ModelInputType","title":"ModelInputType  <code>module-attribute</code>","text":"<pre><code>ModelInputType: TypeAlias = Literal[\n    \"waveform\", \"spectrogram\", \"waveform_and_spectrogram\"\n]\n</code></pre>"},{"location":"api/types/#splifft.types.ModelOutputType","title":"ModelOutputType  <code>module-attribute</code>","text":"<pre><code>ModelOutputType: TypeAlias = Literal[\n    \"waveform\",\n    \"spectrogram_mask\",\n    \"spectrogram\",\n    \"logits\",\n    \"multi_stream\",\n]\n</code></pre>"},{"location":"api/types/#splifft.types.ChunkSize","title":"ChunkSize  <code>module-attribute</code>","text":"<pre><code>ChunkSize: TypeAlias = Gt0[int]\n</code></pre> <p>The length of an audio segment, in samples, processed by the model at one time.</p> <p>A full audio track is often too long to fit into GPU, instead we process it in fixed-size chunks. A larger chunk size may allow the model to capture more temporal context at the cost of increased memory usage.</p>"},{"location":"api/types/#splifft.types.HopSize","title":"HopSize  <code>module-attribute</code>","text":"<pre><code>HopSize: TypeAlias = Gt0[int]\n</code></pre> <p>The step size, in samples, between the start of consecutive chunks.</p> <p>To avoid artifacts at the edges of chunks, we process them with overlap. The hop size is the distance we \"slide\" the chunking window forward. <code>ChunkSize &lt; HopSize</code> implies overlap and the overlap amount is <code>ChunkSize - HopSize</code>.</p>"},{"location":"api/types/#splifft.types.Dropout","title":"Dropout  <code>module-attribute</code>","text":"<pre><code>Dropout: TypeAlias = Annotated[float, Ge(0.0), Le(1.0)]\n</code></pre>"},{"location":"api/types/#splifft.types.ModelOutputStemName","title":"ModelOutputStemName  <code>module-attribute</code>","text":"<pre><code>ModelOutputStemName: TypeAlias = Annotated[str, MinLen(1)]\n</code></pre> <p>The output stem name, e.g. <code>vocals</code>, <code>drums</code>, <code>bass</code>, etc.</p>"},{"location":"api/types/#splifft.types.Samples","title":"Samples  <code>module-attribute</code>","text":"<pre><code>Samples: TypeAlias = Gt0[int]\n</code></pre> <p>Number of samples in the audio signal.</p>"},{"location":"api/types/#splifft.types.SampleRate","title":"SampleRate  <code>module-attribute</code>","text":"<pre><code>SampleRate: TypeAlias = Gt0[int]\n</code></pre> <p>The number of samples of audio recorded per second (hertz).</p> <p>See concepts for more details.</p>"},{"location":"api/types/#splifft.types.Channels","title":"Channels  <code>module-attribute</code>","text":"<pre><code>Channels: TypeAlias = Gt0[int]\n</code></pre> <p>Number of audio streams.</p> <ul> <li>1: Mono audio</li> <li>2: Stereo (left and right). Models are usually trained on stereo audio.</li> </ul>"},{"location":"api/types/#splifft.types.FileFormat","title":"FileFormat  <code>module-attribute</code>","text":"<pre><code>FileFormat: TypeAlias = Literal[\"flac\", \"wav\", \"ogg\", \"npy\"]\n</code></pre>"},{"location":"api/types/#splifft.types.BitRate","title":"BitRate  <code>module-attribute</code>","text":"<pre><code>BitRate: TypeAlias = Literal[8, 16, 24, 32, 64]\n</code></pre> <p>Number of bits of information in each sample.</p> <p>It determines the dynamic range of the audio signal: the difference between the quietest and loudest possible sounds.</p> <ul> <li>16-bit: Standard for CD audio: ~96 dB dynamic range.</li> <li>24-bit: Common in professional audio, allowing for more headroom during mixing</li> <li>32-bit float: Standard in digital audio workstations (DAWs) and deep learning models.     The amplitude is represented by a floating-point number, which prevents clipping (distortion     from exceeding the maximum value). This library primarily works with fp32 tensors.</li> </ul>"},{"location":"api/types/#splifft.types.RawAudioTensor","title":"RawAudioTensor  <code>module-attribute</code>","text":"<pre><code>RawAudioTensor = NewType('RawAudioTensor', Tensor)\n</code></pre> <p>Time domain tensor of audio samples. Shape (channels, samples)</p>"},{"location":"api/types/#splifft.types.NormalizedAudioTensor","title":"NormalizedAudioTensor  <code>module-attribute</code>","text":"<pre><code>NormalizedAudioTensor = NewType(\n    \"NormalizedAudioTensor\", Tensor\n)\n</code></pre> <p>A mixture tensor that has been normalized using on-the-fly statistics. Shape (channels, samples)</p>"},{"location":"api/types/#splifft.types.ComplexSpectrogram","title":"ComplexSpectrogram  <code>module-attribute</code>","text":"<pre><code>ComplexSpectrogram = NewType('ComplexSpectrogram', Tensor)\n</code></pre> <p>A complex-valued representation of audio's frequency content over time via the STFT.</p> <p>Shape (channels, frequency bins, time frames, 2)</p> <p>See concepts for more details.</p>"},{"location":"api/types/#splifft.types.LogMelSpectrogram","title":"LogMelSpectrogram  <code>module-attribute</code>","text":"<pre><code>LogMelSpectrogram = NewType('LogMelSpectrogram', Tensor)\n</code></pre> <p>A real-valued log-mel spectrogram. Shape (1, [mels], [time])</p>"},{"location":"api/types/#splifft.types.HybridModelInput","title":"HybridModelInput  <code>module-attribute</code>","text":"<pre><code>HybridModelInput: TypeAlias = tuple[\n    ComplexSpectrogram,\n    RawAudioTensor | NormalizedAudioTensor,\n]\n</code></pre> <p>Input for hybrid models that require both spectrogram and waveform.</p>"},{"location":"api/types/#splifft.types.WindowShape","title":"WindowShape  <code>module-attribute</code>","text":"<pre><code>WindowShape: TypeAlias = Literal[\n    \"hann\", \"hamming\", \"linear_fade\"\n]\n</code></pre> <p>The shape of the window function applied to each chunk before computing the STFT.</p>"},{"location":"api/types/#splifft.types.FftSize","title":"FftSize  <code>module-attribute</code>","text":"<pre><code>FftSize: TypeAlias = Gt0[int]\n</code></pre> <p>The number of frequency bins in the STFT, controlling the frequency resolution.</p>"},{"location":"api/types/#splifft.types.Bands","title":"Bands  <code>module-attribute</code>","text":"<pre><code>Bands: TypeAlias = Tensor\n</code></pre> <p>Groups of adjacent frequency bins in the spectrogram.</p>"},{"location":"api/types/#splifft.types.BatchSize","title":"BatchSize  <code>module-attribute</code>","text":"<pre><code>BatchSize: TypeAlias = Gt0[int]\n</code></pre> <p>The number of chunks processed simultaneously by the GPU.</p> <p>Increasing the batch size can improve GPU utilisation and speed up training, but it requires more memory.</p>"},{"location":"api/types/#splifft.types.PaddingMode","title":"PaddingMode  <code>module-attribute</code>","text":"<pre><code>PaddingMode: TypeAlias = Literal[\n    \"reflect\", \"constant\", \"replicate\"\n]\n</code></pre> <p>The method used to pad the audio before chunking, crucial for handling the edges of the audio signal.</p> <ul> <li><code>reflect</code>: Pads the signal by reflecting the audio at the boundary. This creates a smooth   continuation and often yields the best results for music.</li> <li><code>constant</code>: Pads with zeros. Simpler, but can introduce silence at the edges.</li> <li><code>replicate</code>: Repeats the last sample at the edge.</li> </ul>"},{"location":"api/types/#splifft.types.ChunkDuration","title":"ChunkDuration  <code>module-attribute</code>","text":"<pre><code>ChunkDuration: TypeAlias = Gt0[float]\n</code></pre> <p>The length of an audio segment, in seconds, processed by the model at one time.</p> <p>Equivalent to chunk size divided by the sample rate.</p>"},{"location":"api/types/#splifft.types.InferenceArchetype","title":"InferenceArchetype  <code>module-attribute</code>","text":"<pre><code>InferenceArchetype: TypeAlias = Literal[\n    \"standard_end_to_end\",\n    \"frequency_masking\",\n    \"sequence_labeling\",\n]\n</code></pre> <p>Inference pipeline archetype used to route runtime execution.</p> <ul> <li><code>standard_end_to_end</code>: waveform -&gt; model -&gt; waveform (e.g. demucs)</li> <li><code>frequency_masking</code>: waveform -&gt; STFT -&gt; model -&gt; iSTFT -&gt; waveform (e.g. bs-roformer)</li> <li><code>sequence_labeling</code>: waveform -&gt; optional feature extraction -&gt; model -&gt; sequence outputs   (e.g. beat tracking, pitch estimation)</li> </ul>"},{"location":"api/types/#splifft.types.OverlapRatio","title":"OverlapRatio  <code>module-attribute</code>","text":"<pre><code>OverlapRatio: TypeAlias = Annotated[float, Ge(0), Lt(1)]\n</code></pre> <p>The fraction of a chunk that overlaps with the next one.</p> <p>The relationship with hop size is: $$ \\text{hop_size} = \\text{chunk_size} \\cdot (1 - \\text{overlap_ratio}) $$</p> <ul> <li>A ratio of <code>0.0</code> means no overlap (hop_size = chunk_size).</li> <li>A ratio of <code>0.5</code> means 50% overlap (hop_size = chunk_size / 2).</li> <li>A higher overlap ratio increases computational cost as more chunks are processed, but it can lead   to smoother results by averaging more predictions for each time frame.</li> </ul>"},{"location":"api/types/#splifft.types.TrimMargin","title":"TrimMargin  <code>module-attribute</code>","text":"<pre><code>TrimMargin: TypeAlias = Annotated[int, Ge(0)]\n</code></pre> <p>Number of frames to trim from the edges of each chunk when aggregating logits.</p> <p>Useful for models that produce artifacts at the boundaries of predictions.</p>"},{"location":"api/types/#splifft.types.OverlapMode","title":"OverlapMode  <code>module-attribute</code>","text":"<pre><code>OverlapMode: TypeAlias = Literal['keep_first', 'keep_last']\n</code></pre> <p>How overlapping logits chunks are resolved during aggregation.</p> <ul> <li><code>keep_first</code>: earlier chunks in time win on overlap</li> <li><code>keep_last</code>: later chunks in time win on overlap</li> </ul>"},{"location":"api/types/#splifft.types.Padding","title":"Padding  <code>module-attribute</code>","text":"<pre><code>Padding: TypeAlias = Gt0[int]\n</code></pre> <p>Samples to add to the beginning and end of each chunk.</p> <ul> <li>To ensure that the very beginning and end of a track can be centerd within a chunk, we often may   add \"reflection padding\" or \"zero padding\" before chunking.</li> <li>To ensure that the last chunk is full-size, we may pad the audio so its length is a multiple of   the hop size.</li> </ul>"},{"location":"api/types/#splifft.types.PaddedChunkedAudioTensor","title":"PaddedChunkedAudioTensor  <code>module-attribute</code>","text":"<pre><code>PaddedChunkedAudioTensor = NewType(\n    \"PaddedChunkedAudioTensor\", Tensor\n)\n</code></pre> <p>A batch of audio chunks from a padded source. Shape (batch size, channels, chunk size)</p>"},{"location":"api/types/#splifft.types.NumModelStems","title":"NumModelStems  <code>module-attribute</code>","text":"<pre><code>NumModelStems: TypeAlias = Gt0[int]\n</code></pre> <p>The number of stems the model outputs. This should be the length of [splifft.models.ModelParamsLike.output_stem_names].</p>"},{"location":"api/types/#splifft.types.SeparatedSpectrogramTensor","title":"SeparatedSpectrogramTensor  <code>module-attribute</code>","text":"<pre><code>SeparatedSpectrogramTensor = NewType(\n    \"SeparatedSpectrogramTensor\", Tensor\n)\n</code></pre> <p>A batch of separated spectrograms. Shape (b, n, f*s, t, c=2)</p>"},{"location":"api/types/#splifft.types.SeparatedChunkedTensor","title":"SeparatedChunkedTensor  <code>module-attribute</code>","text":"<pre><code>SeparatedChunkedTensor = NewType(\n    \"SeparatedChunkedTensor\", Tensor\n)\n</code></pre> <p>A batch of separated audio chunks from the model. Shape (batch size, number of stems, channels, chunk size)</p>"},{"location":"api/types/#splifft.types.WindowTensor","title":"WindowTensor  <code>module-attribute</code>","text":"<pre><code>WindowTensor = NewType('WindowTensor', Tensor)\n</code></pre> <p>A 1D tensor representing a window function. Shape (chunk size)</p>"},{"location":"api/types/#splifft.types.RawSeparatedTensor","title":"RawSeparatedTensor  <code>module-attribute</code>","text":"<pre><code>RawSeparatedTensor = NewType('RawSeparatedTensor', Tensor)\n</code></pre> <p>The final, stitched, raw-domain separated audio. Shape (number of stems, channels, samples)</p>"},{"location":"api/types/#splifft.types.LogitsTensor","title":"LogitsTensor  <code>module-attribute</code>","text":"<pre><code>LogitsTensor = NewType('LogitsTensor', Tensor)\n</code></pre> <p>A time-series tensor representing activation probabilities or logits. Shape (number of stems, [time]) or batched</p>"},{"location":"api/types/#splifft.types.PreprocessFn","title":"PreprocessFn  <code>module-attribute</code>","text":"<pre><code>PreprocessFn: TypeAlias = Callable[\n    [RawAudioTensor | NormalizedAudioTensor],\n    tuple[Tensor, ...],\n]\n</code></pre>"},{"location":"api/types/#splifft.types.PostprocessFn","title":"PostprocessFn  <code>module-attribute</code>","text":"<pre><code>PostprocessFn: TypeAlias = Callable[\n    ..., SeparatedChunkedTensor | LogitsTensor\n]\n</code></pre>"},{"location":"api/types/#splifft.types.Identifier","title":"Identifier  <code>module-attribute</code>","text":"<pre><code>Identifier: TypeAlias = LowerCase[str]\n</code></pre> <p><code>{{architecture}}-{{first_author}}-{{unique_name_short}}</code>, use underscore if it has spaces</p>"},{"location":"api/types/#splifft.types.Instrument","title":"Instrument  <code>module-attribute</code>","text":"<pre><code>Instrument: TypeAlias = Literal[\n    \"instrum\",\n    \"vocals\",\n    \"drums\",\n    \"bass\",\n    \"other\",\n    \"piano\",\n    \"lead_vocals\",\n    \"back_vocals\",\n    \"guitar\",\n    \"vocals1\",\n    \"vocals2\",\n    \"strings\",\n    \"wind\",\n    \"music\",\n    \"sfx\",\n    \"speech\",\n    \"foreground\",\n    \"background\",\n    \"restored\",\n    \"back\",\n    \"lead\",\n    \"back-instrum\",\n    \"kick\",\n    \"snare\",\n    \"toms\",\n    \"hh\",\n    \"cymbals\",\n    \"hh-cymbals\",\n    \"male\",\n    \"female\",\n    \"violin\",\n    \"ride\",\n    \"crash\",\n    \"dry\",\n    \"reverb\",\n    \"clean\",\n    \"crowd\",\n    \"denoised\",\n    \"noise\",\n    \"similarity\",\n    \"difference\",\n    \"center\",\n    \"keyboards\",\n    \"synthesizer\",\n    \"percussion\",\n    \"orchestral\",\n    \"no_drum-bass\",\n    \"karaoke\",\n    \"beat\",\n    \"downbeat\",\n    \"pitch\",\n    \"confidence\",\n    \"volume\",\n    \"activations\",\n    \"onset\",\n    \"note\",\n    \"contour\",\n]\n</code></pre>"},{"location":"api/types/#splifft.types.Metric","title":"Metric  <code>module-attribute</code>","text":"<pre><code>Metric: TypeAlias = Literal[\n    \"sdr\",\n    \"si_sdr\",\n    \"l1_freq\",\n    \"log_wmse\",\n    \"aura_stft\",\n    \"aura_mrstft\",\n    \"bleedless\",\n    \"fullness\",\n]\n</code></pre>"},{"location":"api/types/#splifft.types.Sdr","title":"Sdr  <code>module-attribute</code>","text":"<pre><code>Sdr: TypeAlias = float\n</code></pre> <p>Signal-to-Distortion Ratio (decibels). Higher is better.</p> <p>Measures the ratio of the power of clean reference signal to the power of all other error components (interference, artifacts, and spatial distortion).</p> <p>Definition: $$ \\text{SDR} = 10 \\log_{10} \\frac{|\\mathbf{s}|^2}{|\\mathbf{s} - \\mathbf{\\hat{s}}|^2}, $$ where:</p> <ul> <li>\\(\\mathbf{s}\\): ground truth source signal</li> <li>\\(\\mathbf{\\hat{s}}\\): estimated source signal produced by the model</li> <li>\\(||\\cdot||^2\\): squared L2 norm (power) of the signal</li> </ul>"},{"location":"api/types/#splifft.types.SiSdr","title":"SiSdr  <code>module-attribute</code>","text":"<pre><code>SiSdr: TypeAlias = float\n</code></pre> <p>Scale-Invariant SDR (SI-SDR) is invariant to scaling errors (decibels). Higher is better.</p> <p>It projects the estimate onto the reference to find the optimal scaling factor \\(\\alpha\\), creating a scaled reference that best matches the estimate's amplitude.</p> <ul> <li>Optimal scaling factor: \\(\\alpha = \\frac{\\langle\\mathbf{\\hat{s}}, \\mathbf{s}\\rangle}{||\\mathbf{s}||^2}\\)</li> <li>Scaled reference: \\(\\mathbf{s}_\\text{target} = \\alpha \\cdot \\mathbf{s}\\)</li> <li>Error: \\(\\mathbf{e} = \\mathbf{\\hat{s}} - \\mathbf{s}_\\text{target}\\)</li> <li>\\(\\text{SI-SDR} = 10 \\log_{10} \\frac{||\\mathbf{s}_\\text{target}||^2}{||\\mathbf{e}||^2}\\)</li> </ul>"},{"location":"api/types/#splifft.types.L1Norm","title":"L1Norm  <code>module-attribute</code>","text":"<pre><code>L1Norm: TypeAlias = float\n</code></pre> <p>L1 norm (mean absolute error) between two signals (dimensionless). Lower is better.</p> <p>Measures the average absolute difference between the reference and estimated signals.</p> <ul> <li>Time domain: \\(\\mathcal{L}_\\text{L1} = \\frac{1}{N} \\sum_{n=1}^{N} |\\mathbf{s}[n] - \\mathbf{\\hat{s}}[n]|\\),</li> <li>Frequency domain: \\(\\mathcal{L}_\\text{L1Freq} = \\frac{1}{\\text{MK}}\\sum_{m=1}^{M} \\sum_{k=1}^{K} \\left||S(m, k)| - |\\hat{S}(m, k)|\\right|\\)</li> </ul>"},{"location":"api/types/#splifft.types.DbDifferenceMel","title":"DbDifferenceMel  <code>module-attribute</code>","text":"<pre><code>DbDifferenceMel: TypeAlias = float\n</code></pre> <p>Difference in the dB-scaled mel spectrogram. $$ \\mathbf{D}(m, k) = \\text{dB}(|\\hat{S}\\text{mel}(m, k)|) - \\text{dB}(|S\\text{mel}(m, k)|) $$</p>"},{"location":"api/types/#splifft.types.Bleedless","title":"Bleedless  <code>module-attribute</code>","text":"<pre><code>Bleedless: TypeAlias = float\n</code></pre> <p>A metric to quantify the amount of \"bleeding\" from other sources. Higher is better.</p> <p>Measures the average energy of the parts of the mel spectrogram that are louder than the reference. A high value indicates that the estimate contains unwanted energy (bleed) from other sources: $$ \\text{Bleed} = \\text{mean}(\\mathbf{D}(m, k)) \\quad \\forall \\quad \\mathbf{D}(m, k) &gt; 0 $$</p>"},{"location":"api/types/#splifft.types.Fullness","title":"Fullness  <code>module-attribute</code>","text":"<pre><code>Fullness: TypeAlias = float\n</code></pre> <p>A metric to quantify how much of the original source is missing. Higher is better.</p> <p>Complementary to Bleedless. Measures the average energy of the parts of the mel spectrogram that are quieter than the reference. A high value indicates that parts of the target loss were lost during the separation, indicating that more of the original source's character is preserved. $$ \\text{Fullness} = \\text{mean}(|\\mathbf{D}(m, k)|) \\quad \\forall \\quad \\mathbf{D}(m, k) &lt; 0 $$</p>"}]}